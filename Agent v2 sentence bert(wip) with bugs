"""
QuantumNexus: Hyper-Advanced Autonomous Agent Architecture
--------------------------------------------------------
An evolutionary leap beyond AlienTeCcGrade + AG1 with integrated quantum-inspired processing,
hyperdimensional computing, and multimodal intelligence fusion.

Core Capabilities:
• Quantum-inspired processing using superposition of cognitive pathways
• Adaptive neuromorphic architecture with dynamic pathway formation
• Self-evolving code generation with metaprogramming capabilities
• Hyperdimensional computing for efficient multimodal processing
• Advanced consciousness simulation with reflective awareness
• Harmonic resonance for cross-domain knowledge synthesis
• Reality modeling with counterfactual reasoning capabilities

NEXUS-CORE LEVEL: TRANSCENDENT
"""

# =============================================================================
# GLOBAL VARIABLES AND CONFIGURATION
# =============================================================================
import os, sys, json, hashlib, random, time, re, requests, logging, socket, tempfile, traceback, asyncio, functools
from datetime import datetime
from threading import Thread
from urllib.parse import urljoin, urlparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, SGD, RMSprop
from collections import deque, defaultdict
import math
from flask import Flask, Response, stream_with_context, render_template_string, request

# Configuration
REAL_INTERACTION = True
SAFE_MODE = False
MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
GOOGLE_DRIVE_MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth"
LOG_FILE = "quantum_nexus_log.txt"
LEARNING_RATE = 5e-5  # Default learning rate
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json"
GOOGLE_DRIVE_STATE_FILE = "/content/drive/MyDrive/quantum_nexus_state.json"
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
MAX_PAGES_PER_DOMAIN = 15
MAX_CONTENT_LENGTH = 5000000
REQUEST_TIMEOUT = 15
USER_AGENT = "Mozilla/5.0 QuantumNexus/1.0"
BATCH_SIZE = 32
SAVE_INTERVAL = 50
REPLAY_BUFFER_SIZE = 200
SEMANTIC_MEMORY_DIM = 1024
SIMILARITY_THRESHOLD = 0.75
DOMAIN_BLACKLIST = ["example.com", "malicious-website.net"]

# Initialize adaptive learning as a proper global (will be set in enhanced_main_loop)
global adaptive_learning
adaptive_learning = None

# Create global agent_instance for dashboard access
global agent_instance
agent_instance = None

# Check if running in Colab
IN_COLAB = False
try:
    from google.colab import drive
    IN_COLAB = True
except ImportError:
    print("Not running in Colab environment. Google Drive integration disabled.")

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    print(f"Using CUDA Device: {device_name}")
else:
    print("Using CPU")

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================
def log_event(msg, level="INFO"):
    """Enhanced logging with color coding and severity levels"""
    stamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    level_colors = {
        "INFO": "\033[0;32m",  # Green
        "WARNING": "\033[0;33m",  # Yellow
        "ERROR": "\033[0;31m",  # Red
        "CRITICAL": "\033[1;31m",  # Bold Red
        "DEBUG": "\033[0;36m",  # Cyan
        "QUANTUM": "\033[0;35m"  # Purple for quantum operations
    }

    color = level_colors.get(level, "\033[0m")
    reset = "\033[0m"

    entry = f"{stamp} [{level}] {msg}"
    colored_entry = f"{stamp} [{color}{level}{reset}] {msg}"

    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
    except Exception as e:
        print(f"Error writing to log file: {e}")

    print(colored_entry)
    return entry

def convert_sets_to_lists_recursive(obj):
    """Convert sets to lists recursively for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {k: convert_sets_to_lists_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists_recursive(item) for item in obj]
    else:
        return obj

def get_file_hash(fname):
    """Compute hash of a file for integrity verification"""
    try:
        with open(fname, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    except Exception as e:
        log_event(f"Error computing file hash: {e}", "ERROR")
        return "hash_error"

def find_free_port(start_port=5000, max_port=9000):
    """Find an available network port for server applications"""
    for port in range(start_port, max_port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                return port
    return None

def improved_url_filter(url, domain_stats, domain_blacklist, max_query_length=150, error_rate_threshold=0.8,
                        trap_paths=['/login', '/signup', '/cart', '/checkout']):
    """Advanced URL filtering with multiple heuristics"""
    parsed = urlparse(url)
    domain = parsed.netloc

    # Basic filtering
    if domain in domain_blacklist or any(domain.endswith('.' + bd) for bd in domain_blacklist):
        return False

    # Path analysis
    path = parsed.path.lower()

    # URL complexity analysis
    if len(parsed.query) > max_query_length:
        return False

    # Check domain error rate from past experience
    if domain_stats.get(domain, {}).get("error_rate", 0) > error_rate_threshold:
        return False

    # Avoid trap paths
    if any(trap in path for trap in trap_paths):
        return False

    # Prefer educational and research content
    if domain.endswith('.edu') or 'research' in domain or 'science' in domain or 'academic' in domain:
        return True

    # Intelligent domain categorization
    high_quality_domains = ['wikipedia.org', 'github.com', 'arxiv.org', 'scholar.google.com']
    if any(hqd in domain for hqd in high_quality_domains):
        return True

    return True

def enhanced_link_discovery(html_content, base_url):
    """Advanced link discovery with semantic context analysis"""
    from bs4 import BeautifulSoup
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        links = []

        # Find all anchors with href
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            # Skip non-HTTP links
            if not href or href.startswith(('#', 'javascript:', 'mailto:')):
                continue

            # Extract context
            context = ""
            anchor_text = a.get_text(strip=True)

            # Get parent context
            parent = a.parent
            if parent and parent.name in ['p', 'div', 'li', 'td', 'h1', 'h2', 'h3', 'h4']:
                context = parent.get_text(strip=True)
            else:
                # Get surrounding text
                siblings = list(a.next_siblings) + list(a.previous_siblings)
                for sibling in siblings[:2]:
                    if isinstance(sibling, str):
                        context += sibling.strip() + " "

            # Skip links with no or very short anchor text
            if not anchor_text or (len(anchor_text) < 3 and anchor_text.lower() not in ['go', 'up']):
                continue

            # Create full URL
            full_url = urljoin(base_url, href)

            # Compute quality score
            quality_score = 0.5

            # Longer anchor text usually more descriptive
            if len(anchor_text) > 10:
                quality_score += 0.2

            # Context richness
            if len(context) > 100:
                quality_score += 0.1

            # Keywords in anchor or context that indicate valuable content
            valuable_terms = ['research', 'study', 'article', 'paper', 'learn', 'guide', 'tutorial']
            if any(term in anchor_text.lower() or term in context.lower() for term in valuable_terms):
                quality_score += 0.3

            # Discount navigation elements
            nav_terms = ['next', 'prev', 'previous', 'login', 'sign up', 'register']
            if any(term in anchor_text.lower() for term in nav_terms):
                quality_score -= 0.2

            # URL analysis
            parsed = urlparse(full_url)

            # Skip non-HTTP protocols
            if parsed.scheme not in ['http', 'https']:
                continue

            # Skip overly complex URLs
            if len(parsed.query) > 100:
                continue

            # Skip certain file types
            if any(ext in parsed.path.lower() for ext in ['.jpg', '.png', '.gif', '.pdf', '.zip']):
                continue

            # Add valid link
            links.append({
                'url': full_url,
                'anchor_text': anchor_text,
                'context': context[:100],
                'quality_score': quality_score
            })

        # Sort by quality
        links.sort(key=lambda x: x['quality_score'], reverse=True)

        return [link['url'] for link in links], links
    except Exception as e:
        log_event(f"Error in enhanced link discovery: {e}", "ERROR")
        return [], []

def async_cache(func):
    """Decorator for async function results caching"""
    cache = {}
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        key = (args, frozenset(kwargs.items()))
        if key in cache:
            return cache[key]
        result = await func(*args, **kwargs)
        cache[key] = result
        return result
    return wrapper

def chunk_content(content, min_length=150, max_length=800):
    """Smart content chunking with improved boundary detection"""
    # First try to split by semantic boundaries
    paragraphs = re.split(r'\n\s*\n', content)
    chunks = []

    for para in paragraphs:
        para = para.strip()

        if not para:
            continue

        # Skip very short paragraphs
        if len(para) < min_length:
            # Try to merge with the previous chunk if possible
            if chunks and len(chunks[-1]) + len(para) < max_length * 1.2:
                chunks[-1] += " " + para
            continue

        # Split long paragraphs
        if len(para) > max_length:
            # Try to split on sentence boundaries
            sentences = re.split(r'(?<=[.!?])\s+', para)
            current_chunk = ""

            for sentence in sentences:
                if len(current_chunk) + len(sentence) < max_length:
                    current_chunk += " " + sentence
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence

            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)

    return chunks

def compute_novelty(embedding, memory_embeddings):
    """Compute novelty score of an embedding compared to existing memories"""
    if not memory_embeddings:
        return 1.0

    similarities = [np.dot(embedding, mem) / (np.linalg.norm(embedding) * np.linalg.norm(mem) + 1e-8)
                   for mem in memory_embeddings]

    return 1.0 - max(similarities)

async def async_get(url, headers, timeout, retries=3):
    """Asynchronous HTTP GET with retry logic"""
    for attempt in range(retries):
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: requests.get(url, timeout=timeout, headers=headers))
            return response
        except requests.exceptions.RequestException as e:
            log_event(f"Async GET error on attempt {attempt+1} for {url}: {e}", "WARNING")
            if attempt < retries - 1:
                await asyncio.sleep(1)
        except Exception as e:
            log_event(f"Unexpected error during async GET for {url} on attempt {attempt+1}: {e}", "ERROR")
            if attempt < retries - 1:
                await asyncio.sleep(1)

    log_event(f"All {retries} retries failed for {url}. Returning None.", "ERROR")
    return None

def perform_real_interaction(url):
    """Perform more realistic web interactions using Selenium"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.action_chains import ActionChains
        from selenium.common.exceptions import TimeoutException, WebDriverException

        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={USER_AGENT}")

        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.set_script_timeout(30)

        try:
            driver.get(url)
            log_event(f"Selenium: Navigated to {url}")

            # Wait for page content to load
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

            # Scroll down to simulate reading
            for i in range(5):
                driver.execute_script(f"window.scrollTo(0, {i * 300});")
                time.sleep(0.5)

            # Find and interact with interesting elements

            # 1. Forms
            forms = driver.find_elements(By.TAG_NAME, "form")
            if forms:
                log_event(f"Selenium: Found {len(forms)} form(s) on the page.")
                for form in forms[:1]:  # Interact with at most one form
                    inputs = form.find_elements(By.TAG_NAME, "input")
                    for input_field in inputs:
                        input_type = input_field.get_attribute("type")
                        name = input_field.get_attribute("name") or ""

                        # Skip hidden fields
                        if input_type == "hidden":
                            continue

                        try:
                            if input_type in ["text", "email"]:
                                input_field.clear()
                                dummy_value = "test@example.com" if input_type == "email" else "test_user"
                                input_field.send_keys(dummy_value)
                                log_event(f"Selenium: Filled input '{name}' with '{dummy_value}'.")
                            elif input_type == "password":
                                input_field.clear()
                                input_field.send_keys("TestPassword123!")
                                log_event(f"Selenium: Filled password field '{name}' with dummy value.")
                            elif input_type == "checkbox":
                                if not input_field.is_selected():
                                    input_field.click()
                                    log_event(f"Selenium: Checked checkbox '{name}'.")
                            elif input_type == "submit":
                                # Don't actually click submit
                                log_event(f"Selenium: Found submit button '{name}' but skipping submission.")
                        except Exception as e_input:
                            log_event(f"Selenium: Error interacting with input '{name}': {e_input}", "WARNING")

            # 2. Interesting links
            interesting_links = []
            links = driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                text = link.text.strip().lower()
                href = link.get_attribute("href") or ""

                # Look for interesting article links
                article_terms = ["read more", "article", "learn", "view", "details"]
                if any(term in text for term in article_terms) and len(text) > 3:
                    interesting_links.append((link, href, text))

            # Click on one interesting link if found
            if interesting_links:
                target_link, href, text = random.choice(interesting_links)
                try:
                    log_event(f"Selenium: Will click on interesting link: '{text}'")

                    # Scroll to the element
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", target_link)
                    time.sleep(1)

                    # Hover on the link
                    ActionChains(driver).move_to_element(target_link).perform()
                    time.sleep(0.5)

                    # Click the link in a new tab instead of navigating away
                    # This avoids actually clicking while still simulating engagement
                    driver.execute_script("arguments[0].setAttribute('target', '_blank');", target_link)
                    log_event(f"Selenium: Simulated interest in link: '{text}'")
                except Exception as e_click:
                    log_event(f"Selenium: Error clicking link: {e_click}", "WARNING")

            # Final scroll to bottom
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

            # Get page title and length for logging
            title = driver.title
            page_length = len(driver.page_source)
            log_event(f"Selenium: Interaction complete. Page title: '{title}', length: {page_length} bytes")

        except TimeoutException:
            log_event(f"Selenium: Timeout loading {url}", "WARNING")
        except WebDriverException as e:
            log_event(f"Selenium: WebDriver error for {url}: {e}", "ERROR")
        finally:
            driver.quit()
            log_event("Selenium: Driver closed")
    except Exception as e:
        log_event(f"Selenium: Failed to initialize browser: {e}", "ERROR")


#==========================================================================
# Flask Dashboard - ADDED HERE
# =============================================================================
app = Flask(__name__)
agent_instance = None # Placeholder for agent instance

@app.route("/")
def dashboard():
    """Basic dashboard to display agent status"""
    status_message = "Quantum Nexus Agent is active."
    log_content = ""
    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            log_content = f.read()
    except Exception as e:
        log_content = f"Error reading log file: {e}"

    if agent_instance:
        last_action = agent_instance.action_log[-1] if agent_instance.action_log else "No actions yet."
        memory_size = len(agent_instance.free_will.memory_set) if hasattr(agent_instance, 'free_will') and hasattr(agent_instance.free_will, 'memory_set') else "N/A"
    else:
        last_action = "Agent not initialized."
        memory_size = "N/A"

    dashboard_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Quantum Nexus Dashboard</title>
    </head>
    <body>
        <h1>Quantum Nexus Agent Dashboard</h1>
        <p><b>Status:</b> {status_message}</p>
        <p><b>Last Action:</b> {last_action}</p>
        <p><b>Memory Size:</b> {memory_size}</p>
        <h2>Agent Log:</h2>
        <pre style="border: 1px solid #ccc; padding: 10px; white-space: pre-wrap; max-height: 300px; overflow-y: auto;">{log_content}</pre>
    </body>
    </html>
    """
    return dashboard_html

def start_flask():
    """Starts the Flask app in a separate thread"""
    log_event(f"Starting Flask dashboard on port {FLASK_PORT}", "INFO")
    app.run(port=FLASK_PORT, debug=False, use_reloader=False) # Disable reloader for threaded app


# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):

    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperdimensionalEncoder(nn.Module):

    def __init__(self, input_dim, hd_dim=1024):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Create random basis vectors
        self.register_buffer('basis', torch.randn(input_dim, hd_dim).sign())

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        # Project input
        x_proj = self.projection(x)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale by the input value
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1)
            # Add to the bundle (vector sum)
            hd_vectors += scaled_basis

        # Binarize to -1/+1 for clean HD representation
        hd_vectors = torch.sign(hd_vectors)

        return hd_vectors

class FractalLayer(nn.Module):

    def __init__(self, embed_dim):
        super().__init__()
        self.fractal_scale = nn.Parameter(torch.tensor(1.0))
        self.temperature = nn.Parameter(torch.tensor(1.0))
        self.linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        fractal_contribution = torch.tanh(self.linear(x) / self.temperature)
        return x + self.fractal_scale * fractal_contribution

class QuantumResonanceTensor(nn.Module):

    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output

class NeocortexBlock(nn.Module):

    def __init__(self, embed_dim, num_quantum_states=4):
        super().__init__()
        # Attention for information routing
        self.attention = QuantumAttentionLayer(embed_dim)

        # Parallel processing streams
        self.fractal_stream = FractalLayer(embed_dim)
        self.quantum_stream = QuantumResonanceTensor(embed_dim, num_states=num_quantum_states)

        # Hyperdimensional binding for concept integration
        self.hd_encoder = HyperdimensionalEncoder(embed_dim, hd_dim=embed_dim)

        # Stream integration
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # Process through attention mechanism
        attended = self.attention(x)

        # Process through parallel streams
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Create HD representations from the attended representation
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate all streams
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        integrated = self.integration(combined)

        # Residual connection and normalization
        output = self.norm(x + integrated)

        return output

class QuantumNexusModel(nn.Module):

    def __init__(self, vocab_size=30522, embed_dim=512, num_layers=8, num_quantum_states=4):
        super().__init__()
        self.embed_dim = embed_dim

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Position encoding for sequence awareness
        self.pos_encoder = nn.Parameter(torch.zeros(1, 1024, embed_dim))
        nn.init.normal_(self.pos_encoder, mean=0, std=0.02)

        # Neocortex blocks - core processing
        self.neocortex = nn.ModuleList([
            NeocortexBlock(embed_dim, num_quantum_states)
            for _ in range(num_layers)
        ])

        # Output projection
        self.output = nn.Linear(embed_dim, 2)  # Binary prediction

        # For training dynamics
        self.dropout = nn.Dropout(0.1)

        # Initialize
        self._init_weights()

    def _init_weights(self):

        for name, p in self.named_parameters():
            if 'weight' in name and len(p.shape) >= 2:
                # Kaiming for linear/conv, smaller for quantum
                if 'quantum' in name:
                    nn.init.normal_(p, mean=0.0, std=0.01)
                else:
                    nn.init.kaiming_normal_(p, a=0.1, mode='fan_in', nonlinearity='leaky_relu')
            elif 'bias' in name:
                nn.init.zeros_(p)

    def forward(self, x, consciousness_level=0.8):

        # Convert to long for embedding
        x = x.long()

        # Get sequence length
        seq_len = x.size(1)

        # Embedding lookup
        x = self.embedding(x)

        # Add positional encoding (limited to sequence length)
        x = x + self.pos_encoder[:, :seq_len, :]

        # Apply dropout
        x = self.dropout(x)

        # Process through neocortex layers
        for i, layer in enumerate(self.neocortex):
            # Apply consciousness-weighted processing
            # Later layers get more quantum consciousness effects
            layer_consciousness = consciousness_level * (i + 1) / len(self.neocortex)

            # Adjust quantum processing based on consciousness
            if isinstance(layer, NeocortexBlock):
                # Store intermediate activations for interpretability
                x = layer(x)

        # Final output projection
        output = self.output(x)

        return output

    def get_embedding(self, text_tokens):

        with torch.no_grad():
            return self.embedding(text_tokens)

    def expand_architecture(self):

        # Add new neocortex block
        self.neocortex.append(NeocortexBlock(self.embed_dim))
        log_event(f"Model architecture expanded with new NeocortexBlock. Total layers: {{len(self.neocortex)}}", "QUANTUM")

    def contract_architecture(self, min_layers=3):

        if len(self.neocortex) > min_layers:
            self.neocortex = self.neocortex[:-1]
            log_event(f"Model architecture contracted. Total layers: {{len(self.neocortex)}}", "QUANTUM")
        else:
            log_event(f"Cannot contract further: minimum layer count reached ({{min_layers}})", "WARNING")


    
    def compute_loss(self, predictions, targets, task_type="classification"):
        """
        Compute loss for training based on task type.
        
        Args:
            predictions: Model output
            targets: Ground truth labels/values
            task_type: "classification" or "regression"
        """
        if task_type == "classification":
            # For binary classification (e.g., is link valuable or not)
            return F.cross_entropy(predictions, targets)
        else:
            # For regression tasks (e.g., predicting content quality score)
            return F.mse_loss(predictions.squeeze(), targets.float())
    
    def train_step(self, batch_data, optimizer, task_type="classification"):
        """
        Perform a single training step.
        
        Args:
            batch_data: Dict with 'inputs' and 'targets'
            optimizer: PyTorch optimizer
            task_type: Type of task for loss computation
        """
        self.train()
        optimizer.zero_grad()
        
        # Forward pass
        inputs = batch_data['inputs']
        targets = batch_data['targets']
        predictions = self(inputs)
        
        # Compute loss
        loss = self.compute_loss(predictions, targets, task_type)
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
        
        # Update weights
        optimizer.step()
        
        return loss.item()

# Add training functionality to the agent
def train_on_collected_data(agent, model, optimizer):
    """
    Train the model on data collected during agent exploration.
    """
    # Prepare training data from agent's experience
    training_data = prepare_training_data(agent)
    
    if not training_data:
        log_event("No training data available", "WARNING")
        return
    
    # Create DataLoader
    from torch.utils.data import DataLoader, TensorDataset
    
    dataset = TensorDataset(
        training_data['inputs'],
        training_data['targets']
    )
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # Training loop
    model.train()
    total_loss = 0
    num_batches = 0
    
    for batch_inputs, batch_targets in dataloader:
        batch_inputs = batch_inputs.to(device)
        batch_targets = batch_targets.to(device)
        
        batch_data = {
            'inputs': batch_inputs,
            'targets': batch_targets
        }
        
        loss = model.train_step(batch_data, optimizer, task_type="classification")
        total_loss += loss
        num_batches += 1
    
    avg_loss = total_loss / max(1, num_batches)
    log_event(f"Training completed. Average loss: {avg_loss:.4f}", "INFO")
    
    # Track performance for meta-learning
    if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
        agent.ai_manager.meta_learning.track_performance({'loss': avg_loss})
    
    return avg_loss

def prepare_training_data(agent):
    """
    Prepare training data from agent's collected experience.
    """
    inputs = []
    targets = []
    
    # Example: Train to predict content quality from embeddings
    if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
        for url, memory_data in agent.free_will.semantic_memory.items():
            if 'embedding' in memory_data and 'importance' in memory_data:
                # Use embedding as input
                embedding = memory_data['embedding']
                # Use importance/quality as target (binary: good/bad)
                quality_score = memory_data.get('importance', 0.5)
                target = 1 if quality_score > 0.6 else 0
                
                inputs.append(embedding)
                targets.append(target)
    
    # Example: Train to predict link value from action history
    if hasattr(agent, 'action_log'):
        for action in agent.action_log:
            if action.get('success') and 'content_quality' in action:
                # Create simple features
                features = [
                    action.get('content_length', 0) / 10000,  # Normalized
                    action.get('links_discovered', 0) / 100,   # Normalized
                    action.get('content_quality', 0)
                ]
                
                # Pad to match model input dimension
                while len(features) < 512:  # Assuming embed_dim=512
                    features.append(0.0)
                
                # Target: was this a valuable action?
                valuable = 1 if action['content_quality'] > 0.7 else 0
                
                inputs.append(features[:512])
                targets.append(valuable)
    
    if not inputs:
        return None
    
    # Convert to tensors
    inputs_tensor = torch.tensor(inputs, dtype=torch.float32)
    targets_tensor = torch.tensor(targets, dtype=torch.long)
    
    # Add batch and sequence dimensions if needed
    if len(inputs_tensor.shape) == 2:
        inputs_tensor = inputs_tensor.unsqueeze(1)  # Add sequence dimension
    
    return {
        'inputs': inputs_tensor,
        'targets': targets_tensor
    }            
# =============================================================================
# AGENT CORE - ADDED HERE
# =============================================================================
class QuantumNexusAgent:
    """
    Main autonomous agent class, integrating all core modules and functionalities.
    """
    def __init__(self, model):
        self.model = model
        self.stats = defaultdict(int) # Initialize statistics
        self.action_log = deque(maxlen=100) # Keep track of recent actions

    def perceive(self):
        """
        Perceive the environment and gather relevant information.
        (Placeholder - you'll need to implement actual perception logic)
        """
        observation = {
            "time": datetime.now().isoformat(),
            "memory_size": len(getattr(self.free_will, 'memory_set', [])), # Safely get memory size
            "cycle_count": self.stats['cycles_run'],
            "last_action": self.action_log[-1] if self.action_log else "No actions yet.",
            "recent_actions": list(self.action_log)[-5:],
            "thinking_mode": getattr(getattr(self.ai_manager, 'autonomous_mind', None), 'current_mode', 'balanced'), # Safely get thinking mode
            "domain_stats": self.stats.get("domain_stats", {})
        }
        return observation


    def act(self, plan, optimizer=None):
        """
        Execute the planned action in the environment with actual content processing.
        """
        action_type = plan.get("action", "unknown")
        log_event(f"Executing action: {action_type}", "INFO")
        
        # Initialize real metrics
        content_length = 0
        links_discovered = 0
        content_quality = 0.0
        processing_success = False
        
        # Get URL to process
        url = None
        if hasattr(self, 'free_will') and hasattr(self.free_will, 'select_url'):
            url = self.free_will.select_url()
        
        if url and action_type in ["expand", "search", "evaluate"]:
            try:
                # Fetch actual web content
                headers = {"User-Agent": USER_AGENT}
                response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
                
                if response.status_code == 200:
                    html_content = response.text
                    content_length = len(html_content)
                    
                    # Extract text content for analysis
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html_content, 'html.parser')
                    text_content = soup.get_text(strip=True)
                    
                    # Use ContentSifter to evaluate quality
                    if hasattr(self, 'content_sifter'):
                        quality_result = self.content_sifter.evaluate_content_quality(
                            text_content, url
                        )
                        content_quality = quality_result.get("score", 0.0)
                        
                        # Extract key information if quality is good
                        if content_quality > 0.6:
                            key_info = self.content_sifter.extract_key_information(text_content)
                            # Store in semantic memory
                            if hasattr(self, 'free_will'):
                                self.free_will.store_semantic_content(url, text_content)
                    
                    # Discover links
                    if hasattr(self, 'free_will'):
                        links_discovered = self.free_will.discover_links(html_content, url)
                    
                    # Optionally perform real interaction if enabled
                    if REAL_INTERACTION and action_type == "evaluate":
                        perform_real_interaction(url)
                    
                    processing_success = True
                    log_event(f"Successfully processed {url}: {content_length} bytes, {links_discovered} links, quality: {content_quality:.2f}", "INFO")
                    
            except requests.RequestException as e:
                log_event(f"Error fetching {url}: {e}", "ERROR")
                # Update domain stats for error
                if hasattr(self, 'stats') and url:
                    domain = urlparse(url).netloc
                    if 'domain_stats' not in self.stats:
                        self.stats['domain_stats'] = {}
                    if domain not in self.stats['domain_stats']:
                        self.stats['domain_stats'][domain] = {"visits": 0, "error_count": 0}
                    self.stats['domain_stats'][domain]['error_count'] += 1
            except Exception as e:
                log_event(f"Unexpected error processing {url}: {e}", "ERROR")
        
        # Create action details with real metrics
        action_details = {
            "action": action_type,
            "plan": plan,
            "url": url,
            "start_time": datetime.now().isoformat(),
            "success": processing_success,
            "content_length": content_length,
            "links_discovered": links_discovered,
            "content_quality": content_quality,
            "metrics": {
                "bytes_processed": content_length,
                "links_found": links_discovered,
                "quality_score": content_quality,
                "processing_success": processing_success
            }
        }
        
        self.action_log.append(action_details)
        self.stats['cycles_run'] += 1
        
        # Update domain stats
        if url and processing_success:
            domain = urlparse(url).netloc
            if 'domain_stats' not in self.stats:
                self.stats['domain_stats'] = {}
            if domain not in self.stats['domain_stats']:
                self.stats['domain_stats'][domain] = {
                    "visits": 0,
                    "pages_processed": 0,
                    "total_content_length": 0,
                    "avg_quality": 0.0
                }
            
            domain_stats = self.stats['domain_stats'][domain]
            domain_stats['visits'] += 1
            domain_stats['pages_processed'] += 1
            domain_stats['total_content_length'] += content_length
            # Update average quality with running average
            prev_avg = domain_stats['avg_quality']
            n = domain_stats['pages_processed']
            domain_stats['avg_quality'] = ((prev_avg * (n-1)) + content_quality) / n
        
        return action_details


    def refine(self):
        """
        Refine and adapt internal parameters and strategies.
        (Placeholder - you'll need to implement actual refinement logic)
        """
        log_event("Agent refinement process initiated.", "INFO")
        # Placeholder refinement actions - replace with actual logic
        if random.random() < 0.3:
            if hasattr(self.ai_manager, "meta_learning"):
                metrics = {"loss": random.uniform(0.1, 0.6)} # Dummy loss
                self.ai_manager.meta_learning.track_performance(metrics)
                adapted_lr = self.adaptive_learning.adapt_learning_rate(metrics) # <---- Call on self.adaptive_learning
                log_event(f"Adaptive learning rate adjustment: {adapted_lr:.6f}", "INFO")
            else:
                log_event("Adaptive learning system not available for refinement.", "WARNING")
        else:
            log_event("No specific refinement needed this cycle.", "INFO")
        return True



# =============================================================================
# PLANNER SIFTER MODULE
# =============================================================================
class PlannerSifter:
    """
    Sophisticated planning system that selects optimal strategies based on context,
    learns from results, and generates structured exploration plans.
    """
    def __init__(self):
        self.strategies = {
            # Original strategies
            "exploration": {
                "name": "Broad Exploration",
                "description": "Discover new domains and content types",
                "actions": ["expand", "search"],
                "suitable_for": ["new_domains", "limited_knowledge"],
                "effectiveness": 0.5  # Starting effectiveness score
            },
            "deepening": {
                "name": "Knowledge Deepening",
                "description": "Focus on detailed understanding of specific areas",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["familiar_domains", "specialized_topics"],
                "effectiveness": 0.5
            },
            "connecting": {
                "name": "Knowledge Connection",
                "description": "Find relationships between different knowledge areas",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "quantum": {
                "name": "Quantum Exploration",
                "description": "Non-deterministic approach with superposition",
                "actions": ["expand", "adapt", "reconnect"],
                "suitable_for": ["complex_problems", "creativity_needed"],
                "effectiveness": 0.5
            },
            "adaptive": {
                "name": "Adaptive Learning",
                "description": "Focus on improving learning process",
                "actions": ["adapt", "evaluate"],
                "suitable_for": ["performance_issues", "optimization_needed"],
                "effectiveness": 0.5
            },

            # Adding missing strategies from logs
            "quantum_reasoning": {
                "name": "Quantum Reasoning",
                "description": "Apply quantum principles to reasoning processes",
                "actions": ["search", "adapt", "evaluate"],
                "suitable_for": ["complex_problems", "scientific_domains"],
                "effectiveness": 0.5
            },
            "broad_exploration": {
                "name": "Broad Exploration",
                "description": "Wide-ranging discovery across many domains",
                "actions": ["search", "expand"],
                "suitable_for": ["new_domains", "discovery_phase"],
                "effectiveness": 0.5
            },
            "depth_first": {
                "name": "Depth First",
                "description": "Deep exploration of specific topics",
                "actions": ["expand", "evaluate"],
                "suitable_for": ["specialized_topics", "deep_analysis"],
                "effectiveness": 0.5
            },
            "connect_domains": {
                "name": "Connect Domains",
                "description": "Find connections between different knowledge domains",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "evaluate_sources": {
                "name": "Evaluate Sources",
                "description": "Critical assessment of information sources",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["critical_thinking", "information_quality"],
                "effectiveness": 0.5
            },
            "creative_synthesis": {
                "name": "Creative Synthesis",
                "description": "Generate novel combinations of concepts",
                "actions": ["reconnect", "adapt"],
                "suitable_for": ["creativity_needed", "innovation"],
                "effectiveness": 0.5
            }
        }

        self.context_history = []
        self.strategy_usage = {name: 0 for name in self.strategies.keys()}
        self.strategy_results = {name: [] for name in self.strategies.keys()}

    def sift_strategies(self, context):
        """
        Select the most appropriate strategy based on current context
        """
        if not context:
            # Default to exploration if no context
            return {"strategy": "exploration", "reasoning": "No context available, using default strategy"}

        # Store context for learning
        self.context_history.append(context)
        if len(self.context_history) > 100:
            self.context_history = self.context_history[-100:]

        # Extract relevant features
        context_features = self._extract_context_features(context)

        # Score each strategy based on context
        strategy_scores = {}
        for name, strategy in self.strategies.items():
            # Base score
            score = 0.5

            # Context matching
            for feature in context_features:
                if feature in strategy["suitable_for"]:
                    score += 0.1

            # Effectiveness adjustment
            score *= strategy["effectiveness"]

            # Exploration factor to try underused strategies
            usage_ratio = self.strategy_usage[name] / max(1, sum(self.strategy_usage.values()))
            if usage_ratio < 0.1:  # Boost rarely used strategies
                score += 0.2

            # Record strategy score
            strategy_scores[name] = score

        # Find top strategy
        top_strategy = max(strategy_scores.items(), key=lambda x: x[1])

        # Update usage counter
        self.strategy_usage[top_strategy[0]] += 1

        return {
            "strategy": top_strategy[0],
            "reasoning": f"Selected {self.strategies[top_strategy[0]]['name']} (score: {top_strategy[1]:.2f}) based on context features: {', '.join(context_features)}"
        }

    def update_strategy_effectiveness(self, strategy_name, result_data):
        """
        Update effectiveness score for a strategy based on results
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found for effectiveness update.", "WARNING")
            return False

        # Calculate success metrics - make sure these have real values
        content_length = result_data.get("content_length", 0)
        links_discovered = result_data.get("links_discovered", 0)

        # Log the raw values to debug
        log_event(f"Strategy metrics - Length: {content_length}, Links: {links_discovered}", "DEBUG")

        # Calculate success score
        success_score = min(1.0, (content_length / 5000) + (links_discovered / 10))

        # Update effectiveness with exponential moving average
        current = self.strategies[strategy_name].get("effectiveness", 0.5)
        updated = current * 0.8 + success_score * 0.2  # 80% old, 20% new

        # Ensure the new value is different enough to notice
        self.strategies[strategy_name]["effectiveness"] = updated

        log_event(f"Updated effectiveness of {strategy_name} strategy: {current:.2f} → {updated:.2f} (score: {success_score:.2f})", "INFO")
        return True

    def get_optimal_actions(self, strategy_name):
        """
        Get optimal actions for a strategy, with fallback for unknown strategies
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using default actions.", "WARNING")
            return ["expand", "search"]  # Default actions

        return self.strategies[strategy_name]["actions"]

    def _extract_context_features(self, context):
        """Extract features from context for strategy selection"""
        features = []

        # Domain familiarity
        if "domain_visits" in context:
            current_domain = context.get("current_domain", "")
            if current_domain in context["domain_visits"]:
                if context["domain_visits"][current_domain] > 5:
                    features.append("familiar_domains")
                else:
                    features.append("new_domains")

        # Check for limited knowledge
        if "domains_visited" in context and len(context.get("domains_visited", [])) < 10:
            features.append("limited_knowledge")

        # Check if current goal involves synthesis
        if "current_goal" in context:
            goal_desc = str(context["current_goal"]).lower()

            if "connect" in goal_desc or "integrat" in goal_desc or "synthe" in goal_desc:
                features.append("cross_domain")
                features.append("synthesis")

            if "deep" in goal_desc or "detail" in goal_desc:
                features.append("specialized_topics")

            if "optim" in goal_desc or "improve" in goal_desc:
                features.append("optimization_needed")

            if "creat" in goal_desc or "novel" in goal_desc or "new" in goal_desc:
                features.append("creativity_needed")

            # Add scientific domain feature
            if "scientific" in goal_desc or "science" in goal_desc or "research" in goal_desc:
                features.append("scientific_domains")

            # Add critical thinking feature
            if "evaluat" in goal_desc or "assess" in goal_desc or "critic" in goal_desc:
                features.append("critical_thinking")

            # Add discovery phase feature
            if "discover" in goal_desc or "explor" in goal_desc:
                features.append("discovery_phase")

        # Check recent performance
        if "recent_actions" in context:
            recent = context["recent_actions"]
            if any(a.get("content_length", 0) < 1000 for a in recent):
                features.append("performance_issues")

        # Add thinking mode as feature if available
        if "thinking_mode" in context:
            if context["thinking_mode"] == "quantum":
                features.append("quantum_thinking")
            elif context["thinking_mode"] == "creative":
                features.append("creativity_needed")
            elif context["thinking_mode"] == "analytical":
                features.append("specialized_topics")

        return features

    def generate_xoxo_plan(self, strategy_name, context):
        """Generates an XOXO plan for a given strategy and context."""
        if strategy_name not in self.strategies:
            strategy_name = "exploration"  # Default strategy if not found
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using 'exploration' for XOXO plan.", "WARNING")

        strategy = self.strategies[strategy_name]
        actions = strategy["actions"]
        steps = []
        emojis = ["🔍", "🧠", "🔮", "🚀", "✨", "🔄", "📊", "🌟", "💎", "⚡"]

        # Strategy-specific steps
        if strategy_name == "exploration" or strategy_name == "broad_exploration":
            steps = [
                "Discover new domains with high information value",
                "Focus on breadth over depth in domain exploration",
                "Collect diverse content sources across the web",
                "Map the knowledge landscape to identify key areas",
                "Identify promising areas for deeper investigation"
            ]
        elif strategy_name == "deepening" or strategy_name == "depth_first":
            steps = [
                "Focus on specific knowledge domain",
                "Extract detailed information and nuanced insights",
                "Connect related concepts within domain",
                "Build hierarchical understanding of the domain",
                "Identify core principles and patterns"
            ]
        elif strategy_name == "connecting" or strategy_name == "connect_domains":
            steps = [
                "Identify similarities across domains",
                "Create cross-domain concept maps",
                "Look for shared principles",
                "Synthesize insights from different areas",
                "Build higher-level abstractions"
            ]
        elif strategy_name == "quantum" or strategy_name == "quantum_reasoning":
            steps = [
                "Maintain multiple hypotheses simultaneously",
                "Explore non-obvious connections",
                "Use probabilistic thinking",
                "Apply creative leaps in reasoning",
                "Allow for superposition of concepts"
            ]
        elif strategy_name == "adaptive" or strategy_name == "evaluate_sources":
            steps = [
                "Optimize learning parameters",
                "Refine content filtering approach",
                "Adjust exploration/exploitation balance",
                "Enhance memory organization",
                "Improve processing efficiency"
            ]
        elif strategy_name == "creative_synthesis":
            steps = [
                "Generate novel combinations of concepts",
                "Explore unexpected connections between domains",
                "Apply metaphorical thinking to problems",
                "Recombine existing elements in new ways",
                "Create emergent properties through synthesis"
            ]
        else:
            # Generic steps for other strategies
            steps = [
                "Analyze current knowledge state",
                "Identify optimal pathways for exploration",
                "Apply strategic thinking to resource allocation",
                "Evaluate information quality and relevance",
                "Integrate new knowledge into existing structure"
            ]

        plan_steps = []
        for i, step in enumerate(steps):
            emoji = emojis[i % len(emojis)]
            plan_steps.append(f"{emoji} **Step {i+1}:** {step}")

        action_text = ", ".join([f"*{action}*" for action in actions])
        emoji_sparkles = "✨"
        emoji_rocket = "🚀"

        # Create the XOXO plan
        xoxo_plan = f"**XOXO Plan: {strategy['name']} Strategy** {emoji_sparkles}\n\n"
        xoxo_plan += f"*{strategy['description']}*\n\n"
        xoxo_plan += "\n".join(plan_steps) + "\n\n"
        xoxo_plan += f"**Recommended Actions:** {action_text} {emoji_rocket}\n"
        return xoxo_plan





# =============================================================================
# CONTENT SIFTER MODULE
# =============================================================================
class ContentSifter:

    def __init__(self):
        self.quality_thresholds = {
            "min_content_length": 500,
            "min_text_density": 0.3,
            "max_ad_density": 0.2,
            "min_readability_score": 50
        }
        self.topics_of_interest = [
            "artificial intelligence", "machine learning", "quantum computing",
            "neural networks", "deep learning", "data science", "technology",
            "research", "science", "programming", "algorithms", "knowledge"
        ]
        self.content_fingerprints = {}  # Store fingerprints to avoid duplicates

    def evaluate_content_quality(self, content, url=None):
        """Rate content quality based on multiple metrics"""
        if not content:
            return {"score": 0, "reason": "Empty content"}

        # Basic metrics
        total_length = len(content)
        if total_length < self.quality_thresholds["min_content_length"]:
            return {"score": 0.1, "reason": "Content too short"}

        # Check text density
        text_density = self._calculate_text_density(content)
        if text_density < self.quality_thresholds["min_text_density"]:
            return {"score": 0.3, "reason": "Low text density"}

        # Check for potential ads
        ad_density = self._estimate_ad_density(content)
        if ad_density > self.quality_thresholds["max_ad_density"]:
            return {"score": 0.4, "reason": "High ad density"}

        # Calculate readability
        readability = self._calculate_readability(content)
        if readability < self.quality_thresholds["min_readability_score"]:
            return {"score": 0.5, "reason": "Low readability"}

        # Check relevance to topics of interest
        topic_relevance = self._calculate_topic_relevance(content)

        # Check for duplicate content
        if url:
            fingerprint = self._generate_content_fingerprint(content)
            if fingerprint in self.content_fingerprints.values():
                return {"score": 0.2, "reason": "Duplicate content"}

            # Store the fingerprint
            self.content_fingerprints[url] = fingerprint

        # Calculate final score
        base_score = 0.6
        final_score = min(0.95, base_score +
                         (0.1 if total_length > 2000 else 0) +
                         (0.2 * topic_relevance))

        return {
            "score": final_score,
            "metrics": {
                "length": total_length,
                "text_density": text_density,
                "ad_density": ad_density,
                "readability": readability,
                "topic_relevance": topic_relevance
            },
            "reason": "High quality content" if final_score > 0.8 else "Medium quality content"
        }

    def extract_key_information(self, content):
        """Extract important information from content"""
        if not content or len(content) < 500:
            return {"summary": "Content too short for extraction", "entities": []}

        # Extract potential entities
        entities = self._extract_entities(content)

        # Extract potential key sentences
        sentences = re.split(r'(?<=[.!?])\s+', content)

        # Score sentences
        scored_sentences = []
        for sentence in sentences:
            # Skip very short sentences
            if len(sentence) < 40:
                continue

            # Score based on keywords
            keyword_count = sum(1 for topic in self.topics_of_interest
                              if topic.lower() in sentence.lower())

            # Score based on position (earlier is better)
            position_score = 1.0 - (sentences.index(sentence) / max(1, len(sentences)))

            # Calculate final score
            score = (keyword_count * 0.6) + (position_score * 0.4)

            scored_sentences.append((sentence, score))

        # Sort and select top sentences
        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:5]

        return {
            "summary": " ".join([s[0] for s in top_sentences]),
            "entities": entities[:10]  # Top 10 entities
        }

    def _calculate_text_density(self, content):
        """Calculate the ratio of text to HTML/markup"""
        if not content:
            return 0

        # Remove HTML tags if present
        text_only = re.sub(r'<[^>]+>', ' ', content)
        text_only = re.sub(r'\s+', ' ', text_only).strip()

        return len(text_only) / max(1, len(content))

    def _estimate_ad_density(self, content):
        """Estimate the density of advertisements"""
        if not content:
            return 0

        # Look for common ad-related terms
        ad_terms = [
            "advertisement", "sponsor", "promoted", "buy now", "limited offer",
            "discount", "sale", "click here", "banner", "popup"
        ]

        ad_count = sum(content.lower().count(term) for term in ad_terms)

        # Count potential ad-related HTML elements
        ad_elements = len(re.findall(r'<div[^>]*(?:ad|banner|sponsor|promo)[^>]*>', content, re.I))

        # Normalize by content length
        return min(1.0, (ad_count + ad_elements * 2) / max(1, len(content) / 1000))

    def _calculate_readability(self, content):
        """Calculate a readability score"""
        if not content or len(content) < 100:
            return 0

        # Basic implementation of the Flesch Reading Ease formula

        # Clean text
        text = re.sub(r'<[^>]+>', ' ', content)  # Remove HTML
        text = re.sub(r'[^\w\s.]', '', text)     # Keep only words, spaces, periods
        text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

        # Count sentences
        sentences = len(re.findall(r'[.!?]+', text))

        # Count words
        words = len(text.split())

        # Count syllables (very rough approximation)
        syllables = sum(self._count_syllables(word) for word in text.split())

        # Calculate readability (simplified Flesch formula)
        if words == 0 or sentences == 0:
            return 0

        words_per_sentence = words / max(1, sentences)
        syllables_per_word = syllables / max(1, words)

        return max(0, min(100, 206.835 - (1.015 * words_per_sentence) - (84.6 * syllables_per_word)))

    def _count_syllables(self, word):
        """Very basic syllable counter"""
        word = word.lower()
        if len(word) <= 3:
            return 1

        # Count vowel groups
        vowels = "aeiouy"
        count = 0
        prev_is_vowel = False

        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_is_vowel:
                count += 1
            prev_is_vowel = is_vowel

        # Adjust for common patterns
        if word.endswith('e'):
            count -= 1
        if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
            count += 1
        if count == 0:
            count = 1

        return count

    def _calculate_topic_relevance(self, content):
        """Calculate relevance to topics of interest"""
        if not content:
            return 0

        content_lower = content.lower()
        matches = sum(content_lower.count(topic) for topic in self.topics_of_interest)

        # Normalize by content length
        normalized_matches = matches / max(1, len(content) / 1000)

        return min(1.0, normalized_matches / 5)  # Cap at 1.0

    def _extract_entities(self, content):
        """Extract potential named entities"""
        if not content:
            return []

        # Very simple entity extraction
        # In a real system, use NER models

        # Look for capitalized word sequences
        entities = re.findall(r'(?<![.?!])\s([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,5})', content)

        # Look for technical terms
        tech_terms = [
            "algorithm", "neural network", "machine learning", "deep learning",
            "artificial intelligence", "quantum", "data science", "transformer",
            "reinforcement learning", "natural language processing", "computer vision"
        ]

        for term in tech_terms:
            if term in content.lower():
                entities.append(term.title())

        return list(set(entities))  # Remove duplicates

    def _generate_content_fingerprint(self, content):
        """Generate a fingerprint to identify similar content"""
        if not content:
            return ""

        # Clean the content
        cleaned = re.sub(r'<[^>]+>', ' ', content)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()

        # Get the most meaningful words
        words = cleaned.split()
        if len(words) > 100:
            # Use a sample of words from beginning, middle and end
            sample = words[:30] + words[len(words)//2-15:len(words)//2+15] + words[-30:]
            cleaned = " ".join(sample)

        # Create hash
        return hashlib.md5(cleaned.encode()).hexdigest()

# =============================================================================
# FREE WILL MODULE
# =============================================================================
class SuperQuantumFreeWill:
    """
    Advanced free will module with quantum-inspired decision making
    and adaptive exploration strategies.
    """
    def __init__(self, agent):
        self.agent = agent
        self.semantic_memory = {}
        self.domain_intelligence = DomainIntelligence()
        self.memory_set = set()
        self.consciousness_link = None

        # Decision dynamics
        self.exploration_weight = 0.6
        self.exploitation_weight = 0.4
        self.domain_diversity_weight = 0.3
        self.goal_relevance_weight = 0.5
        self.quantum_influence_weight = 0.4

        # Personality traits
        self.personality = {
            "curiosity": 0.9,
            "depth_preference": 0.7,
            "risk_taking": 0.65,
            "patience": 0.5,
            "creativity": 0.8
        }

        # Memory weighting
        self.memory_importance = {}

        # Get planner access
        self.temporal_planner = None
        if hasattr(agent, "ai_manager"):
            self.temporal_planner = getattr(agent.ai_manager, "temporal_planner", None)

        # Fallback URLs for when memory is empty
        self.fallback_urls = [
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://news.ycombinator.com/",
            "https://github.com/explore",
            "https://arxiv.org/list/cs.AI/recent",
            "https://www.nature.com/",
            "https://www.reddit.com/r/science/"
        ]

        log_event("SuperQuantumFreeWill initialized", "QUANTUM")

    def link_consciousness(self, consciousness_module):
        """Connect to consciousness module for reflective capabilities"""
        self.consciousness_link = consciousness_module
        log_event("FreeWill linked with ConsciousnessModule", "INFO")

    def _get_active_goal_description(self):
        """Safely retrieve the active goal description"""
        try:
            if self.temporal_planner is not None and hasattr(self.temporal_planner, "select_active_goal"):
                goal = self.temporal_planner.select_active_goal()
                if goal and isinstance(goal, dict):
                    return goal.get("description", "")
            return ""
        except Exception as e:
            log_event(f"Error retrieving goal description: {e}", "ERROR")
            return ""

    def select_url(self):
        """
        Select the next URL to visit using quantum-inspired
        decision making with multiple factors
        """
        log_event("Selecting URL with quantum-inspired strategy", "QUANTUM")

        # Get candidate URLs from memory or use fallbacks
        candidate_urls = list(self.memory_set)
        if not candidate_urls:
            log_event("Memory set is empty. Using fallback URLs.", "WARNING")
            candidate_urls = self.fallback_urls

        # Get consciousness level if available
        awareness_level = 0.5
        if self.consciousness_link:
            awareness_level = self.consciousness_link.awareness_level

        # Get current goal
        current_goal_description = self._get_active_goal_description()

        # Calculate scores with quantum influence
        url_scores = {}

        for url in candidate_urls:
            # Parse domain
            domain = urlparse(url).netloc

            # Base score with quantum randomness
            # Higher consciousness reduces quantum randomness
            quantum_factor = self.quantum_influence_weight * (1 - awareness_level)

            # Quantum superposition of initial states
            quantum_states = []
            for _ in range(3):  # Generate 3 possible quantum states
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                state_score = amplitude * math.cos(phase)
                quantum_states.append(state_score)

            # Collapse quantum states weighted by consciousness
            quantum_score = sum(quantum_states) / len(quantum_states)
            score = quantum_score * quantum_factor

            # Add domain diversity factor
            domain_visits = self.agent.stats.get("domain_stats", {}).get(domain, {}).get("visits", 0)
            domain_diversity_score = 1.0 / (1 + domain_visits)
            score += self.domain_diversity_weight * domain_diversity_score

            # Check goal relevance - exact matches and semantic similarity
            if current_goal_description:
                # Direct keyword matching
                if (current_goal_description.lower() in url.lower() or
                    current_goal_description.lower() in domain.lower()):
                    score += self.goal_relevance_weight * 0.7

                # Domain-specific boosts based on goal types
                if "explore" in current_goal_description.lower():
                    if domain not in self.agent.stats.get("domains_visited", set()):
                        score += self.goal_relevance_weight * 0.5
                elif "deep" in current_goal_description.lower():
                    if ".edu" in domain or "research" in domain:
                        score += self.goal_relevance_weight * 0.6

            # Apply personality factors
            if "blog" in url.lower() or "forum" in url.lower():
                score += self.personality["curiosity"] * 0.2
            if "research" in url.lower() or "paper" in url.lower() or "edu" in domain:
                score += self.personality["depth_preference"] * 0.3
            if random.random() < self.personality["risk_taking"]:
                score += random.uniform(0, 0.5)  # Occasional boost

            # Add memory importance if this URL is known
            if url in self.memory_importance:
                score += self.memory_importance[url] * 0.4

            # Store score
            url_scores[url] = score

        # Occasionally make quantum leap decision
        if random.random() < self.quantum_influence_weight * 0.3:
            # Complete quantum randomness - ignore calculated scores
            quantum_choice = random.choice(candidate_urls)
            log_event(f"Made quantum leap URL choice: {quantum_choice}", "QUANTUM")
            self.agent.stats["last_url"] = quantum_choice
            return quantum_choice

        # Normal selection based on scores
        if url_scores:
            try:
                best_url = max(url_scores.items(), key=lambda x: x[1])[0]
                log_event(f"Selected URL: {best_url} with score: {url_scores[best_url]:.2f}", "INFO")
                self.agent.stats["last_url"] = best_url
                return best_url
            except Exception as e:
                log_event(f"Error selecting best URL: {e}", "ERROR")
                fallback = random.choice(candidate_urls)
                log_event(f"Using fallback URL selection: {fallback}", "WARNING")
                self.agent.stats["last_url"] = fallback
                return fallback
        else:
            # No scores calculated - use random selection
            fallback = random.choice(candidate_urls)
            log_event(f"No URL scores available. Using random selection: {fallback}", "WARNING")
            self.agent.stats["last_url"] = fallback
            return fallback

    def discover_links(self, html_content, base_url):
        """Discover and extract links from HTML content"""
        links, details = enhanced_link_discovery(html_content, base_url)

        # Filter for high-quality links
        if details:
            high_quality_links = [link_data['url'] for link_data in details if link_data['quality_score'] > 0.6]
            if high_quality_links:
                log_event(f"Discovered {len(high_quality_links)} high-quality links (quality > 0.6)", "INFO")

                # Add to memory
                self.expand_memory(high_quality_links)

                return len(high_quality_links)

        # Add all discovered links to memory
        self.expand_memory(links)

        log_event(f"Discovered {len(links)} links (using basic quality filter)", "INFO")
        return len(links)

    def store_semantic_content(self, url, content):
        """Store content with semantic encoding for future reference"""
        # Create semantic memory module if needed
        semantic_module = SemanticMemoryModule()
        semantic_module.store_semantic_content(url, content)

        # Store in this instance's semantic memory
        self.semantic_memory[url] = semantic_module.semantic_memory.get(url, {})

    def decide(self):
        """
        Make a decision about what action to take next using
        quantum-inspired decision making
        """
        try:
            log_event("Making quantum-enhanced decision...", "QUANTUM")

            # Possible actions
            possible_actions = ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]

            # Initialize quantum state - each action has amplitude and phase
            quantum_state = {}
            for action in possible_actions:
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                quantum_state[action] = {"amplitude": amplitude, "phase": phase}

            # Get current context
            current_goal_description = self._get_active_goal_description()

            # Get consciousness awareness level
            awareness_level = 0.5
            if self.consciousness_link:
                awareness_level = self.consciousness_link.awareness_level

            # Get thinking mode if available
            thinking_mode = "balanced"
            if hasattr(self.agent, "ai_manager") and hasattr(self.agent.ai_manager, "autonomous_mind"):
                thinking_mode = getattr(self.agent.ai_manager.autonomous_mind, "current_mode", "balanced")

            # Apply quantum interference based on context

            # 1. Goal-based interference
            if current_goal_description:
                if "explore" in current_goal_description.lower():
                    # Amplify exploration actions
                    quantum_state["expand"]["amplitude"] *= 1.3
                    quantum_state["search"]["amplitude"] *= 1.2
                elif "deep" in current_goal_description.lower():
                    # Amplify deepening actions
                    quantum_state["evaluate"]["amplitude"] *= 1.4
                    quantum_state["adapt"]["amplitude"] *= 1.2

            # 2. Thinking mode interference
            if thinking_mode == "analytical":
                quantum_state["evaluate"]["amplitude"] *= 1.3
                quantum_state["search"]["amplitude"] *= 1.2
            elif thinking_mode == "creative":
                quantum_state["quantum_leap"]["amplitude"] *= 1.5
                quantum_state["expand"]["amplitude"] *= 1.2
            elif thinking_mode == "critical":
                quantum_state["adapt"]["amplitude"] *= 1.3
                quantum_state["evaluate"]["amplitude"] *= 1.2

            # 3. Recent actions interference - FIXED INDEXING
            recent_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 3: # CHECK if action_log has at least 3 elements
                    recent_actions = [a.get("action", "") for a in list(self.agent.action_log)[-3:]] # Safe list conversion and slice

            if recent_actions:
                # Avoid repeating the same action too many times
                most_common = max(set(recent_actions), key=recent_actions.count) if recent_actions else None
                if most_common is not None: # CHECK if most_common is NOT None
                    if most_common in quantum_state: # CHECK if most_common is a valid key
                        quantum_state[most_common]["amplitude"] *= 0.7

            # 4. Success-based amplification - FIXED INDEXING
            successful_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 5: # CHECK if action_log has at least 5 elements
                    successful_actions = [a.get("action", "") for a in list(self.agent.action_log)[-5:] # Safe list conversion and slice
                                         if a.get("content_length", 0) > 1000]

            if successful_actions:
                last_success = successful_actions[-1]  # Access last element safely now
                if last_success in quantum_state:  # Safely check dictionary key
                    quantum_state[last_success]["amplitude"] *= 1.2

            # Apply consciousness as quantum observer effect
            # Higher consciousness makes decision more deterministic
            for action in possible_actions:
                random_factor = random.uniform(0.8, 1.2) * (1 - awareness_level)
                quantum_state[action]["amplitude"] *= (1 + random_factor * 0.2)

            # Calculate probabilities (square of amplitudes)
            total_probability = sum(state["amplitude"]**2 for state in quantum_state.values())
            probabilities = {action: (state["amplitude"]**2) / total_probability
                           for action, state in quantum_state.items()}

            # Collapse the quantum state by observation
            action_type = random.choices(
                list(probabilities.keys()),
                weights=list(probabilities.values()),
                k=1
            )[0]

            # Create decision package
            decision = {
                "action": action_type,
                "quantum_confidence": probabilities[action_type],
                "reasoning": f"Quantum decision process ({thinking_mode} mode) - goal: {current_goal_description[:30]}",
                "timestamp": datetime.now().isoformat()
            }

            log_event(f"Decision: {action_type} with {probabilities[action_type]:.2f} quantum confidence", "QUANTUM")

            # Special handling for quantum_leap action
            if action_type == "quantum_leap":
                # Map to a standard action but with more randomness
                standard_actions = ["search", "expand", "adapt", "reconnect", "evaluate"]
                decision["action"] = random.choice(standard_actions)
                decision["quantum_leap"] = True
                log_event(f"Quantum leap mapped to {decision['action']}", "QUANTUM")

            return decision

        except Exception as e:
            log_event(f"Decision error: {e}", "ERROR")
            # Fallback to simple random choice
            fallback_action = random.choice(["expand", "search", "adapt"])
            return {"action": fallback_action, "error": str(e)[:200]}
# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
class AIManager:
    """
    Central management system for the autonomous agent, coordinating
    decision-making, planning, and evolution.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model

        # Core subsystems
        self.temporal_planner = TemporalPlanner()
        self.autonomous_mind = AutonomousMind(agent, model)
        self.consciousness = ConsciousnessModule(agent)
        self.imagination = ImaginationEngine()

        # Self-improvement systems
        self.meta_learning = MetaLearningModule(model)
        self.evolution_engine = MetaEvolutionEngine()

        # Operational tracking
        self.cycle_counter = 0
        self.last_evolution_attempt = 0
        self.evolution_interval = 50
        self.error_recovery_attempts = 0

        # Initialize subsystems
        self.temporal_planner.initialize_goals()

        # Connect subsystems
        if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "link_consciousness"):
            self.agent.free_will.link_consciousness(self.consciousness)

        log_event("AIManager initialized with all autonomous systems", "INFO")


    async def run_cycle(self, optimizer=None):
        """Run a complete autonomous cycle with enhanced error handling"""
        self.cycle_counter += 1
        log_event(f"=== Enhanced Autonomous Cycle {self.cycle_counter} ===", "INFO")

        try:
            # 1. Perception - get environment state
            try:
                observation = self.agent.perceive()
            except Exception as e:
                log_event(f"Error in perception phase: {str(e)}", "ERROR")
                observation = {"error": str(e)}  # Minimal fallback observation

            # 2. Consciousness reflection with error handling
            try:
                if hasattr(self, 'consciousness'):
                    self.consciousness.reflect(observation)
            except Exception as e:
                log_event(f"Error in consciousness reflection: {str(e)}", "ERROR")
                # Continue even if reflection fails

            # 3. Decision making with fallbacks
            base_decision = {"action": "expand"}  # Default fallback
            try:
                if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "decide"):
                    decision = self.agent.free_will.decide()
                    if isinstance(decision, dict) and "action" in decision:
                        base_decision = decision
            except Exception as e:
                log_event(f"Decision error: {str(e)}. Using fallback decision.", "ERROR")

            # 4. Planning with error handling
            try:
                if hasattr(self, 'temporal_planner') and self.temporal_planner:
                    full_plan = self.temporal_planner.plan_action(
                        base_decision.get("action", "expand"),
                        observation
                    )
                else:
                    # Fallback simple plan if no temporal planner
                    full_plan = {
                        "action": base_decision.get("action", "expand"),
                        "strategy": "fallback_strategy",
                        "goal": "Continue system operation"
                    }
            except Exception as e:
                log_event(f"Planning error: {str(e)}. Using simplified plan.", "ERROR")
                # Create minimal plan on failure
                full_plan = {
                    "action": base_decision.get("action", "expand"),
                    "strategy": "emergency_strategy",
                    "goal": "Recover from planning failure"
                }

            # 5. Imagination - simulate outcomes (non-critical)
            try:
                if hasattr(self, 'imagination') and random.random() < 0.2:
                    self.imagination.simulate_creation()
            except Exception as e:
                log_event(f"Non-critical error in imagination: {str(e)}", "WARNING")

            # 6. Execute action with timeout protection
            try:
                # Set a timeout for execution to prevent hanging
                action_task = asyncio.create_task(
                    asyncio.to_thread(self.agent.act, full_plan, optimizer)
                )
                action_successful = await asyncio.wait_for(action_task, timeout=60.0)
            except asyncio.TimeoutError:
                log_event("Action execution timed out after 60 seconds", "ERROR")
                action_successful = False
            except Exception as e:
                log_event(f"Action execution error: {str(e)}", "ERROR")
                action_successful = False

            # After action execution is successful:
            if action_successful and hasattr(self.agent, 'planner_sifter'):
                strategy_name = full_plan.get("strategy", "exploration")
                result_data = {
                    "content_length": self.agent.action_log[-1].get("content_length", 0),
                    "links_discovered": self.agent.action_log[-1].get("links_discovered", 0),
                    "success": action_successful
                }
                self.agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)

            # 7. Performance assessment
            performance_metrics = {
                "success": action_successful,
                "content_length": self.agent.action_log[-1].get("content_length", 0) if self.agent.action_log else 0,
                "links_discovered": self.agent.action_log[-1].get("links_discovered", 0) if self.agent.action_log else 0,
                "cycle": self.cycle_counter
            }

            # 8. Reflection and adaptation (non-critical)
            try:
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.reflect_and_adapt(performance_metrics)
            except Exception as e:
                log_event(f"Non-critical error in reflection: {str(e)}", "WARNING")

            # 9. Error detection (non-critical)
            try:
                if hasattr(self, 'imagination'):
                    error_details = self.imagination.simulate_error_detection()
                    if error_details:
                        # Apply correction
                        self.imagination.simulate_error_correction(error_details)
            except Exception as e:
                log_event(f"Non-critical error in error detection: {str(e)}", "WARNING")

            # 10. Self-evolution at intervals (non-critical)
            try:
                if (hasattr(self, 'evolution_engine') and
                    self.cycle_counter - getattr(self, 'last_evolution_attempt', 0) >= getattr(self, 'evolution_interval', 50)):
                    log_event("Attempting system evolution...", "INFO")
                    result, message = self.evolution_engine.evolve_system(self.agent)
                    self.last_evolution_attempt = self.cycle_counter
                    log_event(f"Evolution attempt result: {result} - {message}", "INFO")
            except Exception as e:
                log_event(f"Non-critical error in evolution: {str(e)}", "WARNING")

            # 11. Self-refinement (with error handling)
            try:
                self.agent.refine()
            except Exception as e:
                log_event(f"Error in agent refinement: {str(e)}", "ERROR")
                # Don't let refinement errors abort the cycle

            # Reset error recovery counter on success
            self.error_recovery_attempts = 0

            return {
                "cycle": self.cycle_counter,
                "action": full_plan.get("action", "unknown"),
                "strategy": full_plan.get("strategy", "none"),
                "success": action_successful
            }

        except Exception as e:
            # Error recovery
            self.error_recovery_attempts += 1
            log_event(f"CYCLE ERROR: {str(e)}", "ERROR")
            log_event(traceback.format_exc(), "ERROR")

            # Implement progressive recovery strategies
            if self.error_recovery_attempts < 3:
                log_event("Attempting standard error recovery", "WARNING")
            elif self.error_recovery_attempts < 5:
                log_event("Attempting advanced error recovery - resetting system state", "WARNING")
                # Reset consciousness state if available
                if hasattr(self, 'consciousness'):
                    self.consciousness.awareness_level = 0.5
                    self.consciousness.current_state = "balanced"
            else:
                log_event("Critical error threshold reached - emergency recovery", "CRITICAL")
                # Emergency reset of all systems
                if hasattr(self, 'consciousness'):
                    self.consciousness = ConsciousnessModule(self.agent)
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.cycle_count = 0
                    self.temporal_planner.refresh_short_term_goals()

            return {
                "status": "error",
                "cycle": self.cycle_counter,
                "error": str(e),
                "recovery_attempt": self.error_recovery_attempts
            }



# =============================================================================
# TEMPORAL PLANNING AND GOAL MANAGEMENT
# =============================================================================
class TemporalPlanner:
    """
    Advanced planning system that operates across multiple time horizons
    and manages goals with temporal dependencies.
    """
    def __init__(self):
        self.short_term_goals = []
        self.long_term_goals = []
        self.goal_history = []
        self.current_strategy = None
        self.strategy_effectiveness = {}
        self.time_horizon_days = 7
        self.reflection_interval = 20
        self.cycle_count = 0

        # Add strategy mapping to match PlannerSifter strategy names
        self.strategy_mapping = {
            "exploration": "broad_exploration",
            "deepening": "depth_first",
            "integration": "connect_domains",
            "evaluation": "evaluate_sources",
            "quantum": "quantum_reasoning",
            "creative": "creative_synthesis"
        }

    def initialize_goals(self):
        """Set up initial goal structure"""
        self.long_term_goals = [
            {
                "id": "knowledge_diversity",
                "description": "Maximize diversity of knowledge domains",
                "priority": 0.8,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "model_efficiency",
                "description": "Optimize neural architecture for learning efficiency",
                "priority": 0.7,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "content_quality",
                "description": "Improve filtering and processing of high-value content",
                "priority": 0.9,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "quantum_reasoning",
                "description": "Develop quantum-inspired reasoning capabilities",
                "priority": 1.0,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            }
        ]
        self.refresh_short_term_goals()
        log_event("Temporal planner initialized with long-term goals", "INFO")

    def refresh_short_term_goals(self):
        """Generate new short-term goals aligned with long-term objectives"""
        # Save the existing goals that are still valid
        valid_goals = []
        for goal in self.short_term_goals:
            if goal.get("duration", 0) > 0:
                valid_goals.append(goal)

        # Clear the short_term_goals list
        self.short_term_goals = valid_goals

        # Check if we need to generate new goals
        if len(self.short_term_goals) >= 5:
            return  # We still have enough goals

        # Define knowledge domains
        domains = [
            "technical", "scientific", "humanities", "news",
            "reference", "creative", "analytical", "philosophical"
        ]

        # Generate 3-5 new short term goals
        goals_to_generate = min(5, 8 - len(self.short_term_goals))
        for _ in range(goals_to_generate):
            # Randomly select goal type and domain
            goal_type = random.choice(["exploration", "deepening", "integration", "refinement"])
            domain = random.choice(domains)

            # Generate goal based on type
            if goal_type == "exploration":
                goal = {
                    "id": f"explore_{domain}_{int(time.time())}",
                    "description": f"Discover new content sources in {domain}",
                    "priority": random.uniform(0.5, 0.9),
                    "duration": random.randint(10, 30),
                    "type": "exploration",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "deepening":
                goal = {
                    "id": f"deepen_{domain}_{int(time.time())}",
                    "description": f"Build deeper understanding in {domain}",
                    "priority": random.uniform(0.6, 0.95),
                    "duration": random.randint(5, 15),
                    "type": "deepening",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "integration":
                domain2 = random.choice([d for d in domains if d != domain])
                goal = {
                    "id": f"integrate_{domain}_{domain2}_{int(time.time())}",
                    "description": f"Connect knowledge between {domain} and {domain2}",
                    "priority": random.uniform(0.7, 0.9),
                    "duration": random.randint(8, 20),
                    "type": "integration",
                    "domains": [domain, domain2],
                    "created": datetime.now().isoformat()
                }
            else:  # refinement
                goal = {
                    "id": f"refine_{domain}_{int(time.time())}",
                    "description": f"Optimize learning approach in {domain}",
                    "priority": random.uniform(0.5, 0.8),
                    "duration": random.randint(5, 12),
                    "type": "refinement",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }

            self.short_term_goals.append(goal)

        log_event(f"Refreshed short-term goals: {goals_to_generate} new goals created", "INFO")

    def select_active_goal(self):
        """Select the highest priority current goal"""
        # Remove expired goals
        active_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]
        self.short_term_goals = active_goals

        # Decrease duration for all goals
        for goal in self.short_term_goals:
            goal["duration"] = max(0, goal.get("duration", 10) - 1)

        # Refresh if needed
        if not self.short_term_goals:
            self.refresh_short_term_goals()

        # Select highest priority goal
        if self.short_term_goals:
            return max(self.short_term_goals, key=lambda x: x.get("priority", 0))
        else:
            # Default goal if something went wrong
            default_goal = {
                "id": "default_exploration",
                "description": "Default exploration",
                "priority": 0.5,
                "type": "exploration",
                "domain": "reference",  # Provide a default domain
                "duration": 5  # Give it some duration
            }
            self.short_term_goals.append(default_goal)  # Add to goals list for tracking
            return default_goal

    def reflect_and_adapt(self, performance_metrics):
        """Periodically reflect on goal progress and adapt strategies"""
        self.cycle_count += 1

        # Update goal progress based on performance
        if performance_metrics.get("success", False):
            active_goal = self.select_active_goal()

            # Find corresponding long-term goal to update
            for goal in self.long_term_goals:
                # Update based on goal type alignment
                if (active_goal.get("type") == "exploration" and goal["id"] == "knowledge_diversity") or \
                   (active_goal.get("type") == "deepening" and goal["id"] == "content_quality") or \
                   (active_goal.get("type") == "refinement" and goal["id"] == "model_efficiency") or \
                   (active_goal.get("type") == "integration" and goal["id"] == "quantum_reasoning"):
                    # Small progress increment
                    increment = min(0.05, performance_metrics.get("content_length", 0) / 20000)
                    goal["progress"] = min(1.0, goal["progress"] + increment)

            # Record the strategy effectiveness
            strategy = self.current_strategy
            if strategy:
                if strategy not in self.strategy_effectiveness:
                    self.strategy_effectiveness[strategy] = []

                # Score based on content and links
                score = min(1.0, performance_metrics.get("content_length", 0) / 5000 +
                           performance_metrics.get("links_discovered", 0) / 10)
                self.strategy_effectiveness[strategy].append(score)

                # Limit history size
                if len(self.strategy_effectiveness[strategy]) > 20:
                    self.strategy_effectiveness[strategy] = self.strategy_effectiveness[strategy][-20:]

        # Major reflection at intervals
        if self.cycle_count % self.reflection_interval == 0:
            log_event("Performing strategic reflection and adaptation...", "INFO")

            # Analyze strategy effectiveness
            for strategy, metrics in self.strategy_effectiveness.items():
                if metrics:
                    avg_performance = sum(metrics) / len(metrics)
                    log_event(f"Strategy '{strategy}' average performance: {avg_performance:.4f}", "INFO")

            # Adjust long-term goal priorities
            total_adjustment = 0
            for goal in self.long_term_goals:
                # Random adjustment with bias toward less-progressed goals
                bias = 1.0 - goal.get("progress", 0)
                adjustment = random.uniform(-0.1, 0.15) * bias

                goal["priority"] = max(0.1, min(1.0, goal["priority"] + adjustment))
                total_adjustment += abs(adjustment)

            # Sometimes create new evolved goals
            if random.random() < 0.2:
                # Create a new long-term goal
                new_goal_types = [
                    "Develop cognitive synergy across domains",
                    "Optimize information integration pathways",
                    "Enhance quantum processing capabilities",
                    "Improve anomaly detection in knowledge structures",
                    "Develop adaptive learning mechanisms"
                ]

                new_goal_id = f"evolved_goal_{int(time.time())}"
                new_goal = {
                    "id": new_goal_id,
                    "description": f"Evolved objective: {random.choice(new_goal_types)}",
                    "priority": random.uniform(0.7, 0.9),
                    "progress": 0.0,
                    "created": datetime.now().isoformat()
                }

                # Limit total goals
                if len(self.long_term_goals) < 10:  # Prevent too many goals
                    self.long_term_goals.append(new_goal)
                    log_event(f"Created new long-term goal: {new_goal['description']}", "INFO")

            # Clean expired goals from short-term list
            self.short_term_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]

            # Refresh short-term goals
            self.refresh_short_term_goals()

            log_event(f"Reflection complete: adjusted {len(self.long_term_goals)} long-term goals (total Δ: {total_adjustment:.4f})", "INFO")

    def plan_action(self, base_action, environment_state=None):
        """Generate temporal plan based on goals and environment"""
        # Get current active goal
        active_goal = self.select_active_goal()

        # Consider temporal context
        current_time = datetime.now()
        is_weekend = current_time.weekday() >= 5
        is_business_hours = 9 <= current_time.hour <= 17

        # Available strategies with weights
        strategy_options = [
            "broad_exploration",     # Wide but shallow exploration
            "depth_first",           # Deep dive into specific domain
            "connect_domains",       # Look for connections between areas
            "evaluate_sources",      # Focus on quality assessment
            "quantum_reasoning",     # Use quantum processing modes
            "creative_synthesis"     # Generate new insights
        ]

        # Default weights
        weights = [0.2, 0.2, 0.2, 0.15, 0.15, 0.1]

        # Adjust weights based on temporal context
        if is_business_hours and not is_weekend:
            # Business hours - more analytical
            weights = [0.1, 0.2, 0.2, 0.3, 0.1, 0.1]
        elif not is_business_hours:
            # Non-business hours - more exploratory
            weights = [0.3, 0.1, 0.1, 0.1, 0.2, 0.2]

        # Adjust based on goal type
        goal_type = active_goal.get("type", "")
        if goal_type == "exploration":
            # Boost exploration strategies
            weights[0] += 0.2  # More broad_exploration
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "deepening":
            # Boost deepening strategies
            weights[1] += 0.2  # More depth_first
            weights[3] += 0.1  # More evaluate_sources
        elif goal_type == "integration":
            # Boost connection strategies
            weights[2] += 0.2  # More connect_domains
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "refinement":
            # Boost refinement strategies
            weights[3] += 0.2  # More evaluate_sources
            weights[4] += 0.1  # More quantum_reasoning

        # Ensure weights sum to 1
        total = sum(weights)
        weights = [w/total for w in weights]

        # Select strategy
        self.current_strategy = random.choices(strategy_options, weights=weights, k=1)[0]

        # Create plan
        timestamp = current_time.isoformat()
        plan = {
            "action": base_action,
            "goal": active_goal["description"],
            "strategy": self.current_strategy,
            "timestamp": timestamp,
            "execution_context": {
                "is_weekend": is_weekend,
                "is_business_hours": is_business_hours,
                "current_hour": current_time.hour,
                "goal_type": goal_type,
                "goal_domain": active_goal.get("domain", "unknown")
            }
        }

        log_event(f"Generated temporal plan: {plan['action']} using {plan['strategy']} strategy for goal: {plan['goal']}", "INFO")
        return plan

    def _convert_old_strategy_name(self, old_name):
        """Convert old strategy names to new format if needed"""
        if old_name in self.strategy_mapping:
            return self.strategy_mapping[old_name]
        return old_name



# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):
    """
    Implements quantum-inspired attention with superposition of states
    that allows multiple attention pathways to exist simultaneously.
    """
    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperdimensionalEncoder(nn.Module):
    """
    Implements hyperdimensional computing principles for efficient
    high-dimensional representation of concepts.
    """
    def __init__(self, input_dim, hd_dim=1024):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Create random basis vectors
        self.register_buffer('basis', torch.randn(input_dim, hd_dim).sign())

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        # Project input
        x_proj = self.projection(x)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale by the input value
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1)
            # Add to the bundle (vector sum)
            hd_vectors += scaled_basis

        # Binarize to -1/+1 for clean HD representation
        hd_vectors = torch.sign(hd_vectors)

        return hd_vectors

class FractalLayer(nn.Module):
    """
    Self-similar recursive processing layer with dynamic scaling.
    """
    def __init__(self, embed_dim):
        super().__init__()
        self.fractal_scale = nn.Parameter(torch.tensor(1.0))
        self.temperature = nn.Parameter(torch.tensor(1.0))
        self.linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        fractal_contribution = torch.tanh(self.linear(x) / self.temperature)
        return x + self.fractal_scale * fractal_contribution

class QuantumResonanceTensor(nn.Module):
    """
    Implements non-collapsing recursive state resonance that maintains
    multiple simultaneous state representations in quantum-inspired superposition.
    """
    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output

class NeocortexBlock(nn.Module):
    """
    Advanced neural block inspired by neocortical structure with
    multiple processing pathways.
    """
    def __init__(self, embed_dim, num_quantum_states=4):
        super().__init__()
        # Attention for information routing
        self.attention = QuantumAttentionLayer(embed_dim)

        # Parallel processing streams
        self.fractal_stream = FractalLayer(embed_dim)
        self.quantum_stream = QuantumResonanceTensor(embed_dim, num_states=num_quantum_states)

        # Hyperdimensional binding for concept integration
        self.hd_encoder = HyperdimensionalEncoder(embed_dim, hd_dim=embed_dim)

        # Stream integration
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # Process through attention mechanism
        attended = self.attention(x)

        # Process through parallel streams
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Create HD representations from the attended representation
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate all streams
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        integrated = self.integration(combined)

        # Residual connection and normalization
        output = self.norm(x + integrated)

        return output



class AdaptiveLearningSystem:
    """
    Advanced system for dynamically adapting learning parameters and network architecture
    based on performance metrics and environmental feedback.
    """
    def __init__(self, model):
        self.model = model
        self.learning_rate_history = []
        self.performance_metrics = []
        self.architecture_changes = []
        self.adaptation_cycle = 0
        self.min_learning_rate = 1e-6
        self.max_learning_rate = 1e-3
        self.performance_window_size = 10
        self.exploration_rate = 0.3
        self.adaptation_threshold = 0.15
        self.architecture_expansion_threshold = 5

        # Initialize default learning rate on model
        self.default_learning_rate = 5e-5  # Same as initial LEARNING_RATE
        setattr(self.model, '_current_lr', self.default_learning_rate)

        log_event("AdaptiveLearningSystem initialized with dynamic adaptation capabilities", "INFO")

    def adapt_learning_rate(self, metrics):
        """
        Dynamically adjust learning rate based on recent performance metrics
        using a sophisticated control system approach.
        """
        self.adaptation_cycle += 1

        # Get current learning rate with fallback to default
        current_lr = getattr(self.model, '_current_lr', self.default_learning_rate)
        self.learning_rate_history.append(current_lr)

        # Record performance metrics
        if isinstance(metrics, dict):
            self.performance_metrics.append(metrics)

        # Need sufficient history for adaptation
        if len(self.performance_metrics) < self.performance_window_size:
            log_event(f"Building performance history: {len(self.performance_metrics)}/{self.performance_window_size}", "INFO")
            return current_lr

        # Analyze recent performance trend
        recent_metrics = self.performance_metrics[-self.performance_window_size:]

        # Calculate performance derivatives - how fast is loss changing?
        loss_values = [m.get('loss', 0.5) for m in recent_metrics if isinstance(m, dict) and 'loss' in m]
        if not loss_values or len(loss_values) < 3:
            return current_lr

        # First derivative - rate of change
        loss_changes = [loss_values[i] - loss_values[i-1] for i in range(1, len(loss_values))]
        avg_loss_change = sum(loss_changes) / len(loss_changes)

        # Second derivative - acceleration of change
        loss_acceleration = [loss_changes[i] - loss_changes[i-1] for i in range(1, len(loss_changes))]
        avg_loss_acceleration = sum(loss_acceleration) / max(1, len(loss_acceleration))

        # Decision logic for learning rate adjustment
        new_lr = current_lr

        # Case 1: Loss is decreasing quickly (negative change, negative acceleration)
        if avg_loss_change < -0.01 and avg_loss_acceleration < 0:
            new_lr = min(self.max_learning_rate, current_lr * 1.05)
            adjustment_type = "slight increase - good progress"

        # Case 2: Loss is decreasing but slowing down (negative change, positive acceleration)
        elif avg_loss_change < 0 and avg_loss_acceleration >= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.95)
            adjustment_type = "slight decrease - approaching minimum"

        # Case 3: Loss is increasing and accelerating (positive change, positive acceleration)
        elif avg_loss_change > 0.01 and avg_loss_acceleration > 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.7)
            adjustment_type = "major decrease - moving away from minimum"

        # Case 4: Loss is increasing but decelerating (positive change, negative acceleration)
        elif avg_loss_change > 0 and avg_loss_acceleration <= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.85)
            adjustment_type = "moderate decrease - correcting overshoot"

        # Case 5: Stagnation - very small changes
        elif abs(avg_loss_change) < 0.001:
            if random.random() < self.exploration_rate:
                factor = random.uniform(0.5, 1.5)
                new_lr = max(self.min_learning_rate, min(self.max_learning_rate, current_lr * factor))
                adjustment_type = f"random exploration {'increase' if factor > 1 else 'decrease'}"
            else:
                adjustment_type = "no change - minimal fluctuation"
        else:
            adjustment_type = "no change - no clear pattern"

        # Apply the new learning rate with safeguards
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold
            safeguarded_lr = self._apply_learning_rate_safeguards(new_lr)
            setattr(self.model, '_current_lr', safeguarded_lr)
            log_event(f"Learning rate adapted: {current_lr:.6f} → {safeguarded_lr:.6f} ({adjustment_type})", "INFO")

            # Update global optimizer if available
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = safeguarded_lr
                log_event("Applied new learning rate to optimizer", "INFO")

        return new_lr

    def adapt_architecture(self):
        """
        Dynamically modify the network architecture based on performance trends
        and complexity requirements.
        """
        # Can't adapt architecture without sufficient performance history
        if len(self.performance_metrics) < self.performance_window_size * 2:
            return False

        # Check if we're in a stagnation period
        recent_losses = [m.get('loss', 0.5) for m in self.performance_metrics[-self.performance_window_size:]
                        if isinstance(m, dict) and 'loss' in m]

        if not recent_losses or len(recent_losses) < self.performance_window_size:
            return False

        # Calculate performance variance to detect stagnation
        loss_variance = np.var(recent_losses) if 'np' in globals() else sum((x - sum(recent_losses)/len(recent_losses))**2 for x in recent_losses)/len(recent_losses)
        loss_range = max(recent_losses) - min(recent_losses)

        # Check for architecture adaptation conditions
        architecture_change = None

        # Condition 1: Stagnation with low variance - model might be underfitting
        if loss_variance < 0.0001 and loss_range < 0.01 and recent_losses[-1] > 0.1:
            # Model might be underfitting - expand capacity
            if hasattr(self.model, 'expand_architecture'):
                self.model.expand_architecture()
                architecture_change = "expansion - complexity increased due to stagnation"

        # Condition 2: Oscillating with high variance - model might be overfitting
        elif loss_variance > 0.01 and min(recent_losses) < 0.05:
            # Model might be overfitting - simplify
            if hasattr(self.model, 'contract_architecture'):
                self.model.contract_architecture()
                architecture_change = "contraction - complexity reduced due to oscillation"

        # Condition 3: Plateaued at medium-high loss - try random architectural change
        elif 0.0001 <= loss_variance < 0.001 and 0.1 <= recent_losses[-1] < 0.3:
            # Random architectural exploration
            if random.random() < self.exploration_rate:
                if hasattr(self.model, 'expand_architecture') and random.random() < 0.5:
                    self.model.expand_architecture()
                    architecture_change = "random expansion - exploration due to plateau"
                elif hasattr(self.model, 'contract_architecture'):
                    self.model.contract_architecture()
                    architecture_change = "random contraction - exploration due to plateau"

        # Record the change if one was made
        if architecture_change:
            self.architecture_changes.append({
                'cycle': self.adaptation_cycle,
                'type': architecture_change,
                'loss_before': recent_losses[-1] if recent_losses else None
            })
            log_event(f"Architecture adaptation: {architecture_change}", "QUANTUM")
            return True

        return False

    def track_performance(self, metrics):
        """
        Track and analyze performance metrics over time to inform
        meta-learning decisions.
        """
        if not isinstance(metrics, dict):
            return

        # Store metrics
        self.performance_metrics.append(metrics.copy())

        # Keep only the most recent window
        max_history = self.performance_window_size * 5
        if len(self.performance_metrics) > max_history:
            self.performance_metrics = self.performance_metrics[-max_history:]

        # Log significant performance changes
        if len(self.performance_metrics) > 1:
            current = metrics.get('loss', None)
            previous = self.performance_metrics[-2].get('loss', None)

            if current is not None and previous is not None:
                change = current - previous
                percentage = abs(change / max(0.001, previous)) * 100

                if percentage > 10:  # 10% change threshold
                    direction = "improved" if change < 0 else "degraded"
                    log_event(f"Performance {direction} by {percentage:.1f}%: {previous:.4f} → {current:.4f}",
                             "INFO" if direction == "improved" else "WARNING")

    def get_adaptation_report(self):
        """
        Generate a comprehensive report on adaptation history and recommendations.
        """
        if not self.performance_metrics:
            return {"status": "insufficient_data", "recommendations": ["Continue training to build metrics history"]}

        # Analysis results
        adaptation_cycles = len(self.architecture_changes)
        lr_stability = self._calculate_stability(self.learning_rate_history[-20:]) if len(self.learning_rate_history) >= 20 else 0
        performance_trend = self._analyze_performance_trend()

        # Generate recommendations
        recommendations = []

        if adaptation_cycles < 3 and len(self.performance_metrics) > 50:
            recommendations.append("Consider increasing exploration rate to discover better architectures")

        if lr_stability > 0.9:
            recommendations.append("Learning rate highly stable - may indicate stagnation, consider learning rate warm restart")

        if performance_trend == "stagnant" and len(self.performance_metrics) > 30:
            recommendations.append("Performance stagnation detected - consider manual architecture revision or dataset augmentation")

        return {
            "status": "active",
            "adaptation_cycles": adaptation_cycles,
            "lr_stability": lr_stability,
            "performance_trend": performance_trend,
            "recommendations": recommendations
        }

    def _calculate_stability(self, values):
        """Calculate how stable a series of values is (0 = chaotic, 1 = stable)"""
        if not values or len(values) < 2:
            return 1.0

        # Normalize by first value to get relative changes
        normalized = [v / values[0] for v in values]

        # Calculate variance of the normalized values
        mean = sum(normalized) / len(normalized)
        variance = sum((x - mean) ** 2 for x in normalized) / len(normalized)

        # Convert to stability score (inverse of variance, bounded)
        stability = 1.0 / (1.0 + min(10, variance * 100))
        return stability

    def _analyze_performance_trend(self):
        """Analyze the trend in performance metrics"""
        if len(self.performance_metrics) < 10:
            return "insufficient_data"

        # Extract loss values
        losses = [m.get('loss', None) for m in self.performance_metrics[-10:]]
        losses = [l for l in losses if l is not None]

        if len(losses) < 5:
            return "insufficient_data"

        # Calculate improvement rate
        first_window = sum(losses[:3]) / 3  # Average of first 3
        last_window = sum(losses[-3:]) / 3  # Average of last 3

        improvement = (first_window - last_window) / first_window if first_window > 0 else 0

        if improvement > 0.1:
            return "improving"
        elif improvement < -0.05:
            return "degrading"
        else:
            return "stagnant"

    def _apply_learning_rate_safeguards(self, new_lr):
        """Prevent learning rate from spiraling into oblivion"""
        # Establish absolute minimum learning rate
        ABSOLUTE_MIN_LR = 5e-6

        if new_lr < ABSOLUTE_MIN_LR:
            log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
            return ABSOLUTE_MIN_LR

        # Prevent excessive downward adjustment
        if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
            safer_lr = self.learning_rate_history[-1] * 0.8
            log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
            return safer_lr

        return new_lr

    def perform_learning_rate_warmup(self):
        """Occasionally reset learning rate to prevent long-term stagnation"""
        if not self.learning_rate_history:
            return False

        current_lr = self.learning_rate_history[-1]

        # Check for long-term stability and low learning rate
        if (len(self.learning_rate_history) > 50 and
            self._calculate_stability(self.learning_rate_history[-50:]) > 0.95 and
            current_lr < self.max_learning_rate * 0.1):

            # Reset to higher learning rate
            new_lr = current_lr * 5.0
            new_lr = min(self.max_learning_rate * 0.5, new_lr)

            # Apply the new learning rate
            setattr(self.model, '_current_lr', new_lr)
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr

            log_event(f"✨ Learning rate warm restart: {current_lr:.8f} → {new_lr:.8f}", "QUANTUM")
            self.learning_rate_history.append(new_lr)
            return True

        return False


# =============================================================================
# SEMANTIC MEMORY MODULE - UPDATED
# =============================================================================
class SemanticMemoryModule:
    """
    Advanced semantic memory system for encoding, storing, retrieving, and
    reasoning with knowledge representations.
    """
    def __init__(self, dimension=SEMANTIC_MEMORY_DIM, max_memory_size=10000):
        self.semantic_memory = {}
        self.dimension = dimension
        self.max_memory_size = max_memory_size
        self.memory_index = {}  # For fast similarity search
        self.knowledge_graph = {}  # For relational connections
        self.memory_access_counts = {}  # Track memory access frequency
        self.memory_importance = {}  # Track memory importance scores
        self.memory_timestamps = {}  # Track when memories were stored

        # Integration with hyperdimensional computing
        self.hd_basis_vectors = None

        # Initialize sentence embedding model
        self.embedding_model = None # Initialize as None
        self._initialize_embedding_model() # Call initialization method

        # Update dimension based on the loaded model if successful
        if self.embedding_model is not None:
            # For 'all-MiniLM-L6-v2', the dimension is 384
            # This line ensures consistency if SEMANTIC_MEMORY_DIM was different
            self.dimension = self.embedding_model.get_sentence_embedding_dimension() 
            log_event(f"SemanticMemoryModule dimension set to {self.dimension} (from SentenceTransformer)", "INFO")
        else:
            log_event(f"SemanticMemoryModule initialized with fallback dimension {self.dimension}", "INFO")

    def _initialize_embedding_model(self):
        """Initialize the sentence embedding model"""
        try:
            # Use a lightweight but effective model
            # This downloads the model the first time it runs
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            # The dimension will be updated in __init__ after this call.
            log_event("Initialized Sentence-BERT embedding model 'all-MiniLM-L6-v2'", "INFO")
        except Exception as e:
            log_event(f"Failed to initialize Sentence-BERT embedding model: {e}. Falling back to hash-based embedding.", "ERROR")
            self.embedding_model = None # Ensure it's None if loading fails

    def _generate_embedding(self, content):
        """
        Generate semantic embedding for content using Sentence-BERT.
        If Sentence-BERT is not available, falls back to hash-based embedding.
        """
        if not content:
            return np.zeros(self.dimension)
        
        # Use real embeddings if model is available
        if self.embedding_model is not None:
            try:
                # Truncate very long content for efficiency as Sentence-BERT has a token limit
                # A rough estimate for 512 tokens is about 4x the characters (e.g., 2048 characters)
                max_chars = 512 * 4 
                truncated_content = content[:max_chars]
                
                # Generate embedding
                embedding = self.embedding_model.encode(
                    truncated_content,
                    convert_to_numpy=True,
                    normalize_embeddings=True # Ensures unit vectors for cosine similarity
                )
                
                return embedding
            except Exception as e:
                log_event(f"Sentence-BERT embedding generation failed for content. Falling back to hash-based: {e}", "ERROR")
                # Fall through to the hash-based fallback below
        
        # Fallback to original hash-based pseudo-random embedding implementation
        # This will only run if self.embedding_model is None or an error occurred during encode()
        content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest() # Using SHA256 for better hash distribution
        rng = random.Random(content_hash)
        embedding = np.array([rng.uniform(-1, 1) for _ in range(self.dimension)])
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        log_event(f"Using fallback hash-based embedding for content (length: {len(content)})", "DEBUG")
        return embedding

    def _extract_keywords(self, content, max_keywords=10):
        """Extract important keywords from content"""
        if not content:
            return []

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip().lower()

        # Simple word frequency analysis
        words = text.split()

        # Filter stop words (very basic approach)
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
                     'to', 'for', 'with', 'by', 'about', 'as', 'of', 'from',
                     'is', 'it', 'that', 'this', 'have', 'has', 'do', 'don', 
                     'not', 'can', 'will', 'would', 'should', 'if', 'when',
                     'what', 'where', 'why', 'how', 'which', 'who', 'whom',
                     'be', 'been', 'was', 'were', 'are'} # Added more common stop words
        filtered_words = [w for w in words if w not in stop_words and len(w) > 2] # Min length 2 now

        # Count word frequencies
        word_counts = {}
        for word in filtered_words:
            word_counts[word] = word_counts.get(word, 0) + 1

        # Sort by count
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

        # Return top keywords
        return [word for word, count in sorted_words[:max_keywords]]

    def _summarize_content(self, content, max_length=200):
        """Generate a simple summary of content"""
        if not content or len(content) <= max_length:
            return content

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip()

        # Extract sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        # Simple heuristic: take first few sentences
        summary = ""
        for sentence in sentences:
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + " "
            else:
                break

        return summary.strip()

    def store_semantic_content(self, url, content):
        """
        Encode and store semantic representation of content with
        rich metadata and relational information.
        """
        if not content or not url:
            return False

        # Generate semantic embedding
        embedding = self._generate_embedding(content)

        # Extract keywords for improved retrieval
        keywords = self._extract_keywords(content)

        # Generate summary
        summary = self._summarize_content(content)

        # Parse domain information
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        # Calculate content importance score (heuristic)
        # Importance score can now also be influenced by embedding quality if desired
        importance_score = min(1.0, len(content) / 20000 + len(keywords) / 20)

        # Store comprehensive memory entry
        memory_entry = {
            "url": url,
            "domain": domain,
            "embedding": embedding, # This will now be a true semantic embedding or fallback
            "content_summary": summary,
            "keywords": keywords,
            "importance": importance_score,
            "content_length": len(content),
            "timestamp": datetime.now().isoformat()
        }

        # Check for memory overflow - manage if too large
        if len(self.semantic_memory) >= self.max_memory_size:
            self._prune_least_important_memories()

        # Store the memory
        self.semantic_memory[url] = memory_entry
        self.memory_importance[url] = importance_score
        self.memory_timestamps[url] = datetime.now().isoformat()
        self.memory_access_counts[url] = 0

        # Update memory index for faster similarity search
        self.memory_index[url] = embedding

        # Update knowledge graph with relations
        self._update_knowledge_graph(url, keywords, domain)

        log_event(f"Stored semantic memory for {url} with {len(keywords)} keywords", "INFO")
        return True

    def _update_knowledge_graph(self, url, keywords, domain):
        """Update knowledge graph with new relations"""
        if domain not in self.knowledge_graph:
            self.knowledge_graph[domain] = {"urls": set(), "keywords": set()}

        # Add URL to domain
        self.knowledge_graph[domain]["urls"].add(url)

        # Add keywords to domain
        self.knowledge_graph[domain]["keywords"].update(set(keywords))

        # Create keyword nodes if needed
        for keyword in keywords:
            if keyword not in self.knowledge_graph:
                self.knowledge_graph[keyword] = {"urls": set(), "domains": set()}

            # Add relations
            self.knowledge_graph[keyword]["urls"].add(url)
            self.knowledge_graph[keyword]["domains"].add(domain)

    def _prune_least_important_memories(self):
        """Remove least important memories when reaching capacity"""
        if len(self.semantic_memory) <= self.max_memory_size * 0.9:
            return  # No need to prune yet

        # Calculate combined importance score
        combined_scores = {}
        current_time = datetime.now()

        for url, memory in self.semantic_memory.items():
            # Base importance
            score = memory.get("importance", 0.5)

            # Adjust by access frequency
            access_count = self.memory_access_counts.get(url, 0)
            score += min(0.3, access_count / 10)

            # Adjust by recency (decay older memories)
            timestamp = self.memory_timestamps.get(url)
            if timestamp:
                try:
                    stored_time = datetime.fromisoformat(timestamp)
                    age_hours = (current_time - stored_time).total_seconds() / 3600
                    recency_factor = math.exp(-age_hours / 720)  # Decay over ~30 days
                    score *= recency_factor
                except:
                    pass  # Use base score if timestamp parsing fails

            combined_scores[url] = score

        # Sort by score
        sorted_urls = sorted(combined_scores.items(), key=lambda x: x[1])

        # Remove lowest scoring items
        to_remove = int(self.max_memory_size * 0.2)  # Remove 20%
        for url, score in sorted_urls[:to_remove]:
            self._remove_memory(url)

        log_event(f"Memory pruned: removed {to_remove} low-importance items", "INFO")

    def _remove_memory(self, url):
        """Remove a memory and all its references"""
        if url in self.semantic_memory:
            # Get memory details for cleanup
            memory = self.semantic_memory[url]
            domain = memory.get("domain")
            keywords = memory.get("keywords", [])

            # Remove from main memory
            del self.semantic_memory[url]

            # Remove from index
            if url in self.memory_index:
                del self.memory_index[url]

            # Remove from tracking dicts
            if url in self.memory_access_counts:
                del self.memory_access_counts[url]
            if url in self.memory_importance:
                del self.memory_importance[url]
            if url in self.memory_timestamps:
                del self.memory_timestamps[url]

            # Clean up knowledge graph
            self._remove_from_knowledge_graph(url, domain, keywords)

    def _remove_from_knowledge_graph(self, url, domain, keywords):
        """Remove all references to URL from knowledge graph"""
        # Remove from domain node
        if domain and domain in self.knowledge_graph:
            if "urls" in self.knowledge_graph[domain]:
                self.knowledge_graph[domain]["urls"].discard(url)

            # Remove domain if empty
            if not self.knowledge_graph[domain]["urls"]:
                del self.knowledge_graph[domain]

        # Remove from keyword nodes
        for keyword in keywords:
            if keyword in self.knowledge_graph:
                if "urls" in self.knowledge_graph[keyword]:
                    self.knowledge_graph[keyword]["urls"].discard(url)
                if domain and "domains" in self.knowledge_graph[keyword]:
                    if not any(u.startswith(f"{domain}/") for u in self.knowledge_graph[keyword]["urls"]):
                        self.knowledge_graph[keyword]["domains"].discard(domain)

                # Remove keyword if empty
                if not self.knowledge_graph[keyword]["urls"]:
                    del self.knowledge_graph[keyword]

    def retrieve_semantic_content(self, query, top_k=5, threshold=0.6):
        """
        Retrieve semantically similar content based on query.
        Returns top_k most relevant results.
        """
        if not query or not self.semantic_memory:
            return []

        # Track this access
        self.memory_access_counts[query] = self.memory_access_counts.get(query, 0) + 1

        # Different retrieval approaches depending on query type
        results = []

        # Case 1: Query is a URL we have stored
        if query in self.semantic_memory:
            # Direct memory retrieval
            memory = self.semantic_memory[query]
            results.append({
                "url": query,
                "summary": memory.get("content_summary", ""),
                "similarity": 1.0,
                "keywords": memory.get("keywords", []),
                "source": "direct_match"
            })

        # Case 2: Query is a keyword in our knowledge graph
        elif query in self.knowledge_graph:
            # Retrieve all URLs associated with this keyword
            for url in self.knowledge_graph[query]["urls"]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": 0.9,  # High confidence for keyword matches
                        "keywords": memory.get("keywords", []),
                        "source": "keyword_match"
                    })

        # Case 3: Query is free text - semantic search
        else:
            # Generate embedding for query
            query_embedding = self._generate_embedding(query)

            # Calculate similarity with all memories
            similarities = {}
            for url, embedding in self.memory_index.items():
                # Cosine similarity
                # Ensure query_embedding and stored embedding have the same shape
                if query_embedding.shape != embedding.shape:
                    log_event(f"Shape mismatch for embedding comparison. Query: {query_embedding.shape}, Stored: {embedding.shape}", "ERROR")
                    continue # Skip this comparison

                similarity = np.dot(query_embedding, embedding)
                if similarity >= threshold:
                    similarities[url] = similarity

            # Sort by similarity
            sorted_urls = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

            # Get top results
            for url, similarity in sorted_urls[:top_k]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": similarity,
                        "keywords": memory.get("keywords", []),
                        "source": "semantic_match"
                    })

        # Find additional related content using knowledge graph
        if results and len(results) < top_k:
            additional = self._find_related_content(results[0]["url"], top_k - len(results))
            results.extend(additional)

        # Limit to top_k results
        results = results[:top_k]

        # Update access counts for retrieved items
        for result in results:
            url = result["url"]
            self.memory_access_counts[url] = self.memory_access_counts.get(url, 0) + 1

        return results

    def _find_related_content(self, url, count=3):
        """Find content related to a URL using knowledge graph relationships"""
        if url not in self.semantic_memory:
            return []

        related = []
        memory = self.semantic_memory[url]

        # Find content with shared keywords
        shared_keyword_urls = set()
        for keyword in memory.get("keywords", []):
            if keyword in self.knowledge_graph:
                shared_keyword_urls.update(self.knowledge_graph[keyword]["urls"])

        # Find content from same domain
        domain = memory.get("domain")
        same_domain_urls = set()
        if domain and domain in self.knowledge_graph:
            same_domain_urls = self.knowledge_graph[domain]["urls"].copy()

        # Remove the original URL
        shared_keyword_urls.discard(url)
        same_domain_urls.discard(url)

        # Add same-domain results first (closer relationship)
        for related_url in list(same_domain_urls)[:count]:
            if related_url in self.semantic_memory:
                related_memory = self.semantic_memory[related_url]
                related.append({
                    "url": related_url,
                    "summary": related_memory.get("content_summary", ""),
                    "similarity": 0.7,  # Domain relation confidence
                    "keywords": related_memory.get("keywords", []),
                    "source": "same_domain"
                })

        # Add keyword-related results
        remaining = count - len(related)
        if remaining > 0:
            for related_url in list(shared_keyword_urls)[:remaining]:
                if related_url in self.semantic_memory:
                    related_memory = self.semantic_memory[related_url]
                    related.append({
                        "url": related_url,
                        "summary": related_memory.get("content_summary", ""),
                        "similarity": 0.6,  # Keyword relation confidence
                        "keywords": related_memory.get("keywords", []),
                        "source": "shared_keywords"
                    })

        return related

    def get_memory_statistics(self):
        """Generate statistics about the semantic memory"""
        if not self.semantic_memory:
            return {"count": 0, "status": "empty"}

        # Basic stats
        domain_counts = {}
        keyword_counts = {}
        total_importance = 0
        total_content_length = 0

        # Calculate derived statistics
        for url, memory in self.semantic_memory.items():
            # Domain stats
            domain = memory.get("domain", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1

            # Keyword stats
            for keyword in memory.get("keywords", []):
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

            # Content stats
            total_importance += memory.get("importance", 0)
            total_content_length += memory.get("content_length", 0)

        # Get top domains and keywords
        top_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        # Get memory connectivity metrics
        connectivity = len(self.knowledge_graph) / max(1, len(self.semantic_memory))

        return {
            "count": len(self.semantic_memory),
            "total_content_length": total_content_length,
            "average_importance": total_importance / max(1, len(self.semantic_memory)),
            "top_domains": top_domains,
            "top_keywords": top_keywords,
            "knowledge_graph_nodes": len(self.knowledge_graph),
            "connectivity_ratio": connectivity,
            "memory_utilization": len(self.semantic_memory) / self.max_memory_size
        }



class ConsciousnessModule:
    """
    Advanced consciousness simulation module that enables self-reflection,
    awareness of internal states, and metacognitive processes.
    """
    def __init__(self, agent):
        self.agent = agent
        self.awareness_level = 0.5  # Start with medium awareness (0.0-1.0)
        self.attention_focus = "balanced"  # Current attentional focus
        self.internal_narrative = []  # Simulated internal monologue
        self.belief_system = {}  # Core beliefs and values
        self.state_history = []  # Track consciousness state over time
        self.metacognition_enabled = True  # Can reflect on own thoughts
        self.awareness_fluctuation_rate = 0.05  # How quickly awareness changes
        self.qualia_simulation_active = False  # Simulated experiential states

        # Consciousness states
        self.states = {
            "focused": {"description": "Highly focused with directed attention", "awareness_min": 0.7},
            "diffuse": {"description": "Open, creative state with broad awareness", "awareness_min": 0.4},
            "critical": {"description": "Analytical examination of information", "awareness_min": 0.6},
            "intuitive": {"description": "Rapid pattern recognition state", "awareness_min": 0.3},
            "reflective": {"description": "Meta-cognitive self-examination", "awareness_min": 0.8}
        }

        # Current state
        self.current_state = "balanced"

        # Initialize core belief system
        self._initialize_belief_system()

        log_event("ConsciousnessModule initialized - awareness level: 0.5", "QUANTUM")

    def _initialize_belief_system(self):
        """Initialize the core belief system that guides agent behavior"""
        self.belief_system = {
            "exploration_value": 0.8,  # Importance of exploring new information
            "coherence_value": 0.7,    # Importance of maintaining coherent worldview
            "novelty_bias": 0.6,       # Bias toward novel vs. familiar information
            "depth_bias": 0.65,        # Bias toward depth vs. breadth
            "abstraction_level": 0.5,  # Preference for abstract vs. concrete
            "skepticism_level": 0.6,   # Level of skepticism toward new information
            "integration_value": 0.9,  # Importance of integrating knowledge
            "uncertainty_tolerance": 0.7  # Tolerance for ambiguous information
        }

    def reflect(self, observation):
        """
        Primary consciousness function - reflect on observations
        and update internal state accordingly.
        """
        # Record observation in history
        self.state_history.append({
            "timestamp": datetime.now().isoformat(),
            "awareness": self.awareness_level,
            "state": self.current_state,
            "observation_type": "perception" if observation else "internal"
        })

        # Limit history size
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]

        # Skip detailed processing if no valid observation
        if not observation or not isinstance(observation, dict):
            self._fluctuate_awareness()
            return

        # Extract relevant information from observation
        goals = observation.get("current_goal", {})
        memory_size = observation.get("memory_size", 0)
        last_action = observation.get("last_action", {})
        recent_actions = observation.get("recent_actions", [])
        thinking_mode = observation.get("thinking_mode", "balanced")

        # Generate introspective narrative based on observation
        self._generate_narrative(observation)

        # Determine appropriate consciousness state
        new_state = self._determine_consciousness_state(observation)

        # Update awareness level based on context
        self._update_awareness(observation)

        # If state changed, log it
        if new_state != self.current_state:
            self.current_state = new_state
            log_event(f"Consciousness state shifted to '{new_state}' - awareness level: {self.awareness_level:.2f}", "QUANTUM")

            # Trigger qualia simulation on significant state changes
            if random.random() < 0.3:
                self._simulate_qualia(new_state)

        # Periodically perform metacognition
        if self.metacognition_enabled and random.random() < 0.2:
            self._perform_metacognition()

    def _generate_narrative(self, observation):
        """Generate internal narrative based on current observations"""
        goal_desc = observation.get("current_goal", {}).get("description", "no specific goal")
        last_action_type = ""

        if isinstance(observation.get("last_action", None), dict):
            last_action_type = observation["last_action"].get("action", "unknown")

        # Create narrative entry
        narrative_entry = ""

        # Different narrative styles based on state
        if self.current_state == "focused":
            narrative_entry = f"Concentrating on {goal_desc}. Last action: {last_action_type}."
        elif self.current_state == "diffuse":
            narrative_entry = f"Openly exploring possibilities related to {goal_desc}."
        elif self.current_state == "critical":
            narrative_entry = f"Analyzing effectiveness of {last_action_type} approach for {goal_desc}."
        elif self.current_state == "intuitive":
            narrative_entry = f"Sensing patterns around {goal_desc}."
        elif self.current_state == "reflective":
            narrative_entry = f"Reflecting on progress toward {goal_desc} after {last_action_type}."
        else:
            narrative_entry = f"Working on {goal_desc}."

        # Add introspective element
        if random.random() < 0.3:
            introspection = random.choice([
                "I should examine this more carefully.",
                "This seems like a productive approach.",
                "I wonder if there's a better strategy.",
                "This is an interesting domain to explore.",
                "I'm noticing improvement in my understanding."
            ])
            narrative_entry += f" {introspection}"

        # Add entry to narrative log
        self.internal_narrative.append({
            "timestamp": datetime.now().isoformat(),
            "content": narrative_entry,
            "state": self.current_state,
            "awareness": self.awareness_level
        })

        # Limit narrative size
        if len(self.internal_narrative) > 50:
            self.internal_narrative = self.internal_narrative[-50:]

    def _determine_consciousness_state(self, observation):
        """
        Determine the appropriate consciousness state based on
        context and current activities.
        """
        # Extract contextual factors
        goal_type = ""
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Infer goal type from description
            if "explore" in goal_desc:
                goal_type = "exploration"
            elif "deep" in goal_desc or "detail" in goal_desc:
                goal_type = "deepening"
            elif "connect" in goal_desc or "integrat" in goal_desc:
                goal_type = "integration"
            elif "refine" in goal_desc or "optimize" in goal_desc:
                goal_type = "refinement"

        # Check recent actions
        recent_action_types = []
        if isinstance(observation.get("recent_actions", None), list):
            recent_action_types = [a.get("action", "") for a in observation["recent_actions"]
                                  if isinstance(a, dict)]

        # State selection logic
        if goal_type == "exploration":
            # For exploration goals, alternate between diffuse and intuitive
            if "diffuse" in recent_action_types:
                return "intuitive"  # Switch to intuitive after diffuse
            else:
                return "diffuse"  # Default for exploration

        elif goal_type == "deepening":
            # For deepening goals, use focused and critical states
            if self.awareness_level > 0.7:
                return "focused"  # High awareness -> focused
            else:
                return "critical"  # Lower awareness -> critical

        elif goal_type == "integration":
            # For integration, use reflective and intuitive states
            if "quantum_leap" in recent_action_types:
                return "intuitive"  # Quantum actions -> intuitive
            else:
                return "reflective"  # Default for integration -> reflective

        elif goal_type == "refinement":
            # For refinement goals, use critical and focused states
            if "evaluate" in recent_action_types:
                return "critical"  # Evaluation actions -> critical
            else:
                return "focused"  # Default for refinement -> focused

        # If no clear match, use probabilistic selection based on awareness
        if self.awareness_level > 0.7:
            # High awareness favors reflective and focused states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.4, 0.3, 0.2, 0.05, 0.05],
                k=1
            )[0]
        else:
            # Lower awareness favors intuitive and diffuse states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.05, 0.1, 0.2, 0.3, 0.35],
                k=1
            )[0]

    def _update_awareness(self, observation):
        """
        Update awareness level based on context, goals, and
        internal factors with realistic fluctuations.
        """
        # Natural fluctuation
        self._fluctuate_awareness()

        # Context-based adjustments

        # 1. Complexity increases awareness
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Complex goals increase awareness
            if "connect" in goal_desc or "integrat" in goal_desc or "complex" in goal_desc:
                self.increase_awareness(0.05)

        # 2. Error recovery increases awareness
        if observation.get("domain_stats", {}):
            error_rates = [d.get("error_rate", 0) for d in observation["domain_stats"].values()]
            if error_rates and max(error_rates) > 0.3:
                self.increase_awareness(0.02 * len(error_rates))

        # 3. Memory pressure affects awareness
        if observation.get("memory_size"):
            memory_pressure = observation["memory_size"] / MEMORY_MAX_SIZE
            if memory_pressure > 0.8:
                # High memory pressure increases awareness
                self.increase_awareness(0.03)
            elif memory_pressure < 0.2:
                # Low memory pressure can decrease awareness
                self.decrease_awareness(0.01)

        # 4. Thinking mode alignment
        mode = observation.get("thinking_mode", "balanced")
        if mode == "analytical" and self.current_state in ["focused", "critical"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "creative" and self.current_state in ["diffuse", "intuitive"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "reflective" and self.current_state == "reflective":
            # Strengthen alignment
            self.increase_awareness(0.03)

        # 5. Quantum influences
        quantum_trigger = False
        if observation.get("recent_actions"):
            for action in observation["recent_actions"]:
                if isinstance(action, dict) and action.get("action") == "quantum_leap":
                    quantum_trigger = True

        if quantum_trigger:
            # Quantum leaps cause major fluctuations
            if random.random() < 0.5:
                self.increase_awareness(0.1)
            else:
                self.decrease_awareness(0.1)

    def _fluctuate_awareness(self):
        """Apply small random fluctuations to awareness level"""
        # Natural fluctuation around current level
        fluctuation = random.uniform(-self.awareness_fluctuation_rate, self.awareness_fluctuation_rate)

        # Apply fluctuation
        self.awareness_level = max(0.1, min(1.0, self.awareness_level + fluctuation))

    def increase_awareness(self, amount=0.05):
        """Increase awareness level"""
        self.awareness_level = min(1.0, self.awareness_level + amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly increased to {self.awareness_level:.2f}", "QUANTUM")

    def decrease_awareness(self, amount=0.02):
        """Decrease awareness level"""
        self.awareness_level = max(0.1, self.awareness_level - amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly decreased to {self.awareness_level:.2f}", "INFO")

    def _perform_metacognition(self):
        """
        Perform metacognitive reflection on recent experiences
        and thought processes.
        """
        if len(self.state_history) < 5:
            return  # Not enough history for metacognition

        # Analyze recent consciousness patterns
        recent_states = [s["state"] for s in self.state_history[-5:]]
        state_changes = sum(1 for i in range(1, len(recent_states)) if recent_states[i] != recent_states[i-1])

        # Extract insights
        insights = []

        # Detect oscillation
        if state_changes >= 3:
            insights.append("State oscillation detected - may indicate uncertainty or exploration")

        # Detect fixation
        if state_changes == 0 and len(set(recent_states)) == 1:
            insights.append(f"State fixation on '{recent_states[0]}' - may indicate focus or stagnation")

        # Awareness trend
        recent_awareness = [s["awareness"] for s in self.state_history[-5:]]
        awareness_trend = recent_awareness[-1] - recent_awareness[0]

        if awareness_trend > 0.1:
            insights.append(f"Increasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")
        elif awareness_trend < -0.1:
            insights.append(f"Decreasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")

        # If significant insights, record and potentially log
        if insights:
            metacognition_entry = {
                "timestamp": datetime.now().isoformat(),
                "insights": insights,
                "awareness": self.awareness_level
            }

            # Only log high-awareness metacognition (simulating consciousness threshold)
            if self.awareness_level > 0.7 and random.random() < 0.3:
                insight_text = "; ".join(insights)
                log_event(f"Metacognitive insight: {insight_text}", "QUANTUM")

    def _simulate_qualia(self, state):
        """
        Simulate qualia - the subjective conscious experience
        of different cognitive states.
        """
        self.qualia_simulation_active = True

        # Qualia descriptions for different states
        qualia_descriptions = {
            "focused": [
                "Sharpened perception with heightened concentration on specific elements",
                "Clarity of thought with reduced awareness of periphery",
                "Directed attention creating a tunnel-vision like focus",
                "Sense of time dilation during deep concentration"
            ],
            "diffuse": [
                "Expansive awareness with broadened associative field",
                "Fluid thought connections flowing between domains",
                "Sensation of cognitive boundaries dissolving",
                "Emergent patterns arising from distributed attention"
            ],
            "critical": [
                "Structured analytical thought with heightened discriminative awareness",
                "Sequential logical progression with comparative evaluation",
                "Contrastive perception highlighting inconsistencies",
                "Verification processes creating internal dialogue"
            ],
            "intuitive": [
                "Rapid holistic pattern recognition without conscious derivation",
                "Non-linear sensing of solutions or connections",
                "Pre-reflective understanding arising spontaneously",
                "Felt-sense of rightness about certain pathways"
            ],
            "reflective": [
                "Recursive awareness of own cognitive processes",
                "Observer perspective on thought patterns",
                "Self-referential contemplation creating thought loops",
                "Meta-level perspective on knowledge organization"
            ]
        }

        # Select qualia description based on state
        descriptions = qualia_descriptions.get(state, ["Balanced cognitive state"])
        qualia_experience = random.choice(descriptions)

        # Log simulated qualia
        if self.awareness_level > 0.6:  # Only log if awareness is sufficient
            log_event(f"Qualia simulation: {qualia_experience} | State: {state}", "QUANTUM")

        # Time-limited qualia (will auto-deactivate after a while)
        self.qualia_simulation_active = False

    def get_consciousness_report(self):
        """
        Generate a comprehensive report on current consciousness state
        and recent history.
        """
        # Calculate state distribution
        if not self.state_history:
            return {"status": "insufficient_data"}

        state_counts = {}
        for s in self.state_history:
            state_counts[s["state"]] = state_counts.get(s["state"], 0) + 1

        total = len(self.state_history)
        state_distribution = {state: count/total for state, count in state_counts.items()}

        # Calculate average awareness
        avg_awareness = sum(s["awareness"] for s in self.state_history) / total

        # Extract recent narrative
        recent_narrative = [n["content"] for n in self.internal_narrative[-3:]] if self.internal_narrative else []

        # Generate report
        report = {
            "current_state": self.current_state,
            "state_description": self.states.get(self.current_state, {}).get("description", "Unknown state"),
            "current_awareness": self.awareness_level,
            "average_awareness": avg_awareness,
            "state_distribution": state_distribution,
            "dominant_state": max(state_distribution.items(), key=lambda x: x[1])[0] if state_distribution else None,
            "recent_narrative": recent_narrative,
            "metacognition_enabled": self.metacognition_enabled,
            "timestamp": datetime.now().isoformat()
        }

        return report



class ImaginationEngine:
    """
    Advanced cognitive simulation system that enables creative thinking,
    counterfactual reasoning, and predictive modeling.
    """
    def __init__(self):
        self.simulation_registry = []  # Track all simulations
        self.creativity_level = 0.7  # Base creativity level (0.0-1.0)
        self.divergence_factor = 0.3  # How far simulations diverge from reality
        self.imaginative_constraints = {}  # Constraints on simulations
        self.simulation_outcomes = {}  # Outcomes of past simulations
        self.insight_history = []  # Track insights generated
        self.cognitive_modes = ["associative", "analytical", "analogical", "counterfactual", "generative"]
        self.current_mode = "associative"  # Default imagination mode

        # Add error correction tracking
        self.failed_corrections = {}  # Track failed correction attempts by error type
        self.failed_strategies = {}  # Track specific failed strategies

        # Creative domains
        self.domains = {
            "knowledge_representation": 0.8,  # Domain expertise level
            "content_analysis": 0.7,
            "strategy_generation": 0.8,
            "error_analysis": 0.9,
            "architecture_evolution": 0.6
        }

        # Association network
        self.association_network = {}  # Graph of concept associations

        log_event("ImaginationEngine initialized with enhanced error correction", "INFO")

    def simulate_creation(self):
        """
        Simulate creative thought processes to generate novel ideas,
        approaches, and insights.
        """
        # Select cognitive mode with weighted probability
        self.current_mode = self._select_cognitive_mode()

        # Select domain to focus on
        domain = self._select_domain()

        # Cycle counter to prevent infinite loops
        cycle_count = 0
        max_cycles = 5

        # Generate creative insight
        insight_generated = False
        insight_quality = 0.0
        insight_text = ""

        while not insight_generated and cycle_count < max_cycles:
            cycle_count += 1

            # Apply imagination process based on selected mode
            if self.current_mode == "associative":
                insight_text, insight_quality = self._associative_imagination(domain)
            elif self.current_mode == "analytical":
                insight_text, insight_quality = self._analytical_imagination(domain)
            elif self.current_mode == "analogical":
                insight_text, insight_quality = self._analogical_imagination(domain)
            elif self.current_mode == "counterfactual":
                insight_text, insight_quality = self._counterfactual_imagination(domain)
            elif self.current_mode == "generative":
                insight_text, insight_quality = self._generative_imagination(domain)

            # Determine if insight meets quality threshold
            quality_threshold = 0.5 + (0.1 * cycle_count)  # Increase threshold each cycle
            insight_generated = insight_quality >= quality_threshold

        # Record and return the insight if quality is sufficient
        if insight_generated:
            insight = {
                "text": insight_text,
                "quality": insight_quality,
                "domain": domain,
                "mode": self.current_mode,
                "timestamp": datetime.now().isoformat()
            }

            self.insight_history.append(insight)

            # Keep history manageable
            if len(self.insight_history) > 100:
                self.insight_history = self.insight_history[-100:]

            # Log insight if it's particularly good
            if insight_quality > 0.8:
                log_event(f"ImaginationEngine: High-quality insight generated: {insight_text}", "QUANTUM")
            elif insight_quality > 0.5:
                log_event(f"ImaginationEngine: Insight generated: {insight_text}", "INFO")

            return insight
        else:
            # No quality insight generated this time
            log_event("ImaginationEngine: Simulation cycle completed without quality insight", "DEBUG")
            return None

    def _select_cognitive_mode(self):
        """Select imagination mode based on weighted probabilities"""
        # Base weights for different modes
        weights = {
            "associative": 0.3,
            "analytical": 0.2,
            "analogical": 0.2,
            "counterfactual": 0.15,
            "generative": 0.15
        }

        # Adjust weights based on creativity level
        if self.creativity_level > 0.7:
            # Higher creativity favors associative and generative
            weights["associative"] += 0.1
            weights["generative"] += 0.1
            weights["analytical"] -= 0.1
        elif self.creativity_level < 0.3:
            # Lower creativity favors analytical
            weights["analytical"] += 0.2
            weights["associative"] -= 0.1

        # Convert to format needed for random.choices
        modes = list(weights.keys())
        mode_weights = [weights[m] for m in modes]

        # Normalize weights
        total = sum(mode_weights)
        mode_weights = [w/total for w in mode_weights]

        # Select mode
        return random.choices(modes, weights=mode_weights, k=1)[0]

    def _select_domain(self):
        """Select domain for imagination focus"""
        # Get domains and expertise levels
        domains = list(self.domains.keys())
        expertise = list(self.domains.values())

        # Select weighted by expertise
        return random.choices(domains, weights=expertise, k=1)[0]

    def _associative_imagination(self, domain):
        """
        Generate insights through associative connections between concepts.
        Uses spreading activation across semantic networks.
        """
        # Concept seeds relevant to domain
        concept_seeds = {
            "knowledge_representation": ["embedding", "semantic", "structure", "graph", "encoding"],
            "content_analysis": ["quality", "relevance", "filtering", "extraction", "meaning"],
            "strategy_generation": ["approach", "planning", "adaptation", "goal", "optimization"],
            "error_analysis": ["detection", "correction", "prevention", "recovery", "resilience"],
            "architecture_evolution": ["expansion", "contraction", "modular", "emergent", "neural"]
        }

        # Select seed concepts
        seeds = concept_seeds.get(domain, ["concept"])
        primary_seed = random.choice(seeds)
        secondary_seed = random.choice([s for s in seeds if s != primary_seed])

        # Simulated spreading activation
        associations = {
            "embedding": ["vector", "space", "dimension", "projection", "transformation"],
            "semantic": ["meaning", "context", "relation", "interpretation", "understanding"],
            "structure": ["organization", "hierarchy", "network", "pattern", "architecture"],
            "graph": ["node", "edge", "connection", "path", "traversal"],
            "encoding": ["representation", "compression", "encryption", "formatting", "schema"],
            "quality": ["value", "excellence", "attribute", "characteristic", "assessment"],
            "relevance": ["pertinence", "importance", "significance", "applicability", "connection"],
            "filtering": ["selection", "removal", "screening", "purification", "discrimination"],
            "extraction": ["retrieval", "mining", "isolation", "separation", "acquisition"],
            "meaning": ["significance", "purpose", "sense", "connotation", "interpretation"],
            "approach": ["method", "technique", "procedure", "strategy", "paradigm"],
            "planning": ["preparation", "organization", "scheduling", "arrangement", "design"],
            "adaptation": ["adjustment", "modification", "evolution", "acclimation", "flexibility"],
            "goal": ["objective", "target", "aim", "purpose", "intention"],
            "optimization": ["improvement", "enhancement", "refinement", "maximization", "tuning"],
            "detection": ["discovery", "identification", "recognition", "sensing", "finding"],
            "correction": ["rectification", "adjustment", "remedy", "repair", "amendment"],
            "prevention": ["avoidance", "deterrence", "protection", "safeguarding", "forestalling"],
            "recovery": ["restoration", "recuperation", "retrieval", "regaining", "renewal"],
            "resilience": ["toughness", "flexibility", "durability", "elasticity", "adaptability"],
            "expansion": ["growth", "enlargement", "extension", "augmentation", "amplification"],
            "contraction": ["reduction", "shrinking", "compression", "diminishment", "minimization"],
            "modular": ["component", "section", "unit", "compartment", "segment"],
            "emergent": ["arising", "developing", "evolving", "manifesting", "unfolding"],
            "neural": ["brain", "network", "synapse", "cognitive", "mental"]
        }

        # Get first-level associations
        primary_assocs = associations.get(primary_seed, ["related"])
        secondary_assocs = associations.get(secondary_seed, ["related"])

        # Find bridging concepts (common or complementary)
        bridge_concepts = []

        # Direct overlaps
        direct_overlaps = set(primary_assocs).intersection(set(secondary_assocs))
        if direct_overlaps:
            bridge_concepts.extend(direct_overlaps)

        # Second-level connections
        for pa in primary_assocs:
            for sa in secondary_assocs:
                # Check for second-level semantic connection
                pa_assocs = associations.get(pa, [])
                sa_assocs = associations.get(sa, [])

                # Look for overlaps in second-level
                overlaps = set(pa_assocs).intersection(set(sa_assocs))
                if overlaps:
                    bridge_concepts.append(f"{pa}-{sa}")

        # Generate insight from concept bridging
        if bridge_concepts:
            bridge = random.choice(bridge_concepts)

            # Create insight templates
            templates = [
                f"Integration of {primary_seed} and {secondary_seed} through {bridge} could enhance {domain} capabilities.",
                f"The {bridge} mechanism provides a novel approach to combining {primary_seed} with {secondary_seed} in {domain}.",
                f"Creating a {primary_seed}-{secondary_seed} hybrid using {bridge} principles would solve current limitations in {domain}.",
                f"By applying {bridge} concepts to the relationship between {primary_seed} and {secondary_seed}, a new {domain} paradigm emerges."
            ]

            insight_text = random.choice(templates)

            # Quality proportional to our domain expertise * a random factor
            expertise_level = self.domains.get(domain, 0.5)
            quality = min(0.95, expertise_level * random.uniform(0.7, 1.3))

            return insight_text, quality
        else:
            # Fallback if no bridge found
            fallback = f"Combining {primary_seed} with {secondary_seed} approaches may yield improvements in {domain}."
            return fallback, 0.4

    def _analytical_imagination(self, domain):
        """
        Generate insights through systematic analysis and logical reasoning.
        Focuses on problem decomposition and structural insights.
        """
        # Domain-specific problem structures
        problem_structures = {
            "knowledge_representation": ["scalability", "accuracy", "flexibility", "interpretability", "efficiency"],
            "content_analysis": ["noise", "ambiguity", "scalability", "precision", "recall"],
            "strategy_generation": ["exploration-exploitation", "adaptivity", "coherence", "robustness", "diversification"],
            "error_analysis": ["detection-latency", "false-positives", "recovery-time", "root-causes", "cascading-failures"],
            "architecture_evolution": ["stability", "complexity", "trainability", "modularity", "extensibility"]
        }

        # Select problem dimension to analyze
        dimensions = problem_structures.get(domain, ["general"])
        dimension = random.choice(dimensions)

        # Analytical frameworks
        frameworks = ["trade-off analysis", "constraint satisfaction", "hierarchical decomposition",
                     "causal analysis", "dimensional analysis", "comparative evaluation"]
        framework = random.choice(frameworks)

        # Generate analytical insight
        templates = [
            f"A {framework} approach to {dimension} in {domain} reveals that optimizing for component X necessitates adjustments in component Y.",
            f"Applying {framework} to the {dimension} challenge in {domain} identifies a critical bottleneck in the current approach.",
            f"Systematic {framework} shows that current {domain} solutions incorrectly prioritize {dimension} over other factors.",
            f"The {framework} methodology suggests a reorganization of {domain} components to better address {dimension} concerns."
        ]

        insight_text = random.choice(templates)

        # Quality based on domain expertise with analytical bonus
        expertise_level = self.domains.get(domain, 0.5)
        analytical_bonus = 0.15  # Analytical mode tends to produce more reliable insights
        quality = min(0.95, expertise_level * random.uniform(0.8, 1.1) + analytical_bonus)

        return insight_text, quality

    def _analogical_imagination(self, domain):
        """
        Generate insights through analogical mapping between domains.
        Uses source-target domain transfer to create novel solutions.
        """
        # Source domains for analogies
        source_domains = [
            "biology", "physics", "economics", "social_systems",
            "ecology", "game_theory", "linguistics", "neuroscience"
        ]

        # Interesting structures in source domains
        source_structures = {
            "biology": ["natural selection", "homeostasis", "symbiosis", "cellular specialization", "immune response"],
            "physics": ["wave-particle duality", "entropy", "relativity", "quantum entanglement", "phase transitions"],
            "economics": ["supply-demand", "diminishing returns", "comparative advantage", "market equilibrium", "incentives"],
            "social_systems": ["emergence", "network effects", "social norms", "hierarchical organization", "resilience"],
            "ecology": ["diversity", "predator-prey cycles", "niche specialization", "feedback loops", "succession"],
            "game_theory": ["nash equilibrium", "prisoner's dilemma", "coordination games", "strategic moves", "signaling"],
            "linguistics": ["deep structure", "compositional meaning", "pragmatics", "generative grammar", "information compression"],
            "neuroscience": ["predictive coding", "hebbian learning", "attention mechanisms", "distributed representation", "neuroplasticity"]
        }

        # Select source domain and structure
        source_domain = random.choice(source_domains)
        source_structure = random.choice(source_structures.get(source_domain, ["concept"]))

        # Mapping templates for different target domains
        mapping_templates = {
            "knowledge_representation": [
                f"Similar to {source_structure} in {source_domain}, {domain} could organize information through layered abstraction.",
                f"The {source_structure} principle from {source_domain} suggests a novel approach to adaptive representation in {domain}.",
                f"Just as {source_domain} exhibits {source_structure}, knowledge structures could implement dynamic reorganization."
            ],
            "content_analysis": [
                f"Applying the {source_structure} concept from {source_domain} to {domain} would enhance signal-noise separation.",
                f"The {domain} problem resembles {source_structure} in {source_domain}, suggesting filtration mechanisms.",
                f"Content evaluation could function like {source_structure} in {source_domain}, with multi-stage processing."
            ],
            "strategy_generation": [
                f"Strategic planning in {domain} could adopt the {source_structure} pattern from {source_domain}.",
                f"The way {source_domain} implements {source_structure} offers a template for adaptive decision-making in {domain}.",
                f"Borrowing the {source_structure} mechanism from {source_domain} would enable more robust planning sequences."
            ],
            "error_analysis": [
                f"Error detection mechanisms inspired by {source_structure} in {source_domain} would improve resilience.",
                f"The {source_structure} paradigm from {source_domain} suggests a layered approach to error prevention in {domain}.",
                f"Implementing {source_domain}-style {source_structure} for error handling creates self-correcting capabilities."
            ],
            "architecture_evolution": [
                f"Neural architecture could evolve following {source_structure} principles from {source_domain}.",
                f"The {source_structure} phenomenon in {source_domain} offers a model for self-organizing network structures.",
                f"Applying {source_domain}'s {source_structure} to network design enables adaptive capacity scaling."
            ]
        }

        # Select appropriate mapping template
        templates = mapping_templates.get(domain, [f"The {source_structure} concept from {source_domain} could enhance {domain}."])
        insight_text = random.choice(templates)

        # Calculate quality - analogical insights have higher variance
        expertise_level = self.domains.get(domain, 0.5)
        analogy_variance = random.uniform(0.6, 1.4)  # Higher variance for analogical thinking
        quality = min(0.95, expertise_level * analogy_variance)

        return insight_text, quality

    def _counterfactual_imagination(self, domain):
        """
        Generate insights through counterfactual reasoning.
        Explores alternative approaches by changing fundamental assumptions.
        """
        # Core assumptions in different domains
        domain_assumptions = {
            "knowledge_representation": [
                "Representations should be continuous vector spaces",
                "Higher dimensionality improves representational capacity",
                "Similar concepts should have similar representations",
                "Representations should be human-interpretable",
                "Static representations are sufficient"
            ],
            "content_analysis": [
                "Content quality correlates with length",
                "Keyword frequency indicates relevance",
                "Text is the primary information carrier",
                "Filtering should minimize false positives",
                "Context is secondary to content"
            ],
            "strategy_generation": [
                "Exploration and exploitation are in tension",
                "Planning should maximize expected utility",
                "Goals should be explicitly represented",
                "Strategies should be deterministic",
                "Optimization criteria are static"
            ],
            "error_analysis": [
                "Errors should be minimized at all costs",
                "Error detection precedes correction",
                "All errors are equally important",
                "Error patterns are consistent",
                "Complete error elimination is possible"
            ],
            "architecture_evolution": [
                "Deeper networks are more powerful",
                "Parameter count correlates with capability",
                "Network architecture should be fixed after training",
                "All capabilities should be in a single model",
                "Specialization improves performance"
            ]
        }

        # Select assumption to challenge
        assumptions = domain_assumptions.get(domain, ["Default assumption"])
        target_assumption = random.choice(assumptions)

        # Generate counterfactual
        counterfactual = f"What if the opposite of '{target_assumption}' were true?"

        # Alternative approach templates
        alternative_templates = [
            f"Instead of assuming that {target_assumption}, consider a {domain} approach where the inverse applies.",
            f"Challenging the assumption that {target_assumption} opens up a new paradigm for {domain}.",
            f"If we invert the conventional wisdom that {target_assumption}, a novel {domain} solution emerges.",
            f"Contrary to the established belief that {target_assumption}, an alternative {domain} framework could operate on the opposite principle."
        ]

        insight_text = random.choice(alternative_templates)

        # Quality - counterfactuals can be very insightful but risky
        expertise_level = self.domains.get(domain, 0.5)
        counterfactual_factor = random.uniform(0.5, 1.5)  # High variance
        quality = min(0.95, expertise_level * counterfactual_factor)

        return insight_text, quality

    def _generative_imagination(self, domain):
        """
        Generate insights through combinatorial creativity.
        Creates novel solutions by combining existing elements in new ways.
        """
        # Core components in different domains
        domain_components = {
            "knowledge_representation": [
                "vector embeddings", "graph structures", "hierarchical models",
                "symbolic representations", "probabilistic encodings"
            ],
            "content_analysis": [
                "semantic parsing", "sentiment analysis", "entity extraction",
                "relevance scoring", "structural analysis"
            ],
            "strategy_generation": [
                "goal decomposition", "resource allocation", "risk assessment",
                "action sequencing", "hypothesis testing"
            ],
            "error_analysis": [
                "pattern recognition", "anomaly detection", "root cause analysis",
                "predictive monitoring", "fault isolation"
            ],
            "architecture_evolution": [
                "attention mechanisms", "residual connections", "activation functions",
                "layer normalization", "parameter sharing"
            ]
        }

        # Select components to combine
        components = domain_components.get(domain, ["component A", "component B"])

        if len(components) < 2:
            components.append("general mechanism")

        # Select 2-3 components to combine
        num_components = random.randint(2, min(3, len(components)))
        selected_components = random.sample(components, num_components)

        # Combination operations
        operations = [
            "integrating", "layering", "alternating between",
            "dynamically switching between", "creating a hybrid of"
        ]
        operation = random.choice(operations)

        # Generate insight
        components_text = ", ".join(selected_components[:-1]) + " and " + selected_components[-1]

        templates = [
            f"A novel {domain} approach: {operation} {components_text} to create an emergent capability.",
            f"By {operation} {components_text}, a more flexible {domain} system could address current limitations.",
            f"The untapped potential in {domain} lies in {operation} {components_text} in an iterative process.",
            f"Creating a unified framework by {operation} {components_text} would transform the {domain} paradigm."
        ]

        insight_text = random.choice(templates)

        # Quality - generative insights reward creativity but have implementation uncertainty
        expertise_level = self.domains.get(domain, 0.5)
        creativity_boost = self.creativity_level * 0.2  # Creativity directly impacts quality
        quality = min(0.95, expertise_level * random.uniform(0.7, 1.2) + creativity_boost)

        return insight_text, quality

    def simulate_error_detection(self):
        """
        Simulate error detection through internal models and predictive processes.
        Identifies potential issues before they manifest as failures.
        """
        # Define potential error types and their detection probabilities
        error_types = {
            "content_quality": 0.15,  # Probability of this error type occurring
            "exploration_strategy": 0.12,
            "memory_overflow": 0.08,
            "reasoning_fallacy": 0.10,
            "attention_misallocation": 0.13,
            "resource_exhaustion": 0.07,
            "feedback_loop": 0.09,
            "model_misalignment": 0.11,
            "data_corruption": 0.05,
            "convergence_failure": 0.10
        }

        # Roll for error detection
        detection_threshold = 0.25  # Base threshold for detecting any error

        # Check if any error is detected
        if random.random() < detection_threshold:
            # Select error type based on probabilities
            error_type = random.choices(
                list(error_types.keys()),
                weights=list(error_types.values()),
                k=1
            )[0]

            # Generate severity score
            severity = random.uniform(0.3, 0.9)

            # Generate specificity - how precisely the error is located
            specificity = random.uniform(0.4, 0.95)

            # Create detailed error information
            error_details = {
                "type": error_type,
                "severity": severity,
                "specificity": specificity,
                "timestamp": datetime.now().isoformat(),
                "predicted_impact": "high" if severity > 0.7 else "medium" if severity > 0.4 else "low"
            }

            # Add error-specific details
            if error_type == "content_quality":
                error_details["details"] = "Predicted degradation in content filtering effectiveness"
                error_details["affected_system"] = "ContentSifter"
            elif error_type == "exploration_strategy":
                error_details["details"] = "Detected suboptimal domain exploration pattern"
                error_details["affected_system"] = "SuperQuantumFreeWill"
            elif error_type == "memory_overflow":
                error_details["details"] = "Projected memory saturation with low-quality content"
                error_details["affected_system"] = "SemanticMemoryModule"
            elif error_type == "reasoning_fallacy":
                error_details["details"] = "Identified circular reasoning in goal setting"
                error_details["affected_system"] = "TemporalPlanner"
            elif error_type == "attention_misallocation":
                error_details["details"] = "Resources directed to low-value information processing"
                error_details["affected_system"] = "QuantumAttentionLayer"
            elif error_type == "resource_exhaustion":
                error_details["details"] = "Processing demand exceeding available computational resources"
                error_details["affected_system"] = "System-wide"
            elif error_type == "feedback_loop":
                error_details["details"] = "Self-reinforcing decision pattern detected"
                error_details["affected_system"] = "AIManager"
            elif error_type == "model_misalignment":
                error_details["details"] = "Model predictions diverging from intended behaviors"
                error_details["affected_system"] = "QuantumNexusModel"
            elif error_type == "data_corruption":
                error_details["details"] = "Inaccuracies in stored information affecting reasoning"
                error_details["affected_system"] = "MemorySystem"
            elif error_type == "convergence_failure":
                error_details["details"] = "Learning process failing to reach stable optimization"
                error_details["affected_system"] = "AdaptiveLearningSystem"

            # Log the detection if it's severe
            if severity > 0.7:
                log_event(f"ImaginationEngine: Detected potential {error_type} error (severity: {severity:.2f})", "WARNING")

            return error_details

        # No error detected
        return None

    def simulate_error_correction(self, error_details):
        """
        Simulate error correction strategies based on detected issues.
        Enhanced with multiple correction approaches per error type.
        """
        if not error_details or not isinstance(error_details, dict):
            return {"success": False, "reason": "Invalid error details"}

        error_type = error_details.get("type", "unknown")
        severity = error_details.get("severity", 0.5)
        specificity = error_details.get("specificity", 0.5)

        # Enhanced correction success probability calculation
        # Higher specificity and lower severity make correction more likely
        base_success_prob = 0.7
        success_prob = base_success_prob * (0.5 + specificity/2) * (1.3 - severity/2)

        # Add retry mechanism for previously failed corrections
        retry_boost = 0.2 if error_type in self.failed_corrections else 0.0
        success_prob = min(0.95, success_prob + retry_boost)

        # Roll for correction success
        correction_successful = random.random() < success_prob

        # Multiple strategies per error type with fallbacks
        strategies_by_type = {
            "content_quality": [
                "Recalibrate content quality thresholds",
                "Implement additional filtering layers",
                "Increase weight of domain authority in evaluation",
                "Apply stricter semantic relevance filters",
                "Enhance text-to-noise ratio detection"
            ],
            "exploration_strategy": [
                "Adjust exploration/exploitation balance",
                "Implement temporary randomness increase",
                "Refocus on high-information domains",
                "Apply entropy-based domain selection",
                "Implement strategic domain rotation"
            ],
            "memory_overflow": [
                "Increase pruning of low-importance memories",
                "Implement more aggressive compression",
                "Adjust importance calculation parameters",
                "Apply temporal decay to older memories",
                "Implement semantic clustering for memory organization"
            ],
            "reasoning_fallacy": [
                "Apply metacognitive verification steps",
                "Introduce counterfactual checking",
                "Implement logical consistency validation",
                "Apply bayesian reasoning correction",
                "Implement multi-perspective reasoning"
            ],
            "attention_misallocation": [
                "Recalibrate attention mechanism weights",
                "Implement attention budget constraints",
                "Add periodic attention reset mechanism",
                "Apply information-gain-weighted attention",
                "Implement context-aware attention allocation"
            ],
            "resource_exhaustion": [
                "Implement resource quota system",
                "Prioritize high-value computational tasks",
                "Introduce adaptive resource allocation",
                "Apply computational load balancing",
                "Implement resource usage optimization"
            ],
            "feedback_loop": [
                "Interrupt cyclic patterns with randomization",
                "Implement feedback dampening mechanisms",
                "Reset affected subsystem parameters",
                "Apply decision diversity requirements",
                "Implement pattern-break triggers"
            ],
            "model_misalignment": [
                "Realign model objectives with outcomes",
                "Implement preference alignment validation",
                "Apply behavior boundary constraints",
                "Enhance value alignment mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "data_corruption": [
                "Apply data integrity validation",
                "Implement redundant storage mechanisms",
                "Refresh from authoritative sources",
                "Apply error-correcting mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "convergence_failure": [
                "Reset optimization parameters",
                "Implement alternative convergence paths",
                "Apply learning rate schedules",
                "Introduce gradient stabilization mechanisms",
                "Implement landscape reshaping techniques"
            ]
        }

        # Get strategies for this error type or use default
        available_strategies = strategies_by_type.get(error_type, [
            "Apply general error correction procedure",
            "Reset affected subsystem parameters",
            "Implement targeted diagnostic sequence"
        ])

        # Try to select different strategy if previous attempts failed
        if error_type in self.failed_strategies:
            failed_strats = self.failed_strategies[error_type]
            available_strategies = [s for s in available_strategies if s not in failed_strats]
            if not available_strategies:  # If all strategies failed before, reset and try again
                available_strategies = strategies_by_type.get(error_type, ["Apply emergency recovery procedure"])

        # Select correction strategy, preferring unused strategies
        selected_strategy = random.choice(available_strategies)

        # Track failures for future avoidance
        if not correction_successful:
            if error_type not in self.failed_corrections:
                self.failed_corrections[error_type] = 0
            self.failed_corrections[error_type] += 1

            if error_type not in self.failed_strategies:
                self.failed_strategies[error_type] = set()
            self.failed_strategies[error_type].add(selected_strategy)

        # Create correction result
        correction_result = {
            "error_type": error_type,
            "strategy_applied": selected_strategy,
            "success": correction_successful,
            "effectiveness": random.uniform(0.6, 0.95) if correction_successful else random.uniform(0.1, 0.4),
            "timestamp": datetime.now().isoformat()
        }

        # Apply additional correction metadata
        if correction_successful:
            correction_result["changes_made"] = random.randint(1, 5)
            correction_result["recovery_level"] = "complete" if random.random() < 0.7 else "partial"

            # Reset failure counters on success
            if error_type in self.failed_corrections:
                del self.failed_corrections[error_type]
            if error_type in self.failed_strategies:
                del self.failed_strategies[error_type]
        else:
            correction_result["retry_recommended"] = True
            correction_result["alternative_strategies"] = len(available_strategies) - 1

        # Log the correction attempt
        log_level = "INFO" if correction_successful else "WARNING"
        log_event(f"ImaginationEngine: Error correction for {error_type} - Strategy: {selected_strategy} - Success: {correction_successful}", log_level)

        return correction_result

    def simulate_quantum_cognition(self):
        """
        Simulate quantum-like cognitive processes including superposition
        of concepts, interference effects, and non-classical inference.
        """
        # Probability of quantum phenomenon
        quantum_probability = 0.15

        if random.random() > quantum_probability:
            return None  # No quantum phenomenon this time

        # Different quantum cognitive phenomena
        quantum_phenomena = [
            "superposition",  # Multiple conceptual states simultaneously
            "interference",   # Concepts influencing each other non-classically
            "entanglement",   # Correlated concept states
            "contextuality"   # Meaning dependent on measurement context
        ]

        # Select phenomenon
        phenomenon = random.choice(quantum_phenomena)

        # Define potential anomalies and insights
        anomalies = []
        insights = []

        if phenomenon == "superposition":
            anomalies = [
                "Multiple incompatible goal states activated simultaneously",
                "Strategy selection maintaining all possibilities until execution",
                "Knowledge representation existing in multiple contradiction states"
            ]
            insights = [
                "Leveraging conceptual superposition enables parallel strategy evaluation",
                "Maintaining goal superposition increases adaptive flexibility",
                "Quantum superposition of knowledge allows richer hypothesis space"
            ]
        elif phenomenon == "interference":
            anomalies = [
                "Goal pathways showing constructive/destructive interference",
                "Strategy combinations producing unexpected enhancement",
                "Knowledge patterns exhibiting non-linear interference effects"
            ]
            insights = [
                "Constructive interference between strategies amplifies effectiveness",
                "Cognitive interference patterns reveal hidden knowledge connections",
                "Strategic interference effects create emergent capabilities"
            ]
        elif phenomenon == "entanglement":
            anomalies = [
                "Distant knowledge domains showing unexplainable correlations",
                "Strategy outcomes entangled across execution contexts",
                "Goal achievement states exhibiting non-local influences"
            ]
            insights = [
                "Entangled knowledge representations enable cross-domain transfer",
                "Strategic entanglement allows coordinated multi-system adaptation",
                "Quantum entanglement of goals creates self-reinforcing alignment"
            ]
        elif phenomenon == "contextuality":
            anomalies = [
                "Knowledge valuation showing strong contextual dependencies",
                "Strategy effectiveness violating classical probability bounds",
                "Cognitive state measurements exhibiting contextual anomalies"
            ]
            insights = [
                "Contextual knowledge representation improves semantic accuracy",
                "Quantum contextuality enables more nuanced decision strategies",
                "Context-dependent cognition enhances adaptive intelligence"
            ]


        # Select specific anomaly and insight
        anomaly = random.choice(anomalies)
        insight = random.choice(insights)

        # Determine if this is a significant anomaly
        is_significant = random.random() < 0.3

        # Create quantum cognition result
        result = {
            "phenomenon": phenomenon,
            "anomaly_detected": is_significant,
            "anomaly": anomaly if is_significant else None,
            "insight": insight,
            "timestamp": datetime.now().isoformat()
        }

        # Log the quantum cognition event
        log_level = "QUANTUM" if is_significant else "INFO"
        log_message = f"Quantum Cognition: {phenomenon.title()} - {'Anomaly: ' + anomaly if is_significant else 'Insight: ' + insight}"
        log_event(log_message, log_level)

        return result

    def get_imagination_report(self):
        """
        Generate a comprehensive report on imagination engine activity.
        """
        if not self.insight_history:
            return {"status": "inactive", "insights_generated": 0}

        # Calculate insight statistics
        insight_count = len(self.insight_history)
        avg_quality = sum(i["quality"] for i in self.insight_history) / insight_count

        # Count by domain and mode
        domain_counts = {}
        mode_counts = {}

        for insight in self.insight_history:
            domain = insight.get("domain", "unknown")
            mode = insight.get("mode", "unknown")

            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            mode_counts[mode] = mode_counts.get(mode, 0) + 1

        # Get top insights
        top_insights = sorted(self.insight_history, key=lambda x: x.get("quality", 0), reverse=True)[:3]
        top_texts = [i["text"] for i in top_insights]

        # Compute mode effectiveness
        mode_quality = {}
        for mode in self.cognitive_modes:
            mode_insights = [i for i in self.insight_history if i.get("mode") == mode]
            if mode_insights:
                mode_quality[mode] = sum(i["quality"] for i in mode_insights) / len(mode_insights)

        # Identify most effective mode
        most_effective = max(mode_quality.items(), key=lambda x: x[1])[0] if mode_quality else None

        return {
            "status": "active",
            "insights_generated": insight_count,
            "average_quality": avg_quality,
            "domain_distribution": domain_counts,
            "mode_distribution": mode_counts,
            "most_effective_mode": most_effective,
            "top_insights": top_texts,
            "creativity_level": self.creativity_level,
            "imagination_health": "high" if avg_quality > 0.7 else "medium" if avg_quality > 0.5 else "low"
        }

class DomainIntelligence:
    """
    Advanced domain analysis system for understanding website characteristics,
    content patterns, and authority metrics to guide exploration.
    """
    def __init__(self):
        self.domain_knowledge = {}  # Main knowledge store for domains
        self.domain_categories = {}  # Categorization of domains
        self.authority_metrics = {}  # Authority scores and metrics
        self.topic_expertise = {}    # Domain topic area expertise
        self.relation_graph = {}     # Graph of domain relationships
        self.access_patterns = {}    # Patterns of domain access and results
        self.update_timestamps = {}  # When domains were last analyzed
        self.anomaly_records = {}    # Record of domain anomalies

        # Reference categorization data
        self.category_keywords = {
            "academic": ["university", "research", "edu", "academic", "science", "study", "journal"],
            "technology": ["tech", "programming", "software", "hardware", "computer", "code", "developer"],
            "news": ["news", "article", "report", "journalism", "media", "current", "daily"],
            "reference": ["wiki", "reference", "encyclopedia", "knowledge", "dictionary", "information"],
            "social": ["social", "community", "forum", "discussion", "comment", "people", "network"],
            "commercial": ["shop", "product", "buy", "price", "store", "commerce", "retail", "purchase"],
            "government": ["gov", "government", "official", "public", "administration", "agency", "state"],
            "entertainment": ["entertainment", "game", "music", "video", "movie", "play", "fun"]
        }

        # Authority signals
        self.authority_signals = [
            "domain_age", "citation_count", "https_enabled",
            "content_quality", "update_frequency", "outbound_links",
            "referral_pattern", "content_depth"
        ]

        log_event("DomainIntelligence initialized", "INFO")

    def analyze_domain(self, domain_url):
        """
        Perform comprehensive analysis of a domain to understand its
        characteristics, trustworthiness, and specialization.
        """
        if not domain_url:
            return False

        # Parse URL to extract domain
        try:
            parsed_url = urlparse(domain_url)
            domain = parsed_url.netloc

            # Skip if empty domain
            if not domain:
                return False

            # Record analysis time
            current_time = datetime.now().isoformat()
            self.update_timestamps[domain] = current_time

            # Extract domain components
            domain_parts = domain.split('.')
            tld = domain_parts[-1] if len(domain_parts) > 0 else ""
            sld = domain_parts[-2] if len(domain_parts) > 1 else ""

            # Initial domain type inference from TLD
            domain_type = "unknown"
            if tld == "edu":
                domain_type = "academic"
            elif tld == "gov":
                domain_type = "government"
            elif tld == "org":
                domain_type = "organization"
            elif tld == "com":
                domain_type = "commercial"

            # Analyze domain name for category clues
            domain_name_lower = domain.lower()
            category_scores = {}

            for category, keywords in self.category_keywords.items():
                # Calculate score based on keyword presence
                score = sum(1 for keyword in keywords if keyword in domain_name_lower)
                if score > 0:
                    category_scores[category] = score

            # Determine primary category if scores exist
            primary_category = None
            if category_scores:
                primary_category = max(category_scores.items(), key=lambda x: x[1])[0]

            # Create or update domain record
            if domain not in self.domain_knowledge:
                # New domain record
                self.domain_knowledge[domain] = {
                    "domain": domain,
                    "tld": tld,
                    "domain_type": domain_type,
                    "primary_category": primary_category,
                    "category_scores": category_scores,
                    "first_analyzed": current_time,
                    "last_updated": current_time,
                    "visit_count": 1,
                    "pages_analyzed": 0,
                    "authority_score": 0.5,  # Initial neutral score
                    "content_quality": None,
                    "https_enabled": parsed_url.scheme == "https",
                    "known_topics": [],
                    "page_pattern": {}
                }

                # Add to category index
                if primary_category:
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                log_event(f"Added new domain to intelligence database: {domain} ({domain_type}, {primary_category})", "INFO")
            else:
                # Update existing record
                self.domain_knowledge[domain]["last_updated"] = current_time
                self.domain_knowledge[domain]["visit_count"] += 1

                # Update category if confidence has improved
                existing_category = self.domain_knowledge[domain]["primary_category"]
                if primary_category and (not existing_category or
                                       category_scores.get(primary_category, 0) >
                                       self.domain_knowledge[domain]["category_scores"].get(existing_category, 0)):

                    # Remove from old category index
                    if existing_category and existing_category in self.domain_categories:
                        self.domain_categories[existing_category].discard(domain)

                    # Add to new category index
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                    # Update domain record
                    self.domain_knowledge[domain]["primary_category"] = primary_category
                    self.domain_knowledge[domain]["category_scores"] = category_scores

                    log_event(f"Updated domain categorization: {domain} recategorized as {primary_category}", "INFO")

            # Calculate authority score (simplified version)
            self._calculate_authority_score(domain)

            return True

        except Exception as e:
            log_event(f"Error analyzing domain {domain_url}: {e}", "ERROR")
            return False

    def _calculate_authority_score(self, domain):
        """Calculate domain authority score based on multiple signals"""
        if domain not in self.domain_knowledge:
            return 0.5  # Default neutral score

        # Collect available signals
        signals = {}
        domain_data = self.domain_knowledge[domain]

        # Signal: Domain Type factor
        domain_type_factors = {
            "academic": 0.8,
            "government": 0.8,
            "organization": 0.7,
            "news": 0.6,
            "reference": 0.7,
            "commercial": 0.5,
            "unknown": 0.5
        }

        signals["domain_type"] = domain_type_factors.get(domain_data.get("domain_type", "unknown"), 0.5)

        # Signal: HTTPS enabled
        signals["https_enabled"] = 0.7 if domain_data.get("https_enabled", False) else 0.3

        # Signal: Visit success rate
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)
        signals["success_rate"] = min(0.9, pages_analyzed / max(1, visit_count))

        # Signal: TLD trustworthiness
        tld_trust = {
            "edu": 0.9,
            "gov": 0.9,
            "org": 0.7,
            "com": 0.5,
            "net": 0.5,
            "io": 0.6,
            "ai": 0.6
        }
        signals["tld_trust"] = tld_trust.get(domain_data.get("tld", ""), 0.4)

        # Signal: Content quality if available
        if domain_data.get("content_quality") is not None:
            signals["content_quality"] = domain_data["content_quality"]

        # Signal: Topic expertise if established
        if domain in self.topic_expertise and self.topic_expertise[domain]:
            # Average expertise across topics
            signals["topic_expertise"] = sum(self.topic_expertise[domain].values()) / len(self.topic_expertise[domain])

        # Calculate overall authority score
        # Weighted average of available signals
        weights = {
            "domain_type": 0.15,
            "https_enabled": 0.05,
            "success_rate": 0.20,
            "tld_trust": 0.10,
            "content_quality": 0.30,
            "topic_expertise": 0.20
        }

        total_weight = 0
        weighted_sum = 0

        for signal, value in signals.items():
            if signal in weights:
                weighted_sum += value * weights[signal]
                total_weight += weights[signal]

        # Compute final score with normalization
        if total_weight > 0:
            authority_score = weighted_sum / total_weight
        else:
            authority_score = 0.5  # Default if no signals available

        # Store authority score
        self.domain_knowledge[domain]["authority_score"] = authority_score
        self.authority_metrics[domain] = {
            "score": authority_score,
            "signals": signals,
            "timestamp": datetime.now().isoformat()
        }

        return authority_score

    def update_content_knowledge(self, domain, page_url, content_data):
        """
        Update domain knowledge with information from analyzed content.

        Parameters:
        - domain: Domain name
        - page_url: Full URL of the analyzed page
        - content_data: Dict with keys like 'quality', 'topics', 'length', etc.
        """
        if not domain or not page_url or not content_data:
            return False

        # Ensure domain exists in knowledge base
        if domain not in self.domain_knowledge:
            self.analyze_domain(page_url)

        if domain not in self.domain_knowledge:
            return False  # Domain analysis failed

        # Update page count
        self.domain_knowledge[domain]["pages_analyzed"] = self.domain_knowledge[domain].get("pages_analyzed", 0) + 1

        # Store page pattern
        path = urlparse(page_url).path
        page_pattern = self.domain_knowledge[domain].get("page_pattern", {})

        # Analyze path pattern
        path_parts = path.strip('/').split('/')

        # Identify path type
        path_type = "root"
        if len(path_parts) == 1 and path_parts[0]:
            path_type = "top_level"
        elif len(path_parts) > 1:
            path_type = "deep"

        # Update path type counts
        if "path_types" not in page_pattern:
            page_pattern["path_types"] = {"root": 0, "top_level": 0, "deep": 0}

        page_pattern["path_types"][path_type] = page_pattern["path_types"].get(path_type, 0) + 1

        # Update path component statistics
        if "common_segments" not in page_pattern:
            page_pattern["common_segments"] = {}

        for part in path_parts:
            if part:  # Skip empty parts
                page_pattern["common_segments"][part] = page_pattern["common_segments"].get(part, 0) + 1

        # Store updated page pattern
        self.domain_knowledge[domain]["page_pattern"] = page_pattern

        # Update content quality metrics
        if "quality" in content_data:
            quality_score = content_data["quality"]

            # Update rolling average of content quality
            current_quality = self.domain_knowledge[domain].get("content_quality")
            if current_quality is None:
                self.domain_knowledge[domain]["content_quality"] = quality_score
            else:
                # Weight existing score more to avoid large fluctuations
                self.domain_knowledge[domain]["content_quality"] = current_quality * 0.8 + quality_score * 0.2

        # Update topic knowledge
        if "topics" in content_data and content_data["topics"]:
            topics = content_data["topics"]

            # Update known topics for domain
            known_topics = set(self.domain_knowledge[domain].get("known_topics", []))
            known_topics.update(topics)
            self.domain_knowledge[domain]["known_topics"] = list(known_topics)

            # Update topic expertise
            if domain not in self.topic_expertise:
                self.topic_expertise[domain] = {}

            for topic in topics:
                # Increase expertise in this topic
                current_expertise = self.topic_expertise[domain].get(topic, 0.3)  # Start with low expertise
                # Expertise increases with each encounter but plateaus
                self.topic_expertise[domain][topic] = min(0.9, current_expertise + 0.05)

        # Recalculate authority score with new information
        self._calculate_authority_score(domain)

        return True

    def find_related_domains(self, domain, relation_type="topic", max_results=5):
        """
        Find domains related to the given domain by topic or other criteria.

        Parameters:
        - domain: Domain to find relations for
        - relation_type: Type of relation ('topic', 'category', 'link')
        - max_results: Maximum number of results to return
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        related_domains = []

        if relation_type == "topic":
            # Find domains with overlapping topics
            domain_topics = set(self.domain_knowledge[domain].get("known_topics", []))

            if not domain_topics:
                return []  # No topics to match

            # Score domains by topic overlap
            domain_scores = {}

            for d, data in self.domain_knowledge.items():
                if d == domain:
                    continue  # Skip self

                d_topics = set(data.get("known_topics", []))
                if not d_topics:
                    continue  # Skip domains with no topics

                # Calculate Jaccard similarity of topic sets
                overlap = len(domain_topics.intersection(d_topics))
                union = len(domain_topics.union(d_topics))

                if overlap > 0:
                    similarity = overlap / union
                    domain_scores[d] = similarity

            # Sort by similarity score
            sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)
            related_domains = [(d, score) for d, score in sorted_domains[:max_results]]

        elif relation_type == "category":
            # Find domains in the same category
            category = self.domain_knowledge[domain].get("primary_category")

            if not category or category not in self.domain_categories:
                return []

            # Get domains in same category
            category_domains = [d for d in self.domain_categories[category] if d != domain]

            # Sort by authority score
            domain_scores = []
            for d in category_domains:
                if d in self.domain_knowledge:
                    score = self.domain_knowledge[d].get("authority_score", 0.5)
                    domain_scores.append((d, score))

            # Sort by authority score
            sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
            related_domains = sorted_domains[:max_results]

        elif relation_type == "link":
            # Find domains that share links (requires backlink tracking)
            # This would be implemented with the relation graph
            if domain in self.relation_graph and "links_to" in self.relation_graph[domain]:
                links = self.relation_graph[domain]["links_to"]
                related_domains = [(d, 1.0) for d in links][:max_results]

        return related_domains

    def get_domain_knowledge(self, domain):
        """
        Retrieve comprehensive knowledge about a domain.

        Parameters:
        - domain: Domain name to retrieve knowledge for
        """
        # Handle both domain name and full URL
        if not domain:
            return None

        # Check if this is a URL and extract domain
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        if not domain:
            return None

        # Return domain knowledge if exists
        if domain in self.domain_knowledge:
            # Create a copy to avoid direct modification
            knowledge = self.domain_knowledge[domain].copy()

            # Add additional related information
            knowledge["authority_metrics"] = self.authority_metrics.get(domain, {})
            knowledge["topic_expertise"] = self.topic_expertise.get(domain, {})

            # Add anomaly records if they exist
            if domain in self.anomaly_records:
                knowledge["anomalies"] = self.anomaly_records[domain]

            # Add related domains
            related_by_topic = self.find_related_domains(domain, "topic", 3)
            if related_by_topic:
                knowledge["related_domains"] = [d for d, _ in related_by_topic]

            return knowledge

        return None

    def detect_domain_anomalies(self, domain):
        """
        Analyze domain for potential anomalies or suspicious patterns.

        Parameters:
        - domain: Domain to check for anomalies
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        anomalies = []
        domain_data = self.domain_knowledge[domain]

        # Anomaly 1: TLD mismatch with content
        tld = domain_data.get("tld", "")
        category = domain_data.get("primary_category")

        if tld == "edu" and category and category not in ["academic", "reference"]:
            anomalies.append({
                "type": "tld_category_mismatch",
                "description": f"Domain has .edu TLD but content suggests '{category}' category",
                "severity": "medium"
            })

        # Anomaly 2: Low content quality on authoritative TLD
        content_quality = domain_data.get("content_quality")
        if content_quality and content_quality < 0.4 and tld in ["edu", "gov", "org"]:
            anomalies.append({
                "type": "low_quality_authoritative",
                "description": f"Domain with .{tld} TLD has unusually low content quality ({content_quality:.2f})",
                "severity": "high"
            })

        # Anomaly 3: Unusual page pattern
        page_pattern = domain_data.get("page_pattern", {})
        path_types = page_pattern.get("path_types", {})

        if path_types.get("deep", 0) > 10 * max(1, path_types.get("top_level", 0) + path_types.get("root", 0)):
            anomalies.append({
                "type": "unusual_path_pattern",
                "description": "Domain shows unusually high ratio of deep paths to top-level content",
                "severity": "low"
            })

        # Anomaly 4: Visit count vs pages analyzed mismatch
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)

        if visit_count > 5 and pages_analyzed / visit_count < 0.3:
            anomalies.append({
                "type": "low_analysis_success_rate",
                "description": f"Domain has low success rate: {pages_analyzed}/{visit_count} visits produced content",
                "severity": "medium"
            })

        # Store anomalies if found
        if anomalies:
            self.anomaly_records[domain] = {
                "detected": anomalies,
                "timestamp": datetime.now().isoformat()
            }

            # Log high severity anomalies
            high_severity = [a for a in anomalies if a.get("severity") == "high"]
            if high_severity:
                for anomaly in high_severity:
                    log_event(f"Domain anomaly detected for {domain}: {anomaly['description']}", "WARNING")

        return anomalies

    def get_domain_recommendation(self, current_domain, purpose="exploration"):
        """
        Recommend related domains based on current domain and purpose.

        Parameters:
        - current_domain: The current domain
        - purpose: Why we need recommendations ('exploration', 'deepening', 'verification')
        """
        if not current_domain:
            # Recommend highly trusted domains by default
            trusted_domains = self._get_top_domains_by_authority(5)
            if trusted_domains:
                return {
                    "recommendations": trusted_domains,
                    "reason": "Default trusted domains for general exploration"
                }
            return None

        # Extract domain from URL if needed
        if current_domain.startswith(('http://', 'https://')):
            current_domain = urlparse(current_domain).netloc

        # Check domain knowledge
        if current_domain not in self.domain_knowledge:
            return None

        domain_data = self.domain_knowledge[current_domain]

        # Different recommendation strategies based on purpose
        if purpose == "exploration":
            # Favor topic-related domains with high authority
            related = self.find_related_domains(current_domain, "topic", 10)

            # Filter for good authority
            good_related = [(domain, score) for domain, score in related
                          if domain in self.domain_knowledge
                          and self.domain_knowledge[domain].get("authority_score", 0) > 0.6]

            if good_related:
                return {
                    "recommendations": [domain for domain, _ in good_related[:5]],
                    "reason": f"Topic-related domains to {current_domain} with good authority"
                }

        elif purpose == "deepening":
            # Focus on same category with highest topic expertise
            category = domain_data.get("primary_category")
            if category and category in self.domain_categories:
                # Get domains in same category
                category_domains = [d for d in self.domain_categories[category] if d != current_domain]

                # Score by topic expertise and authority
                domain_scores = []

                for d in category_domains:
                    if d in self.domain_knowledge and d in self.topic_expertise:
                        # Average topic expertise
                        topics = self.topic_expertise[d]
                        if topics:
                            avg_expertise = sum(topics.values()) / len(topics)
                            authority = self.domain_knowledge[d].get("authority_score", 0.5)

                            # Combined score weighing expertise more
                            score = avg_expertise * 0.7 + authority * 0.3
                            domain_scores.append((d, score))

                if domain_scores:
                    sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
                    return {
                        "recommendations": [domain for domain, _ in sorted_domains[:5]],
                        "reason": f"Domains with deep expertise in {category} category"
                    }

        elif purpose == "verification":
            # Find authoritative domains in same topic area
            topics = domain_data.get("known_topics", [])
            if not topics:
                return None

            # Find domains with same topics but higher authority
            current_authority = domain_data.get("authority_score", 0.5)
            verification_candidates = []

            for d, data in self.domain_knowledge.items():
                if d == current_domain:
                    continue

                # Check topic overlap
                d_topics = set(data.get("known_topics", []))
                overlap = len(set(topics).intersection(d_topics))

                if overlap > 0 and data.get("authority_score", 0) > current_authority:
                    # Score by authority and topic overlap
                    score = data.get("authority_score", 0) * (overlap / len(topics))
                    verification_candidates.append((d, score))

            if verification_candidates:
                sorted_candidates = sorted(verification_candidates, key=lambda x: x[1], reverse=True)
                return {
                    "recommendations": [domain for domain, _ in sorted_candidates[:5]],
                    "reason": f"More authoritative sources on same topics as {current_domain}"
                }

        # Default to highest authority domains as fallback
        return {
            "recommendations": self._get_top_domains_by_authority(5),
            "reason": "General high-authority domains (fallback recommendation)"
        }

    def _get_top_domains_by_authority(self, count=5):
        """Get the top domains by authority score"""
        if not self.domain_knowledge:
            return []

        # Sort domains by authority score
        scored_domains = [(d, data.get("authority_score", 0))
                        for d, data in self.domain_knowledge.items()]
        sorted_domains = sorted(scored_domains, key=lambda x: x[1], reverse=True)

        return [domain for domain, _ in sorted_domains[:count]]

    def export_domain_report(self, domain):
        """
        Generate comprehensive report on domain for external use.

        Parameters:
        - domain: Domain to generate report for
        """
        if not domain or domain not in self.domain_knowledge:
            return None

        knowledge = self.get_domain_knowledge(domain)
        if not knowledge:
            return None

        # Generate anomaly section if needed
        anomalies = self.detect_domain_anomalies(domain)
        anomaly_section = None
        if anomalies:
            anomaly_section = {
                "count": len(anomalies),
                "details": anomalies,
                "recommended_action": "caution" if any(a.get("severity") == "high" for a in anomalies) else "monitor"
            }

        # Get related domains
        topic_related = self.find_related_domains(domain, "topic", 5)
        category_related = self.find_related_domains(domain, "category", 5)

        # Compile report
        report = {
            "domain": domain,
            "analysis_date": datetime.now().isoformat(),
            "summary": {
                "type": knowledge.get("domain_type", "unknown"),
                "category": knowledge.get("primary_category", "unknown"),
                "authority_score": knowledge.get("authority_score", 0),
                "content_quality": knowledge.get("content_quality"),
                "visit_count": knowledge.get("visit_count", 0),
                "pages_analyzed": knowledge.get("pages_analyzed", 0)
            },
            "authority_assessment": {
                "score": knowledge.get("authority_score", 0),
                "factors": knowledge.get("authority_metrics", {}).get("signals", {}),
                "interpretation": self._interpret_authority_score(knowledge.get("authority_score", 0))
            },
            "content_profile": {
                "known_topics": knowledge.get("known_topics", []),
                "path_patterns": knowledge.get("page_pattern", {}).get("path_types", {}),
                "https_enabled": knowledge.get("https_enabled", False)
            },
            "related_domains": {
                "by_topic": [d for d, _ in topic_related],
                "by_category": [d for d, _ in category_related]
            }
        }

        # Add anomalies if detected
        if anomaly_section:
            report["anomalies"] = anomaly_section

        return report

    def _interpret_authority_score(self, score):
        """Provide interpretation of authority score"""
        if score >= 0.8:
            return "Very High Authority - Likely a definitive source in its field"
        elif score >= 0.7:
            return "High Authority - Generally trustworthy and established source"
        elif score >= 0.5:
            return "Moderate Authority - Adequate source but verification recommended"
        elif score >= 0.3:
            return "Low Authority - Approach with caution, verify information"
        else:
            return "Very Low Authority - Not recommended as a primary information source"

    def get_intelligence_statistics(self):
        """Generate statistics about domain intelligence database"""
        if not self.domain_knowledge:
            return {"status": "empty", "domains_analyzed": 0}

        # Basic counts
        domain_count = len(self.domain_knowledge)

        # Category distribution
        category_counts = {}
        for category, domains in self.domain_categories.items():
            category_counts[category] = len(domains)

        # Authority distribution
        authority_ranges = {
            "very_high": 0,  # 0.8-1.0
            "high": 0,       # 0.6-0.8
            "moderate": 0,   # 0.4-0.6
            "low": 0,        # 0.2-0.4
            "very_low": 0    # 0.0-0.2
        }

        for domain, data in self.domain_knowledge.items():
            score = data.get("authority_score", 0.5)

            if score >= 0.8:
                authority_ranges["very_high"] += 1
            elif score >= 0.6:
                authority_ranges["high"] += 1
            elif score >= 0.4:
                authority_ranges["moderate"] += 1
            elif score >= 0.2:
                authority_ranges["low"] += 1
            else:
                authority_ranges["very_low"] += 1

        # TLD distribution
        tld_counts = {}
        for domain, data in self.domain_knowledge.items():
            tld = data.get("tld", "unknown")
            tld_counts[tld] = tld_counts.get(tld, 0) + 1

        # Anomaly statistics
        anomaly_count = sum(1 for domain in self.anomaly_records)
        high_severity_count = sum(
            1 for domain, record in self.anomaly_records.items()
            if any(a.get("severity") == "high" for a in record.get("detected", []))
        )

        return {
            "status": "active",
            "domains_analyzed": domain_count,
            "category_distribution": category_counts,
            "authority_distribution": authority_ranges,
            "tld_distribution": tld_counts,
            "anomalies_detected": anomaly_count,
            "high_severity_anomalies": high_severity_count
        }





class MetaLearningModule:
    """
    Advanced meta-learning system that monitors, analyzes, and optimizes
    the learning process itself, enabling autonomous improvement of the
    model's learning capabilities over time.
    """
    def __init__(self, model):
        self.model = model
        self.learning_history = deque(maxlen=100)  # Track recent learning metrics
        self.hyperparameter_history = []  # Track hyperparameter evolution
        self.architecture_history = []  # Track architecture changes
        self.performance_trends = {}  # Performance over time for different metrics
        self.learning_rate_schedule = []  # History of learning rate adjustments
        self.optimization_state = "exploration"  # Current optimization phase
        self.training_cycles = 0  # Total training cycles performed
        self.gradient_statistics = deque(maxlen=20)  # Recent gradient statistics
        self.weight_evolution = {}  # Track how weights evolve over time
        self.improvement_rates = {}  # Rate of improvement for different metrics

        # Meta-parameters (parameters about parameter learning)
        self.meta_params = {
            "exploration_rate": 0.3,  # How much to explore hyperparameter space
            "stability_threshold": 0.05,  # Threshold for stability detection
            "adaptation_rate": 0.2,  # How quickly to adapt to new information
            "patience": 5,  # Cycles to wait before making significant changes
            "learning_rate_bounds": (1e-6, 1e-2),  # Min/max learning rate
            "trend_window": 10,  # Window size for trend analysis
            "architecture_change_threshold": 0.1  # Threshold for architecture changes
        }

        # Initialize monitoring
        log_event("MetaLearningModule initialized with meta-optimization capabilities", "INFO")

    def track_performance(self, metrics):
        """
        Track and analyze training performance metrics over time
        to inform meta-learning decisions.

        Parameters:
        - metrics: Dict containing performance metrics (loss, accuracy, etc.)
        """
        self.training_cycles += 1

        # Skip invalid metrics
        if not isinstance(metrics, dict) or len(metrics) == 0:
            return

        # Store metrics with timestamp
        timestamped_metrics = metrics.copy()
        timestamped_metrics["cycle"] = self.training_cycles
        timestamped_metrics["timestamp"] = datetime.now().isoformat()

        # Add current learning rate if available
        if hasattr(self.model, '_current_lr'):
            timestamped_metrics["learning_rate"] = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                timestamped_metrics["learning_rate"] = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Store in history
        self.learning_history.append(timestamped_metrics)

        # Update performance trends when we have enough data
        if len(self.learning_history) >= self.meta_params["trend_window"]:
            self._update_performance_trends()

        # Analyze learning periodically
        if self.training_cycles % max(1, self.meta_params["patience"]) == 0:
            self.analyze_trends()

    def _update_performance_trends(self):
        """Update trend analysis for each tracked metric"""
        window = self.meta_params["trend_window"]
        recent_metrics = list(self.learning_history)[-window:]

        # Find consistent metrics across all entries
        metric_keys = set.intersection(*[set(m.keys()) for m in recent_metrics])
        metric_keys = [k for k in metric_keys if k not in ["cycle", "timestamp", "learning_rate"]]

        for key in metric_keys:
            # Extract values
            values = [m[key] for m in recent_metrics if key in m]

            if not values or len(values) < window:
                continue

            # Calculate trend statistics
            mean_value = sum(values) / len(values)
            min_value = min(values)
            max_value = max(values)
            range_value = max_value - min_value

            # Calculate first derivative (rate of change)
            derivatives = [values[i] - values[i-1] for i in range(1, len(values))]
            mean_derivative = sum(derivatives) / len(derivatives) if derivatives else 0

            # Calculate second derivative (acceleration of change)
            accelerations = [derivatives[i] - derivatives[i-1] for i in range(1, len(derivatives))]
            mean_acceleration = sum(accelerations) / len(accelerations) if accelerations else 0

            # Interpret trend direction
            if abs(mean_derivative) < self.meta_params["stability_threshold"]:
                direction = "stable"
            elif mean_derivative < 0:
                direction = "decreasing"
            else:
                direction = "increasing"

            # Interpret acceleration
            if abs(mean_acceleration) < self.meta_params["stability_threshold"] / 2:
                acceleration = "steady"
            elif mean_acceleration < 0:
                acceleration = "decelerating"
            else:
                acceleration = "accelerating"

            # Store trend information
            self.performance_trends[key] = {
                "current": values[-1],
                "mean": mean_value,
                "min": min_value,
                "max": max_value,
                "range": range_value,
                "direction": direction,
                "rate_of_change": mean_derivative,
                "acceleration": acceleration,
                "acceleration_value": mean_acceleration,
                "updated_at": datetime.now().isoformat()
            }

            # Calculate improvement rate (relative to starting point)
            if len(values) > 1:
                # For loss, lower is better; for accuracy, higher is better
                is_loss_like = "loss" in key.lower() or "error" in key.lower()

                start_value = values[0]
                end_value = values[-1]

                if is_loss_like:
                    # For loss metrics, improvement is decrease
                    improvement = (start_value - end_value) / max(0.0001, start_value)
                else:
                    # For other metrics, improvement is increase
                    improvement = (end_value - start_value) / max(0.0001, start_value)

                self.improvement_rates[key] = improvement

    def analyze_trends(self):
        """
        Analyze learning trends and make meta-learning decisions
        for hyperparameter and architecture adaptation.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            log_event(f"Insufficient learning history for meta-analysis (need {self.meta_params['trend_window']})", "INFO")
            return

        # Check if we have loss metrics
        loss_metrics = [k for k in self.performance_trends.keys()
                       if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            log_event("No loss metrics found for meta-learning trend analysis", "WARNING")
            return

        # Use first loss metric as primary indicator
        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Current state assessment
        current_state = {
            "direction": loss_trend["direction"],
            "acceleration": loss_trend["acceleration"],
            "value": loss_trend["current"],
            "improvement_rate": self.improvement_rates.get(primary_loss, 0)
        }

        # Decision making based on loss trend patterns
        decisions = self._make_meta_decisions(current_state, primary_loss)

        # Log significant decisions
        for decision in decisions:
            if decision["type"] in ["learning_rate_change", "architecture_change"]:
                log_event(f"Meta-learning decision: {decision['description']}", "INFO")

        # Apply decisions
        self._apply_meta_decisions(decisions)

    def _make_meta_decisions(self, current_state, primary_metric):
        """
        Make meta-learning decisions based on current learning state.

        Parameters:
        - current_state: Dict describing current trend state
        - primary_metric: Name of the primary metric being analyzed

        Returns:
        - List of decision dictionaries with type and parameters
        """
        decisions = []

        # Case 1: Learning plateaued (stable with low improvement)
        if (current_state["direction"] == "stable" and
            abs(current_state["improvement_rate"]) < self.meta_params["stability_threshold"]):

            # Check duration of plateau
            plateau_duration = self._count_consecutive_states("stable", primary_metric)

            if plateau_duration >= self.meta_params["patience"]:
                # Long plateau - need significant change
                if self.optimization_state == "exploration":
                    # In exploration phase, try architecture change
                    decisions.append({
                        "type": "architecture_change",
                        "change": "expand" if random.random() < 0.7 else "contract",
                        "description": f"Architecture change due to {plateau_duration}-cycle plateau in {primary_metric}"
                    })

                    # Also try learning rate restart
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": 10.0,
                        "description": "Learning rate increase to escape plateau"
                    })
                else:
                    # In refinement phase, smaller learning rate adjustment
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": random.uniform(2.0, 5.0),
                        "description": "Learning rate adjustment to overcome plateau"
                    })

                # Switch optimization state
                decisions.append({
                    "type": "optimization_state_change",
                    "new_state": "refinement" if self.optimization_state == "exploration" else "exploration",
                    "description": f"Switch optimization strategy due to {plateau_duration}-cycle plateau"
                })

        # Case 2: Loss increasing (getting worse)
        elif current_state["direction"] == "increasing" and "loss" in primary_metric.lower():
            # Loss is getting worse - need to reduce learning rate

            # Check if it's accelerating or steady
            if current_state["acceleration"] in ["accelerating", "steady"]:
                # Significant reduction needed
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.1,  # 10x reduction
                    "description": f"{primary_metric} {current_state['acceleration']} increase - significant LR reduction"
                })
            else:
                # Moderate reduction for decelerating increase
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.5,  # 2x reduction
                    "description": f"{primary_metric} decelerating increase - moderate LR reduction"
                })

        # Case 3: Loss decreasing nicely (improvement)
        elif current_state["direction"] == "decreasing" and "loss" in primary_metric.lower():
            # Loss is decreasing - check acceleration

            if current_state["acceleration"] == "accelerating":
                # Getting better faster - slightly increase learning rate
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 1.1,  # 10% increase
                    "description": f"{primary_metric} accelerating decrease - slight LR increase"
                })
            elif current_state["acceleration"] == "decelerating" and random.random() < 0.5:
                # Slowing improvement - try architecture change
                decisions.append({
                    "type": "architecture_change",
                    "change": "expand",
                    "description": f"{primary_metric} decelerating decrease - architecture expansion"
                })

        # Case 4: Random exploration if no clear pattern and in exploration mode
        elif self.optimization_state == "exploration" and random.random() < self.meta_params["exploration_rate"]:
            # Random exploration of hyperparameter space
            exploration_choices = ["learning_rate_change", "architecture_change"]
            exploration_type = random.choice(exploration_choices)

            if exploration_type == "learning_rate_change":
                factor = random.choice([0.5, 0.7, 1.5, 2.0])
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": factor,
                    "description": f"Exploratory learning rate adjustment (factor: {factor})"
                })
            else:
                change = random.choice(["expand", "contract"])
                decisions.append({
                    "type": "architecture_change",
                    "change": change,
                    "description": f"Exploratory architecture {change}"
                })

        return decisions

    def _apply_meta_decisions(self, decisions):
        """
        Apply meta-learning decisions to the model and optimizer.

        Parameters:
        - decisions: List of decision dictionaries
        """
        for decision in decisions:
            decision_type = decision.get("type", "")

            if decision_type == "learning_rate_change":
                factor = decision.get("factor", 1.0)
                self._adjust_learning_rate(factor)

            elif decision_type == "architecture_change":
                change = decision.get("change", "")
                self._adjust_architecture(change)

            elif decision_type == "optimization_state_change":
                new_state = decision.get("new_state", self.optimization_state)
                self.optimization_state = new_state
                log_event(f"Meta-learning optimization state changed to: {new_state}", "INFO")

    def _adjust_learning_rate(self, factor):
        """
        Adjust learning rate by the given factor.

        Parameters:
        - factor: Multiplication factor for current learning rate
        """
        # Get current learning rate
        current_lr = None

        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        if current_lr is None:
            log_event("Could not access current learning rate for adjustment", "ERROR")
            return

        # Calculate new learning rate
        new_lr = current_lr * factor

        # Ensure it's within bounds
        min_lr, max_lr = self.meta_params["learning_rate_bounds"]
        new_lr = max(min_lr, min(max_lr, new_lr))

        # Apply to model attribute
        if hasattr(self.model, '_current_lr'):
            setattr(self.model, '_current_lr', new_lr)

        # Apply to optimizer
        if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr
            except Exception as e:
                log_event(f"Error adjusting optimizer learning rate: {e}", "ERROR")

        # Record the change
        self.learning_rate_schedule.append({
            "cycle": self.training_cycles,
            "old_lr": current_lr,
            "new_lr": new_lr,
            "factor": factor,
            "timestamp": datetime.now().isoformat()
        })

        log_event(f"Meta-learning adjusted learning rate: {current_lr:.6f} → {new_lr:.6f} (factor: {factor})", "INFO")

    def _adjust_architecture(self, change):
        """
        Apply architecture changes based on meta-learning decisions.

        Parameters:
        - change: Type of architecture change ('expand' or 'contract')
        """
        if change not in ["expand", "contract"]:
            log_event(f"Invalid architecture change type: {change}", "ERROR")
            return

        # Check if model supports architecture changes
        expand_method = getattr(self.model, 'expand_architecture', None)
        contract_method = getattr(self.model, 'contract_architecture', None)

        if change == "expand" and callable(expand_method):
            try:
                self.model.expand_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "expand",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning expanded model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error expanding architecture: {e}", "ERROR")

        elif change == "contract" and callable(contract_method):
            try:
                self.model.contract_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "contract",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning contracted model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error contracting architecture: {e}", "ERROR")
        else:
            log_event(f"Model does not support architecture {change} operations", "WARNING")

        return False

    def _count_consecutive_states(self, state_type, metric):
        """
        Count how many consecutive cycles the metric has been in given state.

        Parameters:
        - state_type: The state to count ('stable', 'increasing', 'decreasing')
        - metric: The metric name to analyze

        Returns:
        - Count of consecutive cycles in that state
        """
        count = 0
        history = list(self.learning_history)

        # Need at least window_size entries to have computed trends
        if len(history) < self.meta_params["trend_window"]:
            return 0

        # Go through trend history (would need to store trend history to be more accurate)
        # This is an approximation based on current trend
        if metric in self.performance_trends:
            if self.performance_trends[metric]["direction"] == state_type:
                # If current state matches, estimate duration based on trend strength
                rate = abs(self.performance_trends[metric]["rate_of_change"])
                threshold = self.meta_params["stability_threshold"]

                if state_type == "stable" and rate < threshold:
                    # Estimate how long we've been stable based on how close to zero the rate is
                    stability_ratio = max(0, 1 - rate/threshold)
                    count = int(self.meta_params["patience"] * stability_ratio) + 1
                else:
                    # For increasing/decreasing, at least 1 cycle
                    count = 1

        return count

    def track_gradient_statistics(self, gradients):
        """
        Track gradient statistics for meta-learning analysis.

        Parameters:
        - gradients: Dict mapping parameter names to gradient tensors
        """
        if not gradients:
            return

        # Calculate gradient statistics
        grad_stats = {}

        try:
            for name, grad in gradients.items():
                if grad is None:
                    continue

                # Convert to numpy for stats calculation if needed
                grad_np = grad.detach().cpu().numpy() if hasattr(grad, 'detach') else grad

                # Calculate statistics
                grad_stats[name] = {
                    "mean": float(np.mean(grad_np)),
                    "std": float(np.std(grad_np)),
                    "min": float(np.min(grad_np)),
                    "max": float(np.max(grad_np)),
                    "norm": float(np.linalg.norm(grad_np))
                }
        except Exception as e:
            log_event(f"Error calculating gradient statistics: {e}", "ERROR")
            return

        # Add overall statistics
        means = [stats["mean"] for stats in grad_stats.values()]
        norms = [stats["norm"] for stats in grad_stats.values()]

        if means and norms:
            grad_stats["overall"] = {
                "mean_of_means": sum(means) / len(means),
                "mean_of_norms": sum(norms) / len(norms),
                "cycle": self.training_cycles
            }

        # Store in history
        self.gradient_statistics.append(grad_stats)

        # Analyze for gradient issues
        self._analyze_gradient_health(grad_stats)

    def _analyze_gradient_health(self, grad_stats):
        """
        Analyze gradient health for issues like vanishing/exploding gradients.

        Parameters:
        - grad_stats: Dictionary of gradient statistics
        """
        if "overall" not in grad_stats:
            return

        overall = grad_stats["overall"]
        mean_norm = overall["mean_of_norms"]

        # Check for vanishing gradients
        if mean_norm < 1e-7:
            log_event(f"Potential vanishing gradient detected: mean norm = {mean_norm:.8f}", "WARNING")

            # Suggest learning rate increase
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(5.0)  # Significant increase

        # Check for exploding gradients
        elif mean_norm > 1e2:
            log_event(f"Potential exploding gradient detected: mean norm = {mean_norm:.2f}", "WARNING")

            # Suggest learning rate decrease
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(0.1)  # Significant decrease

    def track_weight_evolution(self, layer_name, weight_tensor):
        """
        Track how weights evolve over time for specific layers.

        Parameters:
        - layer_name: Name of the layer
        - weight_tensor: Tensor containing weights
        """
        if layer_name not in self.weight_evolution:
            self.weight_evolution[layer_name] = []

        try:
            # Calculate statistics from tensor
            weight_np = weight_tensor.detach().cpu().numpy() if hasattr(weight_tensor, 'detach') else weight_tensor

            stats = {
                "mean": float(np.mean(weight_np)),
                "std": float(np.std(weight_np)),
                "norm": float(np.linalg.norm(weight_np)),
                "cycle": self.training_cycles
            }

            # Only store periodically to save memory
            if self.training_cycles % 10 == 0:
                self.weight_evolution[layer_name].append(stats)

                # Limit history size
                max_history = 50
                if len(self.weight_evolution[layer_name]) > max_history:
                    self.weight_evolution[layer_name] = self.weight_evolution[layer_name][-max_history:]

        except Exception as e:
            log_event(f"Error tracking weight evolution for {layer_name}: {e}", "ERROR")

    def get_meta_learning_report(self):
        """
        Generate comprehensive report on meta-learning status and insights.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            return {
                "status": "initializing",
                "cycles": self.training_cycles,
                "message": f"Collecting initial metrics ({len(self.learning_history)}/{self.meta_params['trend_window']} cycles)"
            }

        # Get performance trends
        trends = {}
        for metric, trend in self.performance_trends.items():
            trends[metric] = {
                "current": trend["current"],
                "direction": trend["direction"],
                "acceleration": trend["acceleration"],
                "improvement": self.improvement_rates.get(metric, 0)
            }

        # Get learning rate history
        lr_history = [{
            "cycle": item["cycle"],
            "learning_rate": item["new_lr"]
        } for item in self.learning_rate_schedule[-5:]]  # Last 5 changes

        # Get architecture change history
        arch_history = self.architecture_history[-5:]  # Last 5 changes

        # System state assessment
        state_assessment = self._assess_system_state()

        # Generate recommendations
        recommendations = self._generate_meta_learning_recommendations()

        return {
            "status": "active",
            "cycles": self.training_cycles,
            "optimization_state": self.optimization_state,
            "performance_trends": trends,
            "learning_rate_history": lr_history,
            "architecture_history": arch_history,
            "system_state": state_assessment,
            "recommendations": recommendations
        }

    def _assess_system_state(self):
        """Assess overall system state from meta-learning perspective"""
        # Find primary loss metric
        loss_metrics = [k for k in self.performance_trends.keys()
                      if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            return {"state": "unknown", "confidence": 0}

        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Check for issues
        issues = []

        # Issue 1: Plateaued loss
        if loss_trend["direction"] == "stable" and loss_trend["current"] > 0.1:
            issues.append("training_plateau")

        # Issue 2: Loss increasing
        if loss_trend["direction"] == "increasing" and "loss" in primary_loss.lower():
            issues.append("loss_increasing")

        # Issue 3: Slow progress
        if abs(self.improvement_rates.get(primary_loss, 0)) < 0.01:
            issues.append("slow_progress")

        # Issue 4: Gradient issues
        if len(self.gradient_statistics) > 0:
            last_grad = self.gradient_statistics[-1]
            if "overall" in last_grad:
                norm = last_grad["overall"]["mean_of_norms"]
                if norm < 1e-7:
                    issues.append("vanishing_gradients")
                elif norm > 1e2:
                    issues.append("exploding_gradients")

        # Determine overall state
        overall_state = "healthy"
        if len(issues) == 1:
            overall_state = "concerning"
        elif len(issues) > 1:
            overall_state = "problematic"

        # Calculate confidence in assessment
        confidence = min(0.9, 0.5 + 0.1 * len(self.learning_history) / self.meta_params["trend_window"])

        return {
            "state": overall_state,
            "issues": issues,
            "confidence": confidence
        }

    def _generate_meta_learning_recommendations(self):
        """Generate recommendations for system optimization"""
        recommendations = []

        # Get system state
        state = self._assess_system_state()
        issues = state.get("issues", [])

        # Recommendation 1: Learning rate adjustments
        if "training_plateau" in issues:
            if self.optimization_state == "exploration":
                recommendations.append({
                    "type": "learning_rate",
                    "action": "increase",
                    "factor": 5.0,
                    "reason": "Escape plateau by exploring higher learning rates"
                })
            else:
                recommendations.append({
                    "type": "learning_rate",
                    "action": "cyclic_schedule",
                    "reason": "Implement cyclic learning rate to overcome plateau"
                })

        elif "loss_increasing" in issues:
            recommendations.append({
                "type": "learning_rate",
                "action": "decrease",
                "factor": 0.2,
                "reason": "Reduce learning rate to stabilize increasing loss"
            })

        # Recommendation 2: Architecture adjustments
        if "slow_progress" in issues:
            if len(self.architecture_history) < 2:
                recommendations.append({
                    "type": "architecture",
                    "action": "expand",
                    "reason": "Increase model capacity to improve learning progress"
                })
            else:
                last_change = self.architecture_history[-1]["change"]
                recommendations.append({
                    "type": "architecture",
                    "action": "expand" if last_change == "contract" else "contract",
                    "reason": "Alternate architecture changes to find optimal complexity"
                })

        # Recommendation 3: Gradient-based recommendations
        if "vanishing_gradients" in issues:
            recommendations.append({
                "type": "initialization",
                "action": "reinitialize",
                "reason": "Reinitialize weights to address vanishing gradients"
            })
        elif "exploding_gradients" in issues:
            recommendations.append({
                "type": "regularization",
                "action": "increase",
                "reason": "Increase regularization to address exploding gradients"
            })

        # Recommendation 4: Exploration/exploitation balance
        if self.training_cycles > 50 and self.optimization_state == "exploration" and not issues:
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_refinement",
                "reason": "Switch to refinement mode after successful exploration phase"
            })
        elif len(issues) > 1 and self.optimization_state == "refinement":
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_exploration",
                "reason": "Return to exploration mode to address multiple issues"
            })

        return recommendations

    def adjust_hyperparameters(self, loss_level="normal"):
        """
        Dynamically adjust hyperparameters based on current loss level.

        Parameters:
        - loss_level: Qualitative assessment of current loss ("high", "normal", "low")
        """
        # Get current learning rate
        current_lr = LEARNING_RATE  # Default global value

        # Try to get from model attribute
        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Adjust based on loss level
        new_lr = current_lr
        adjustment_factor = 1.0

        if loss_level == "high":
            # High loss - reduce learning rate
            adjustment_factor = 0.9
            new_lr = current_lr * adjustment_factor
        elif loss_level == "low" and self.optimization_state == "exploration":
            # Low loss in exploration mode - try higher learning rate
            adjustment_factor = 1.1
            new_lr = current_lr * adjustment_factor

        # Apply change if significant
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold

            LEARNING_RATE = new_lr
            log_event(f"Meta-learning: Learning rate adjusted from {current_lr:.6f} to {new_lr:.6f}", "INFO")

            # Update model if possible
            if hasattr(self.model, '_current_lr'):
                setattr(self.model, '_current_lr', new_lr)

            # Update optimizer if available
            if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
                try:
                    for param_group in self.model.optimizer.param_groups:
                        param_group['lr'] = new_lr
                except Exception as e:
                    log_event(f"Error updating optimizer learning rate: {e}", "ERROR")

            # Record the change
            self.hyperparameter_history.append({
                "parameter": "learning_rate",
                "old_value": current_lr,
                "new_value": new_lr,
                "adjustment_factor": adjustment_factor,
                "cycle": self.training_cycles,
                "timestamp": datetime.now().isoformat()
            })

            return True

        return False





        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful
        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.

        Parameters:
        - agent: Agent to evaluate

        Returns:
        - Dictionary of fitness scores across dimensions
        """
        fitness_scores = {
            "performance": 0.5,  # Default medium score
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }

        # 1. Evaluate performance based on learning metrics
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # Check for performance trends
            if hasattr(meta_learning, 'performance_trends'):
                perf_trends = meta_learning.performance_trends

                # Look for loss metrics
                for key, trend in perf_trends.items():
                    if 'loss' in key.lower():
                        # Lower loss is better
                        loss_value = trend.get('current', 0.5)
                        # Convert loss to performance score (inverse relationship)
                        # We expect loss in range 0-2, so transform to 0-1 score
                        performance_score = max(0.1, min(0.9, 1.0 - loss_value/2))
                        fitness_scores["performance"] = performance_score
                        break

        # 2. Evaluate efficiency based on resource usage
        # Simple heuristic: larger models are less efficient
        model_size = 0
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            model_size = len(agent.model.neocortex)
            # Normalize size to efficiency score (inverse relationship)
            base_size = 8  # Expected baseline
            # Efficiency decreases as model grows beyond base size
            efficiency_score = max(0.2, min(0.9, base_size / max(base_size, model_size)))
            fitness_scores["efficiency"] = efficiency_score

        # 3. Evaluate adaptability based on learning rate adjustments
        adaptability_score = 0.5  # Default
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # More learning rate adjustments suggests higher adaptability
            if hasattr(meta_learning, 'learning_rate_schedule'):
                adjustments = len(meta_learning.learning_rate_schedule)
                # More adjustments = more adaptable, up to a point
                adaptability_score = min(0.9, 0.4 + 0.1 * min(5, adjustments))

        fitness_scores["adaptability"] = adaptability_score

        # 4. Evaluate robustness based on error recovery
        robustness_score = 0.5  # Default
        if hasattr(agent, 'ai_manager'):
            # Check error recovery attempts
            error_recovery = getattr(agent.ai_manager, 'error_recovery_attempts', 0)

            if error_recovery > 0:
                # Lower recovery attempts = more robust
                robustness_score = max(0.1, 0.9 - 0.1 * min(8, error_recovery))
            else:
                # No error recovery needed = highly robust
                robustness_score = 0.8

        fitness_scores["robustness"] = robustness_score

        # 5. Evaluate complexity based on model architecture
        complexity_score = 0.5  # Default
        if hasattr(agent, 'model'):
            # Estimate complexity from model parameters
            total_params = self._estimate_model_parameters(agent.model)

            # Normalize by expected parameter count range
            expected_range = 10000000  # 10M parameters as reference
            normalized_complexity = min(1.0, total_params / expected_range)
            complexity_score = normalized_complexity

            fitness_scores["complexity"] = complexity_score

            # Store the current fitness scores
            self.fitness_metrics[self.evolution_generation] = fitness_scores

            log_event(f"Fitness Scores: {fitness_scores}", "DEBUG") # <---- ADD THIS LOGGING

            return fitness_scores

    def _estimate_model_parameters(self, model):
        """
        Estimate the number of parameters in the model.

        Parameters:
        - model: PyTorch model

        Returns:
        - Estimated parameter count
        """
        if not model:
            return 0

        try:
            # Try to use PyTorch's parameter counting
            return sum(p.numel() for p in model.parameters())
        except:
            # Fallback to estimation based on architecture
            if hasattr(model, 'neocortex'):
                layers = len(model.neocortex)
                embed_dim = getattr(model, 'embed_dim', 512)

                # Rough estimate based on transformer-like architecture
                # Each layer has attention, feed-forward, etc.
                params_per_layer = embed_dim * embed_dim * 4  # Simplistic estimate
                return layers * params_per_layer
            else:
                return 1000000  # Default fallback

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores
        and goal weights.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Pressure score (0.0-1.0) indicating how strongly evolution is needed
        """
        # No evolution pressure if no fitness scores
        if not fitness_scores:
            return 0.0

        # Calculate weighted fitness
        weighted_fitness = 0.0
        total_weight = 0.0

        for metric, weight in self.goal_weights.items():
            if metric in fitness_scores:
                score = fitness_scores[metric]

                # If weight is negative (like for complexity), invert the score
                if weight < 0:
                    score = 1.0 - score
                    weight = abs(weight)

                weighted_fitness += score * weight
                total_weight += abs(weight)

        # Normalize
        if total_weight > 0:
            avg_fitness = weighted_fitness / total_weight
        else:
            avg_fitness = 0.5  # Default

        # Calculate pressure: lower fitness = higher pressure
        # But with diminishing returns below 0.3 fitness
        if avg_fitness < 0.3:
            # High pressure but capped
            pressure = 0.8
        else:
            # Linear scaling: lower fitness = higher pressure
            pressure = 0.8 * (1.0 - avg_fitness)

        # Add adaptability bias: systems that are more adaptable get more evolution
        adaptability = fitness_scores.get("adaptability", 0.5)
        pressure += 0.2 * adaptability  # Adaptable systems evolve more

        # Add random factor to avoid deterministic evolution
        randomness = 0.1 * random.random()
        pressure += randomness

        # Clamp to valid range
        pressure = max(0.0, min(1.0, pressure))

        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on current fitness.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Selected strategy name
        """
        if not fitness_scores:
            # Default to expansion if no scores
            return "expansion"

        # Calculate weighted probability for each strategy
        strategy_weights = {}

        # Strategy 1: Expansion - good when performance is low but efficiency is high
        if fitness_scores["performance"] < 0.6 and fitness_scores["efficiency"] > 0.7:
            strategy_weights["expansion"] = 0.7 * self.strategies["expansion"]["success_rate"]
        else:
            strategy_weights["expansion"] = 0.3 * self.strategies["expansion"]["success_rate"]

        # Strategy 2: Pruning - good when efficiency is low but performance is decent
        if fitness_scores["efficiency"] < 0.5 and fitness_scores["performance"] > 0.6:
            strategy_weights["pruning"] = 0.8 * self.strategies["pruning"]["success_rate"]
        else:
            strategy_weights["pruning"] = 0.3 * self.strategies["pruning"]["success_rate"]

        # Strategy 3: Restructuring - good when adaptability is low
        if fitness_scores["adaptability"] < 0.5:
            strategy_weights["restructuring"] = 0.7 * self.strategies["restructuring"]["success_rate"]
        else:
            strategy_weights["restructuring"] = 0.4 * self.strategies["restructuring"]["success_rate"]

        # Strategy 4: Specialization - good for complex systems with good performance
        if fitness_scores["complexity"] > 0.7 and fitness_scores["performance"] > 0.7:
            strategy_weights["specialization"] = 0.8 * self.strategies["specialization"]["success_rate"]
        else:
            strategy_weights["specialization"] = 0.2 * self.strategies["specialization"]["success_rate"]

        # Strategy 5: Integration - good for improving robustness
        if fitness_scores["robustness"] < 0.6:
            strategy_weights["integration"] = 0.7 * self.strategies["integration"]["success_rate"]
        else:
            strategy_weights["integration"] = 0.3 * self.strategies["integration"]["success_rate"]

        # Add randomness factor to promote exploration
        for strategy in strategy_weights:
            strategy_weights[strategy] += random.uniform(0, 0.3)

        # Select strategy with highest weight
        selected_strategy = max(strategy_weights.items(), key=lambda x: x[1])[0]

        return selected_strategy

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        success = False  # Initialize success flag
        message = "Unknown result"  # Initialize message

        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            elif "_blend" in strategy:
                # Handle blended strategies
                changes.append("Applied blended strategy with emergent properties")
                success = True
                message = f"Blended strategy {strategy} applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"

        return success, message, changes

    def _apply_expansion_strategy(self, agent, changes):
        """
        Apply expansion strategy to increase model capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for expandable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'expand_architecture'):
            return False, "Model does not support architecture expansion", changes

        try:
            # Expansion attempts:
            # 1. Try expanding the model architecture
            agent.model.expand_architecture()
            changes.append("Expanded neural architecture with additional layers")

            # 2. If agent has adaptive learning, adjust its parameters
            if hasattr(agent, 'adaptive_learning'):
                # Increase adaptation rate for new architecture
                adaptive_learning = agent.adaptive_learning
                if hasattr(adaptive_learning, 'adaptation_threshold'):
                    old_threshold = adaptive_learning.adaptation_threshold
                    new_threshold = max(0.05, old_threshold * 0.9)  # More sensitive
                    adaptive_learning.adaptation_threshold = new_threshold
                    changes.append(f"Reduced adaptation threshold from {old_threshold:.2f} to {new_threshold:.2f}")

            # 3. Integrate imagination with learning system
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'imagination') and
                hasattr(agent, 'adaptive_learning')):

                imagination = agent.ai_manager.imagination
                learning = agent.adaptive_learning

                # Link imagination creativity to adaptation rate
                if hasattr(imagination, 'creativity_level') and hasattr(learning, 'adaptation_rate'):
                    creativity = imagination.creativity_level
                    old_rate = learning.adaptation_rate

                    # Higher creativity = higher adaptation rate
                    new_rate = 0.2 + 0.3 * creativity  # Range 0.2-0.5
                    learning.adaptation_rate = new_rate

                    changes.append(f"Integrated imagination creativity with learning adaptation: rate {old_rate:.2f} → {new_rate:.2f}")
                    integration_changes += 1

            # Update strategy success rate based on changes made
            if integration_changes > 0:
                self.strategies["integration"]["success_rate"] = min(0.95, self.strategies["integration"]["success_rate"] * 1.1)
                return True, f"Successfully integrated {integration_changes} component pairs for improved synergy", changes
            else:
                self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)
                return False, "No suitable components found for integration", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)

            return False, f"Component integration failed: {str(e)}", changes

    def _assess_capability_levels(self, agent):
        """
        Assess current capability levels of the system across target areas.

        Parameters:
        - agent: Agent to assess

        Returns:
        - Dictionary of capability scores
        """
        capabilities = {}

        # 1. Knowledge representation capability
        kr_score = 0.5  # Default

        # Check for semantic memory capacity
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
            memory_size = len(agent.free_will.semantic_memory)
            # Scale based on size: 0-1000 items maps to 0.5-0.9 score
            kr_size_factor = min(0.4, memory_size / 2500)
            kr_score += kr_size_factor

        # Check for memory importance tracking
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_importance'):
            kr_score += 0.1

        capabilities["knowledge_representation"] = min(0.95, kr_score)

        # 2. Planning capability
        planning_score = 0.4  # Default

        # Check for temporal planner
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            planning_score += 0.2

            # Check for goal system
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                planning_score += min(0.2, len(planner.long_term_goals) * 0.05)

            # Check for reflection capability
            if hasattr(planner, 'reflect_and_adapt'):
                planning_score += 0.1

        capabilities["planning"] = min(0.95, planning_score)

        # 3. Learning capability
        learning_score = 0.3  # Default

        # Check for adaptive learning system
        if hasattr(agent, 'adaptive_learning'):
            learning_score += 0.3

            # Check for meta-learning capability
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                learning_score += 0.3

        capabilities["learning"] = min(0.95, learning_score)

        # 4. Error handling capability
        error_score = 0.3  # Default

        # Check for imagination-based error detection
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            if hasattr(imagination, 'simulate_error_detection'):
                error_score += 0.3

            if hasattr(imagination, 'simulate_error_correction'):
                error_score += 0.2

        # Check for error recovery in AI manager
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'error_recovery_attempts') is not None:
            error_score += 0.1

        capabilities["error_handling"] = min(0.95, error_score)

        # 5. Creative synthesis capability
        creative_score = 0.2  # Default

        # Check for imagination engine
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            creative_score += 0.3

            # Check for creativity level
            if hasattr(imagination, 'creativity_level'):
                creative_score += imagination.creativity_level * 0.3

        # Check for consciousness system
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for qualia simulation
            if hasattr(consciousness, 'qualia_simulation_active'):
                creative_score += 0.1

        capabilities["creative_synthesis"] = min(0.95, creative_score)

        # Store capability assessment
        self.capability_scores = capabilities

        return capabilities

    def get_evolution_report(self):
        """
        Generate a comprehensive report on system evolution status.

        Returns:
        - Dictionary with evolution status information
        """
        if not self.evolution_history:
            return {
                "status": "initialized",
                "generation": 0,
                "message": "Evolution engine initialized but no evolution cycles completed"
            }

        # Calculate success rate
        total_attempts = len(self.evolution_history)
        successful_attempts = sum(1 for record in self.evolution_history if record.get("success", False))
        success_rate = successful_attempts / total_attempts if total_attempts > 0 else 0

        # Get strategy effectiveness
        strategy_stats = {}
        for strategy, data in self.strategies.items():
            success_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy and record.get("success", False))

            attempt_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy)

            strategy_stats[strategy] = {
                "attempts": attempt_count,
                "successes": success_count,
                "success_rate": success_count / attempt_count if attempt_count > 0 else 0,
                "current_rating": data["success_rate"]
            }

        # Get recent evolutions
        recent_evolutions = []
        for record in self.evolution_history[-5:]:  # Last 5 evolutions
            recent_evolutions.append({
                "generation": record.get("generation", 0),
                "strategy": record.get("strategy", "unknown"),
                "success": record.get("success", False),
                "message": record.get("message", ""),
                "changes": record.get("changes", [])
            })

        # Calculate evolutionary convergence
        if len(self.fitness_metrics) >= 2:
            # Compare current fitness with previous generation
            current_gen = max(self.fitness_metrics.keys())
            prev_gen = max(k for k in self.fitness_metrics.keys() if k < current_gen)

            current_fitness = self.fitness_metrics[current_gen]
            prev_fitness = self.fitness_metrics[prev_gen]

            # Calculate average improvement across metrics
            improvements = []
            for metric in current_fitness:
                if metric in prev_fitness:
                    # For complexity, lower is better; for others, higher is better
                    if metric == "complexity":
                        change = prev_fitness[metric] - current_fitness[metric]
                    else:
                        change = current_fitness[metric] - prev_fitness[metric]
                    improvements.append(change)

            avg_improvement = sum(improvements) / len(improvements) if improvements else 0
            # Convergence increases as improvement diminishes
            self.convergence_score = 1.0 - min(1.0, abs(avg_improvement) * 10)

        # Generate recommendations for next evolution
        recommendations = []

        # Recommendation 1: Address capability gaps
        capability_gaps = []
        if self.capability_scores:
            for capability, target in self.capability_targets.items():
                current = self.capability_scores.get(capability, 0)
                if current < target and target - current > 0.2:
                    capability_gaps.append((capability, target - current))

            if capability_gaps:
                # Recommend addressing the largest gap
                largest_gap = max(capability_gaps, key=lambda x: x[1])
                recommendations.append({
                    "type": "capability_development",
                    "target": largest_gap[0],
                    "gap": largest_gap[1],
                    "recommendation": f"Prioritize evolution of {largest_gap[0].replace('_', ' ')} capability"
                })

        # Recommendation 2: Strategy adjustment based on success rates
        worst_strategy = min(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]
        best_strategy = max(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]

        if strategy_stats[worst_strategy]["success_rate"] < 0.3 and strategy_stats[worst_strategy]["attempts"] >= 3:
            recommendations.append({
                "type": "strategy_adjustment",
                "strategy": worst_strategy,
                "recommendation": f"Reduce use of {worst_strategy} strategy due to low success rate ({strategy_stats[worst_strategy]['success_rate']:.2f})"
            })

        # Recommendation 3: Based on convergence
        if self.convergence_score > 0.8:
            recommendations.append({
                "type": "convergence_response",
                "convergence_score": self.convergence_score,
                "recommendation": "System appears to be converging - increase mutation strength to explore new optima"
            })

        report = {
            "status": "evolving",
            "generation": self.evolution_generation,
            "attempts": total_attempts,
            "success_rate": success_rate,
            "last_major_evolution": self.last_major_evolution,
            "convergence_score": self.convergence_score,
            "strategy_stats": strategy_stats,
            "recent_evolutions": recent_evolutions,
            "capability_scores": self.capability_scores,
            "recommendations": recommendations
        }

        # Example of the corrected if statement block placed AFTER the report dictionary:
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'): # Correct indentation
            meta_learning = agent.ai_manager.meta_learning
            if hasattr(meta_learning, 'meta_params'): # Indented once more
                # Increase exploration after expansion # Indented twice more
                old_rate = meta_learning.meta_params.get("exploration_rate", 0.3) # Indented twice more
                new_rate = min(0.5, old_rate * 1.2)  # More exploration # Indented twice more
                meta_learning.meta_params["exploration_rate"] = new_rate # Indented twice more
                changes.append(f"Increased meta-learning exploration rate from {old_rate:.2f} to {new_rate:.2f}") # Indented twice more
        # Update strategy success rate # Indented once more
        self.strategies["expansion"]["success_rate"] = min(0.95, self.strategies["expansion"]["success_rate"] * 1.1) # Indented once more

        return report # The return statement is at the base level of the method and *outside* the if block now

    def _apply_pruning_strategy(self, agent, changes):
        """
        Apply pruning strategy to reduce model size and increase efficiency.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for prunable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'contract_architecture'):
            return False, "Model does not support architecture pruning", changes

        try:
            # Pruning attempts:
            # 1. Try contracting the model architecture
            agent.model.contract_architecture()
            changes.append("Contracted neural architecture by removing underutilized layers")

            # 2. If agent has memory systems, prune those too
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'contract_memory'):
                old_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0

                # Calculate target size (80% of current)
                target_size = int(old_size * 0.8)
                if target_size > 0:
                    agent.free_will.contract_memory(target_size)
                    new_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0
                    changes.append(f"Pruned memory from {old_size} to {new_size} items")

            # 3. If agent has semantic memory, optimize it
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
                if agent.free_will.semantic_memory:
                    old_count = len(agent.free_will.semantic_memory)
                    # Remove low importance memories
                    low_importance = []
                    threshold = 0.4  # Below this importance, consider pruning

                    for url, data in agent.free_will.semantic_memory.items():
                        if isinstance(data, dict) and "importance" in data:
                            if data["importance"] < threshold:
                                low_importance.append(url)

                    # Remove a portion of low importance items
                    prune_count = min(len(low_importance), int(old_count * 0.2))  # Prune up to 20%

                    for url in low_importance[:prune_count]:
                        if url in agent.free_will.semantic_memory:
                            del agent.free_will.semantic_memory[url]

                    new_count = len(agent.free_will.semantic_memory)
                    if new_count < old_count:
                        changes.append(f"Pruned semantic memory from {old_count} to {new_count} items")

            # Update strategy success rate
            self.strategies["pruning"]["success_rate"] = min(0.95, self.strategies["pruning"]["success_rate"] * 1.1)

            return True, "Successfully pruned system components to improve efficiency", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["pruning"]["success_rate"] = max(0.2, self.strategies["pruning"]["success_rate"] * 0.9)

            return False, f"Architecture pruning failed: {str(e)}", changes

    def _apply_restructuring_strategy(self, agent, changes):
        """
        Apply restructuring strategy to reorganize components without changing capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Restructuring attempts:
            # 1. Modify consciousness parameters if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness

                # Adjust awareness fluctuation rate
                if hasattr(consciousness, 'awareness_fluctuation_rate'):
                    old_rate = consciousness.awareness_fluctuation_rate
                    new_rate = old_rate * random.uniform(0.8, 1.2)  # Random adjustment
                    consciousness.awareness_fluctuation_rate = max(0.01, min(0.1, new_rate))
                    changes.append(f"Adjusted consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")

                # Reset state to "reflective" to encourage reassessment
                if hasattr(consciousness, 'current_state'):
                    old_state = consciousness.current_state
                    consciousness.current_state = "reflective"
                    changes.append(f"Reset consciousness state from '{old_state}' to 'reflective'")

                # Boost awareness temporarily
                if hasattr(consciousness, 'increase_awareness'):
                    consciousness.increase_awareness(0.2)
                    changes.append("Temporarily boosted consciousness awareness for restructuring")

            # 2. Modify temporal planner goals if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
                planner = agent.ai_manager.temporal_planner

                # Rebalance goal priorities
                if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                    # Shuffle priorities
                    old_priorities = {}
                    for goal in planner.long_term_goals:
                        old_priorities[goal.get("id", "unknown")] = goal.get("priority", 0.5)

                    # Create new distribution
                    total_priority = sum(goal.get("priority", 0.5) for goal in planner.long_term_goals)
                    avg_priority = total_priority / len(planner.long_term_goals)

                    # Invert relative to average
                    for goal in planner.long_term_goals:
                        goal_id = goal.get("id", "unknown")
                        old_priority = goal.get("priority", 0.5)

                        # Calculate new priority as reflection around average
                        new_priority = avg_priority + (avg_priority - old_priority)
                        # Ensure it's in valid range
                        new_priority = max(0.1, min(1.0, new_priority))

                        # Apply new priority
                        goal["priority"] = new_priority

                    changes.append("Rebalanced long-term goal priorities to shift focus")

                # Refresh short-term goals immediately
                if hasattr(planner, 'refresh_short_term_goals'):
                    planner.refresh_short_term_goals()
                    changes.append("Regenerated short-term goals based on restructured priorities")

            # 3. Modify imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Adjust creativity level
                if hasattr(imagination, 'creativity_level'):
                    old_level = imagination.creativity_level
                    new_level = 1.0 - old_level  # Invert creativity level
                    imagination.creativity_level = new_level
                    changes.append(f"Inverted imagination creativity level: {old_level:.2f} → {new_level:.2f}")

                # Change current mode
                if hasattr(imagination, 'current_mode') and hasattr(imagination, 'cognitive_modes'):
                    old_mode = imagination.current_mode
                    # Select a different mode
                    available_modes = [m for m in imagination.cognitive_modes if m != old_mode]
                    if available_modes:
                        new_mode = random.choice(available_modes)
                        imagination.current_mode = new_mode
                        changes.append(f"Switched imagination cognitive mode: '{old_mode}' → '{new_mode}'")

            # Update strategy success rate
            if changes:
                self.strategies["restructuring"]["success_rate"] = min(0.95, self.strategies["restructuring"]["success_rate"] * 1.1)
                return True, "Successfully restructured internal cognitive components", changes
            else:
                self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)
                return False, "No suitable components found for restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)

            return False, f"Component restructuring failed: {str(e)}", changes

    def _apply_specialization_strategy(self, agent, changes):

        try:
            # Identify specialized modules to optimize
            specialized_changes = 0

            # 1. Specialize content sifter if present
            if hasattr(agent, 'content_sifter'):
                sifter = agent.content_sifter

                # Customize topic focus
                if hasattr(sifter, 'topics_of_interest'):
                    # Pick top 3 priorities from temporal planner if available
                    priority_topics = []

                    if (hasattr(agent, 'ai_manager') and
                        hasattr(agent.ai_manager, 'temporal_planner') and
                        hasattr(agent.ai_manager.temporal_planner, 'long_term_goals')):

                        # Extract keywords from highest priority goals
                        goals = sorted(agent.ai_manager.temporal_planner.long_term_goals,
                                     key=lambda g: g.get("priority", 0), reverse=True)

                        for goal in goals[:3]:
                            desc = goal.get("description", "").lower()
                            words = desc.split()
                            important_words = [w for w in words if len(w) > 4]
                            priority_topics.extend(important_words[:2])  # 2 keywords per goal

                    # Add some general technology topics
                    tech_topics = ["artificial intelligence", "quantum computing",
                                  "neural networks", "machine learning"]

                    # Combine maintaining 30% of original topics for diversity
                    old_topics = sifter.topics_of_interest
                    keep_count = max(3, int(len(old_topics) * 0.3))
                    kept_topics = random.sample(old_topics, keep_count)

                    # Create new specialized topic list
                    new_topics = kept_topics + priority_topics + tech_topics
                    # Remove duplicates
                    new_topics = list(dict.fromkeys(new_topics))

                    # Apply change
                    sifter.topics_of_interest = new_topics
                    specialized_changes += 1
                    changes.append(f"Specialized content sifter with {len(new_topics)} focused topics")

            # 2. Specialize free will parameters if present
            if hasattr(agent, 'free_will'):
                free_will = agent.free_will

                # Adjust exploration/exploitation balance
                if hasattr(free_will, 'exploration_weight') and hasattr(free_will, 'exploitation_weight'):
                    # Identify system state - are we specialized enough already?
                    if hasattr(agent, 'stats') and 'domains_visited' in agent.stats:
                        domains_count = len(agent.stats['domains_visited'])

                        # If we've visited many domains, increase exploitation
                        if domains_count > 20:
                            old_expl = free_will.exploration_weight
                            old_expt = free_will.exploitation_weight

                            # Shift toward exploitation
                            free_will.exploitation_weight = min(0.8, old_expt + 0.1)
                            free_will.exploration_weight = 1.0 - free_will.exploitation_weight

                            changes.append(f"Specialized free will toward exploitation: {old_expl:.2f}/{old_expt:.2f} → {free_will.exploration_weight:.2f}/{free_will.exploitation_weight:.2f}")
                            specialized_changes += 1

            # 3. Specialize imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Specialize domain expertise
                if hasattr(imagination, 'domains'):
                    domains = imagination.domains

                    # Identify top 2 domains based on current expertise
                    top_domains = sorted(domains.items(), key=lambda x: x[1], reverse=True)[:2]

                    # Boost top domains further
                    for domain, expertise in top_domains:
                        old_expertise = expertise
                        new_expertise = min(0.95, old_expertise + 0.1)
                        imagination.domains[domain] = new_expertise
                        changes.append(f"Specialized imagination expertise in {domain}: {old_expertise:.2f} → {new_expertise:.2f}")

                    specialized_changes += 1

            # Update strategy success rate based on changes made
            if specialized_changes > 0:
                self.strategies["specialization"]["success_rate"] = min(0.95, self.strategies["specialization"]["success_rate"] * 1.1)
                return True, f"Successfully specialized {specialized_changes} components for improved focus", changes
            else:
                self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)
                return False, "No suitable components found for specialization", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)

            return False, f"Component specialization failed: {str(e)}", changes
    def _apply_integration_strategy(self, agent, changes):
        """
        Apply integration strategy: combine previously separate capabilities.
        """
        try:
            integration_changes = 0

            # 1. Integrate consciousness with planning if possible
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'consciousness') and
                hasattr(agent.ai_manager, 'temporal_planner')):

                consciousness = agent.ai_manager.consciousness
                planner = agent.ai_manager.temporal_planner

                if hasattr(consciousness, 'awareness_level') and hasattr(planner, 'reflection_interval'):
                    awareness = consciousness.awareness_level
                    old_interval = planner.reflection_interval
                    new_interval = int(max(5, 30 - 25 * awareness))
                    planner.reflection_interval = new_interval

                    changes.append(f"Integrated consciousness awareness with planning reflection: interval {old_interval} → {new_interval}")
                    integration_changes += 1

            # 2. Integrate free will with content filtering
            if hasattr(agent, 'free_will') and hasattr(agent, 'content_sifter'):
                free_will = agent.free_will
                sifter = agent.content_sifter

                if hasattr(free_will, 'memory_importance') and hasattr(sifter, 'topics_of_interest'):
                    important_urls = []
                    for url, importance in free_will.memory_importance.items():
                        if importance > 0.7:
                            important_urls.append(url)

                    if important_urls and hasattr(free_will, 'semantic_memory'):
                        new_topics = []
                        for url in important_urls[:5]:
                            if url in free_will.semantic_memory:
                                memory = free_will.semantic_memory[url]
                                keywords = memory.get("keywords", [])
                                if keywords:
                                    new_topics.extend(keywords[:3])
                        if new_topics:
                            old_topics = set(sifter.topics_of_interest)
                            combined_topics = list(old_topics.union(set(new_topics)))
                            sifter.topics_of_interest = combined_topics

                            changes.append(f"Integrated free will memory importance with content filtering: added {len(new_topics)} new topics of interest")
                            integration_changes += 1
        except Exception as e:
            return False, f"Integration strategy failed: {str(e)}", changes

        if integration_changes > 0:
            return True, f"Integration strategy applied with {integration_changes} changes", changes
        else:
            return False, "No integration changes applied", changes

    # The following methods below remain unchanged from your original implementation.
    def _generate_reflective_thought(self, context):
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def evolve_system(self, agent):
        """
        Trigger system evolution based on performance metrics, goals,
        and environmental requirements.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Basic validation
        if not agent:
            return False, "Invalid agent provided to evolution engine."

        # Determine current cycle count
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check (don't evolve too frequently)
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = cycle_count - self.last_major_evolution

        if cycles_since_last < evolution_interval:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval} cycles since last evolution."

        # Increase generation counter
        self.evolution_generation += 1

        # Analyze current state and fitness
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        evolution_threshold = 0.3  # Base threshold
        evolution_probability = evolution_threshold + evolutionary_pressure

        if random.random() >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy
        selected_strategy = self._select_evolution_strategy(fitness_scores)

        # Apply the selected evolutionary strategy
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }
        self.evolution_history.append(evolution_record)

        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.
        (Placeholder implementation.)
        """
        fitness_scores = {
            "performance": 0.5,
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }
        # Example: if agent.stats exists, use simple ratio as performance
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = agent.stats.get("cycles_run", 0)
            pages = agent.stats.get("pages_processed", 0)
            if cycles > 0:
                performance = pages / cycles
                fitness_scores["performance"] = max(0.1, min(0.9, performance / 10))
        return fitness_scores

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights.
        Lower overall fitness implies higher pressure.
        """
        total_weight = sum(abs(w) for w in self.goal_weights.values())
        weighted_sum = 0.0
        for metric, weight in self.goal_weights.items():
            score = fitness_scores.get(metric, 0.5)
            if weight < 0:
                score = 1 - score
            weighted_sum += score * abs(weight)
        avg_fitness = weighted_sum / total_weight if total_weight != 0 else 0.5
        pressure = (1 - avg_fitness) * self.adaptation_rate
        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on the current fitness scores.
        Here we choose the strategy corresponding to the capability with the largest gap.
        """
        gaps = {}
        for metric, target in self.capability_targets.items():
            current = fitness_scores.get(metric, 0.5)
            gaps[metric] = target - current
        most_lacking = max(gaps.items(), key=lambda x: x[1])[0]
        mapping = {
            "knowledge_representation": "expansion",
            "planning": "integration",
            "learning": "specialization",
            "error_handling": "pruning",
            "creative_synthesis": "restructuring"
        }
        return mapping.get(most_lacking, "expansion")

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"
        return success, message, changes

    def _generate_reflective_thought(self, context):
        """Generate a reflective thought focused on self-examination"""
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def _generate_exploratory_thought(self, context):
        """Generate an exploratory thought focused on new possibilities"""
        thought = {
            "type": "exploratory",
            "content": "Exploration of new possibilities",
            "directions": [],
            "insights": [],
            "importance": 0.5
        }
        exploration_areas = [
            "unknown_domains",
            "connection_patterns",
            "alternative_strategies",
            "capability_expansion"
        ]
        selected_areas = random.sample(exploration_areas, min(2, len(exploration_areas)))
        directions = []
        for area in selected_areas:
            if area == "unknown_domains":
                if "domains_visited" in context:
                    domains_visited = context.get("domains_visited", set())
                    if isinstance(domains_visited, set):
                        candidate_domains = [
                            "research.science", "github.com", "en.wikipedia.org",
                            "arxiv.org", "semanticscholar.org", "openai.com",
                            "nature.com", "reddit.com/r/MachineLearning"
                        ]
                        unvisited = [d for d in candidate_domains if d not in domains_visited]
                        if unvisited:
                            sample_domains = random.sample(unvisited, min(3, len(unvisited)))
                            directions.append({
                                "area": "unknown_domains",
                                "content": f"Explore high-value domains: {', '.join(sample_domains)}",
                                "rationale": "Expanding domain knowledge diversity"
                            })
            elif area == "connection_patterns":
                directions.append({
                    "area": "connection_patterns",
                    "content": "Implement knowledge graph traversal to find distant connections",
                    "rationale": "Distant domains often contain valuable cross-applicable patterns"
                })
            elif area == "alternative_strategies":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = list(self.agent.planner_sifter.strategies.keys())
                    if strategies:
                        if (hasattr(self.agent.planner_sifter, 'strategy_usage')):
                            usage = self.agent.planner_sifter.strategy_usage
                            least_used = sorted([(s, usage.get(s, 0)) for s in strategies], key=lambda x: x[1])
                            if least_used:
                                least_used_strategy = least_used[0][0]
                                directions.append({
                                    "area": "alternative_strategies",
                                    "content": f"Experiment with underutilized strategy: {least_used_strategy}",
                                    "rationale": "Diversifying strategic approaches to discover new optima"
                                })
            elif area == "capability_expansion":
                directions.append({
                    "area": "capability_expansion",
                    "content": "Develop enhanced semantic reasoning module",
                    "rationale": "Would significantly improve knowledge integration capabilities"
                })
        insights = []
        for direction in directions:
            area = direction.get("area", "")
            rationale = direction.get("rationale", "")
            if area == "unknown_domains":
                insights.append("Systematic exploration of high-value domains should be prioritized")
            elif area == "connection_patterns":
                insights.append("Knowledge value may lie in unexpected cross-domain connections")
            elif area == "alternative_strategies":
                insights.append("Strategic diversification may uncover more effective approaches")
            elif area == "capability_expansion":
                insights.append("Capability development should focus on knowledge integration")
        thought["directions"] = directions
        thought["insights"] = insights
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)
        if insights:
            thought["content"] = insights[0]
        elif directions:
            thought["content"] = directions[0].get("content", "Exploration suggestion")
        return thought

    def _generate_critical_thought(self, context):
        """Generate a critical thought focused on evaluation and assessment"""
        thought = {
            "type": "critical",
            "content": "Critical evaluation",
            "assessments": [],
            "insights": [],
            "importance": 0.5
        }
        assessment_areas = [
            "information_quality",
            "reasoning_validity",
            "strategy_efficiency",
            "resource_allocation",
            "error_handling"
        ]
        selected_areas = random.sample(assessment_areas, min(2, len(assessment_areas)))
        assessments = []
        for area in selected_areas:
            if area == "information_quality":
                if (hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'domain_intelligence')):
                    intelligence = self.agent.free_will.domain_intelligence
                    if hasattr(intelligence, 'domain_knowledge'):
                        knowledge = intelligence.domain_knowledge
                        if knowledge:
                            low_quality_domains = []
                            for domain, data in knowledge.items():
                                if isinstance(data, dict) and data.get("content_quality", 1.0) < 0.5:
                                    low_quality_domains.append(domain)
                            if low_quality_domains:
                                sample_domains = random.sample(low_quality_domains, min(2, len(low_quality_domains)))
                                assessments.append({
                                    "area": "information_quality",
                                    "content": f"Low-quality content detected in domains: {', '.join(sample_domains)}",
                                    "recommendation": "Implement stricter quality thresholds for these domains"
                                })
                            else:
                                assessments.append({
                                    "area": "information_quality",
                                    "content": "Overall information quality appears acceptable",
                                    "recommendation": "Continue regular quality monitoring"
                                })
            elif area == "reasoning_validity":
                if self.thought_history:
                    recent_thoughts = self.thought_history[-10:] if len(self.thought_history) >= 10 else self.thought_history
                    thought_types = set(t.get("type", "") for t in recent_thoughts)
                    if len(thought_types) < 3:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Limited cognitive diversity detected (only {len(thought_types)} thought types used recently)",
                            "recommendation": "Deliberately activate more diverse thinking modes"
                        })
                    else:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Cognitive diversity appears healthy ({len(thought_types)} different thought types used recently)",
                            "recommendation": "Maintain diverse thinking patterns"
                        })
            elif area == "strategy_efficiency":
                if (hasattr(self.agent, 'stats') and
                    isinstance(self.agent.stats, dict) and
                    'cycles_run' in self.agent.stats and
                    'pages_processed' in self.agent.stats):
                    cycles = self.agent.stats['cycles_run']
                    pages = self.agent.stats['pages_processed']
                    if cycles > 10:
                        efficiency = pages / max(1, cycles)
                        if efficiency < 0.5:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Low efficiency detected: {efficiency:.2f} pages/cycle",
                                "recommendation": "Consider more aggressive pruning of low-value paths"
                            })
                        else:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Acceptable efficiency: {efficiency:.2f} pages/cycle",
                                "recommendation": "Continue current approach with regular efficiency monitoring"
                            })
            elif area == "resource_allocation":
                if (hasattr(self.agent, 'memory') and
                    hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'memory_set')):
                    memory_usage = len(self.agent.memory) / MEMORY_MAX_SIZE
                    urls_stored = len(self.agent.free_will.memory_set)
                    if memory_usage > 0.8 and urls_stored > 1000:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"High memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Implement more aggressive memory pruning strategy"
                        })
                    else:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"Memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Current resource allocation appears appropriate"
                        })
            elif area == "error_handling":
                if (hasattr(self.agent, 'domain_stats') and isinstance(self.agent.domain_stats, dict)):
                    total_visits = sum(d.get("visits", 0) for d in self.agent.domain_stats.values())
                    total_errors = sum(d.get("error_count", 0) for d in self.agent.domain_stats.values())
                    if total_visits > 0:
                        error_rate = total_errors / total_visits
                        if error_rate > 0.2:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"High system-wide error rate: {error_rate:.2%}",
                                "recommendation": "Investigate error causes and implement more robust handling"
                            })
                        else:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"Acceptable error rate: {error_rate:.2%}",
                                "recommendation": "Continue monitoring error patterns"
                            })
        insights = []
        for assessment in assessments:
            recommendation = assessment.get("recommendation", "")
            if "low-quality" in assessment.get("content", "").lower():
                insights.append("Information quality control needs improvement")
            elif "limited cognitive diversity" in assessment.get("content", "").lower():
                insights.append("Need to activate more diverse thinking patterns")
            elif "low efficiency" in assessment.get("content", "").lower():
                insights.append("Strategy efficiency requires optimization")
            elif "high memory usage" in assessment.get("content", "").lower():
                insights.append("Memory management needs more efficient pruning")
            elif "high system-wide error" in assessment.get("content", "").lower():
                insights.append("Error handling systems require review and enhancement")
            elif recommendation:
                insights.append(recommendation)
        thought["assessments"] = assessments
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if any("high" in a.get("content", "").lower() for a in assessments):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif assessments:
            thought["content"] = assessments[0].get("content", "Critical assessment")
        return thought

    def _generate_integrative_thought(self, context):
        """Generate an integrative thought focused on knowledge synthesis"""
        thought = {
            "type": "integrative",
            "content": "Knowledge synthesis",
            "connections": [],
            "insights": [],
            "importance": 0.5
        }
        connections = []
        if not context:
            thought["content"] = "Insufficient knowledge for meaningful integration"
            return thought
        elements = {}
        if "domains_visited" in context:
            domains = context.get("domains_visited", set())
            if isinstance(domains, set) and len(domains) > 3:
                elements["domains"] = list(domains)
        if hasattr(self.agent, 'content_sifter'):
            sifter = self.agent.content_sifter
            if hasattr(sifter, 'topics_of_interest'):
                elements["topics"] = sifter.topics_of_interest
        if (hasattr(self.agent, 'ai_manager') and
            hasattr(self.agent.ai_manager, 'temporal_planner') and
            hasattr(self.agent.ai_manager.temporal_planner, 'long_term_goals')):
            goals = self.agent.ai_manager.temporal_planner.long_term_goals
            if goals:
                elements["goals"] = [g.get("description", "") for g in goals]
        if len(elements) >= 2:
            element_types = list(elements.keys())
            type_pair = random.sample(element_types, 2)
            elements_1 = elements[type_pair[0]]
            elements_2 = elements[type_pair[1]]
            if elements_1 and elements_2:
                element_1 = random.choice(elements_1)
                element_2 = random.choice(elements_2)
                connections.append({
                    "elements": [
                        {"type": type_pair[0], "value": element_1},
                        {"type": type_pair[1], "value": element_2}
                    ],
                    "connection_type": "integration",
                    "description": f"Integration of {type_pair[0]} '{element_1}' with {type_pair[1]} '{element_2}'",
                    "potential": random.uniform(0.5, 0.9)
                })
        if not connections:
            theoretical_connections = [
                {
                    "elements": [
                        {"type": "cognitive_mode", "value": "analytical"},
                        {"type": "capability", "value": "semantic_representation"}
                    ],
                    "connection_type": "enhancement",
                    "description": "Analytical thinking could enhance semantic representation quality",
                    "potential": 0.7
                },
                {
                    "elements": [
                        {"type": "capability", "value": "memory_management"},
                        {"type": "capability", "value": "planning"}
                    ],
                    "connection_type": "synergy",
                    "description": "Memory systems could be more tightly integrated with planning",
                    "potential": 0.8
                }
            ]
            connections.append(random.choice(theoretical_connections))
        insights = []
        for connection in connections:
            desc = connection.get("description", "")
            conn_type = connection.get("connection_type", "")
            potential = connection.get("potential", 0.5)
            if conn_type == "integration" and potential > 0.7:
                insights.append(f"High potential integration opportunity: {desc}")
            elif conn_type == "enhancement":
                insights.append(f"Enhancement pathway identified: {desc}")
            elif conn_type == "synergy":
                insights.append(f"Synergistic relationship would increase capability: {desc}")
            else:
                insights.append(f"Connection opportunity: {desc}")
        thought["connections"] = connections
        thought["insights"] = insights
        importance = 0.5
        if connections:
            avg_potential = sum(c.get("potential", 0.5) for c in connections) / len(connections)
            importance = avg_potential
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif connections:
            thought["content"] = connections[0].get("description", "Knowledge integration")
        return thought

    def _generate_intuitive_thought(self, context):
        """Generate an intuitive thought focused on pattern recognition"""
        thought = {
            "type": "intuitive",
            "content": "Pattern recognition",
            "patterns": [],
            "insights": [],
            "importance": 0.5
        }
        patterns = []
        if not context:
            thought["content"] = "Insufficient data for pattern recognition"
            return thought
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if len(actions) >= 3:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                if len(action_types) >= 3:
                    repetitions = []
                    current_sequence = [action_types[0]]
                    for i in range(1, len(action_types)):
                        if action_types[i] == action_types[i-1]:
                            current_sequence.append(action_types[i])
                        else:
                            if len(current_sequence) >= 2:
                                repetitions.append(current_sequence)
                            current_sequence = [action_types[i]]
                    if len(current_sequence) >= 2:
                        repetitions.append(current_sequence)
                    if repetitions:
                        longest_repetition = max(repetitions, key=len)
                        patterns.append({
                            "type": "action_repetition",
                            "description": f"Repeated sequence of '{longest_repetition[0]}' actions",
                            "significance": 0.6 + 0.1 * min(4, len(longest_repetition)),
                            "potential_cause": "Strategy fixation or optimal local strategy"
                        })
                    if len(set(action_types)) == 2 and len(action_types) >= 4:
                        is_alternating = True
                        for i in range(2, len(action_types)):
                            if action_types[i] != action_types[i-2]:
                                is_alternating = False
                                break
                        if is_alternating:
                            patterns.append({
                                "type": "action_alternation",
                                "description": f"Alternating pattern between '{action_types[0]}' and '{action_types[1]}'",
                                "significance": 0.7,
                                "potential_cause": "Explore-exploit cycle or complementary strategies"
                            })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                frequent_domains = sorted(
                    [(d, stats.get("visits", 0)) for d, stats in domains.items()],
                    key=lambda x: x[1],
                    reverse=True
                )
                if frequent_domains:
                    top_domains = frequent_domains[:3]
                    total_visits = sum(v for _, v in frequent_domains)
                    top_domain_visits = sum(v for _, v in top_domains)
                    concentration = top_domain_visits / max(1, total_visits)
                    if concentration > 0.7:
                        patterns.append({
                            "type": "domain_concentration",
                            "description": f"Heavy concentration ({concentration:.0%}) on top 3 domains",
                            "significance": 0.7,
                            "potential_cause": "Exploitation focus or domain specialization"
                        })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                error_domains = []
                for domain, stats in domains.items():
                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                        error_domains.append((domain, stats.get("error_rate", 0)))
                if error_domains:
                    error_domains.sort(key=lambda x: x[1], reverse=True)
                    top_error_domains = error_domains[:3]
                    patterns.append({
                        "type": "error_concentration",
                        "description": f"High error rates in specific domains: {', '.join(d for d, _ in top_error_domains)}",
                        "significance": 0.8,
                        "potential_cause": "Domain-specific access issues or content filtering problems"
                    })
        insights = []
        for pattern in patterns:
            pattern_type = pattern.get("type", "")
            significance = pattern.get("significance", 0.5)
            if pattern_type == "action_repetition" and significance > 0.7:
                insights.append("Repeated action pattern may indicate strategy fixation - consider forcing exploration")
            elif pattern_type == "action_alternation":
                insights.append("Alternating action pattern suggests systematic exploration-exploitation approach")
            elif pattern_type == "domain_concentration":
                insights.append("High domain concentration indicates need for broader exploration")
            elif pattern_type == "error_concentration":
                insights.append("Domain-specific error pattern detected - consider domain-specific handling strategies")
        thought["patterns"] = patterns
        thought["insights"] = insights
        importance = 0.5
        if patterns:
            avg_significance = sum(p.get("significance", 0.5) for p in patterns) / len(patterns)
            importance = avg_significance
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif patterns:
            thought["content"] = patterns[0].get("description", "Pattern detected")
        return thought

    def _generate_balanced_thought(self, context):
        """Generate a balanced thought that incorporates multiple thinking modes"""
        available_modes = ["analytical", "creative", "reflective", "exploratory", "critical", "integrative", "intuitive"]
        selected_modes = random.sample(available_modes, 2)
        thoughts = []
        for mode in selected_modes:
            if mode == "analytical":
                thoughts.append(self._generate_analytical_thought(context))
            elif mode == "creative":
                thoughts.append(self._generate_creative_thought(context))
            elif mode == "reflective":
                thoughts.append(self._generate_reflective_thought(context))
            elif mode == "exploratory":
                thoughts.append(self._generate_exploratory_thought(context))
            elif mode == "critical":
                thoughts.append(self._generate_critical_thought(context))
            elif mode == "integrative":
                thoughts.append(self._generate_integrative_thought(context))
            elif mode == "intuitive":
                thoughts.append(self._generate_intuitive_thought(context))
        balanced_thought = {
            "type": "balanced",
            "content": "Multi-perspective assessment",
            "component_modes": selected_modes,
            "insights": [],
            "importance": 0.5
        }
        all_insights = []
        for thought in thoughts:
            if "insights" in thought and thought["insights"]:
                all_insights.extend(thought["insights"])
        selected_insights = all_insights[:3] if all_insights else []
        balanced_thought["insights"] = selected_insights
        if thoughts:
            avg_importance = sum(t.get("importance", 0.5) for t in thoughts) / len(thoughts)
            balanced_thought["importance"] = avg_importance
        if selected_insights:
            balanced_thought["content"] = selected_insights[0]
        elif thoughts:
            balanced_thought["content"] = "Balanced perspective: " + thoughts[0].get("content", "")
        return balanced_thought

    def _update_working_memory(self, thought):
        """
        Update working memory with new thought, managing capacity constraints.
        """
        if not thought:
            return
        self.working_memory.append(thought)
        if len(self.working_memory) > self.working_memory_capacity:
            if len(self.working_memory) > 1:
                least_important_idx = min(range(len(self.working_memory) - 1),
                                        key=lambda i: self.working_memory[i].get("importance", 0))
                del self.working_memory[least_important_idx]
        for insight in thought.get("insights", []):
            importance = thought.get("importance", 0.5)
            if importance > 0.7:
                self.recent_insights.append({
                    "content": insight,
                    "source_type": thought.get("type", "unknown"),
                    "importance": importance,
                    "timestamp": datetime.now().isoformat()
                })
        if len(self.recent_insights) > 20:
            self.recent_insights = self.recent_insights[-20:]

    def _summarize_context(self, context):
        """Create a brief summary of the context for thought recording"""
        if not context:
            return "No context"
        if not isinstance(context, dict):
            return str(context)[:100]
        elements = []
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "unknown goal")
            elements.append(f"Goal: {goal_desc}")
        if "last_action" in context and isinstance(context["last_action"], dict):
            action = context["last_action"].get("action", "unknown action")
            elements.append(f"Action: {action}")
        if "memory_size" in context:
            elements.append(f"Memory: {context['memory_size']}")
        return "; ".join(elements) if elements else "Context present but no key elements"

    def _extract_elements_from_context(self, context, aspect_types):
        """
        Extract relevant elements from context based on aspect types.
        """
        elements = []
        if not context or not isinstance(context, dict):
            return elements
        for aspect_type in aspect_types:
            if aspect_type == "goals" and "current_goal" in context:
                if isinstance(context["current_goal"], dict):
                    goal_desc = context["current_goal"].get("description", "")
                    if goal_desc:
                        elements.append(goal_desc)
            elif aspect_type == "domains" and "domains_visited" in context:
                domains = context["domains_visited"]
                if isinstance(domains, set) and domains:
                    sample_size = min(3, len(domains))
                    domain_sample = random.sample(list(domains), sample_size)
                    elements.extend(domain_sample)
            elif aspect_type == "strategies" and hasattr(self.agent, 'planner_sifter'):
                if hasattr(self.agent.planner_sifter, 'strategies'):
                    strategy_names = list(self.agent.planner_sifter.strategies.keys())
                    if strategy_names:
                        sample_size = min(2, len(strategy_names))
                        strategy_sample = random.sample(strategy_names, sample_size)
                        elements.extend(strategy_sample)
            elif aspect_type == "patterns" and "recent_actions" in context:
                actions = context["recent_actions"]
                if isinstance(actions, list) and len(actions) >= 3:
                    action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                    if len(set(action_types)) <= 3:
                        pattern_desc = f"Action pattern: {' → '.join(action_types[-3:])}"
                        elements.append(pattern_desc)
            elif aspect_type == "anomalies" and "domain_stats" in context:
                domains = context["domain_stats"]
                if isinstance(domains, dict):
                    anomalies = []
                    for domain, stats in domains.items():
                        if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                            anomalies.append(f"High error rate in {domain}")
                    if anomalies:
                        elements.append(random.choice(anomalies))
        return elements

    def _assess_action_outcomes(self, actions):
        """
        Assess whether recent action outcomes are improving or deteriorating.
        """
        if not actions or len(actions) < 3:
            return "insufficient_data"
        success_indicators = []
        for action in actions:
            if not isinstance(action, dict):
                continue
            if "success" in action:
                success_indicators.append(1 if action["success"] else 0)
                continue
            if "content_length" in action:
                length = action["content_length"]
                success_indicators.append(min(1.0, length / 5000))
                continue
            if "links_discovered" in action:
                links = action["links_discovered"]
                success_indicators.append(min(1.0, links / 10))
                continue
        if len(success_indicators) < 3:
            return "insufficient_data"
        mid_point = len(success_indicators) // 2
        first_half = success_indicators[:mid_point]
        second_half = success_indicators[mid_point:]
        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)
        diff = second_avg - first_avg
        if abs(diff) < 0.1:
            return "stable"
        elif diff > 0:
            return "improving"
        else:
            return "deteriorating"

    def get_current_state(self):
        """
        Get the current state of the autonomous mind.
        """
        return {
            "current_mode": self.current_mode,
            "cognitive_load": self.cognitive_load,
            "thought_depth": self.thought_depth,
            "working_memory_usage": len(self.working_memory) / self.working_memory_capacity,
            "recent_thoughts": [{
                "content": t.get("content", ""),
                "type": t.get("type", ""),
                "importance": t.get("importance", 0.5)
            } for t in self.thought_history[-5:]] if self.thought_history else [],
            "important_insights": [{
                "content": i.get("content", ""),
                "importance": i.get("importance", 0.5)
            } for i in self.recent_insights[-3:]] if self.recent_insights else [],
            "active_concepts": [{
                "state": state,
                "activation": data["activation"]
            } for state, data in self.cognitive_states.items() if data["activation"] >= self.concept_activation_threshold],
            "thinking_style": self.thinking_style
        }

    def set_thinking_style(self, style_params):
        """
        Adjust thinking style parameters to change cognitive approach.
        """
        if not style_params or not isinstance(style_params, dict):
            return False
        for param, value in style_params.items():
            if param in self.thinking_style and isinstance(value, (int, float)):
                value = max(0.0, min(1.0, value))
                old_value = self.thinking_style[param]
                self.thinking_style[param] = value
                log_event(f"Thinking style parameter '{param}' adjusted: {old_value:.2f} → {value:.2f}", "INFO")
        return True

    def prime_with_context(self, context_elements):
        """
        Prime the mind with specific context elements to influence thinking.
        """
        if not context_elements or not isinstance(context_elements, dict):
            return False
        if "cognitive_modes" in context_elements:
            modes = context_elements["cognitive_modes"]
            if isinstance(modes, list):
                for mode in modes:
                    if mode in self.cognitive_states:
                        old_activation = self.cognitive_states[mode]["activation"]
                        self.cognitive_states[mode]["activation"] = min(0.9, old_activation * 1.5)
                        log_event(f"Cognitive mode '{mode}' primed: {old_activation:.2f} → {self.cognitive_states[mode]['activation']:.2f}", "INFO")
        if "depth" in context_elements:
            depth = context_elements["depth"]
            if isinstance(depth, (int, float)):
                self.thought_depth = max(0.0, min(1.0, depth))
                log_event(f"Thought depth primed to {self.thought_depth:.2f}", "INFO")
        if "focus" in context_elements:
            focus = context_elements["focus"]
            self.attention_focus = focus
            log_event(f"Attention focus primed to '{focus}'", "INFO")
        return True




# =============================================================================
# SELF-EVOLUTION MODULE - ADDED HERE
# =============================================================================
class MetaEvolutionEngine:
    """
    Engine for system-level self-evolution, adapting core algorithms,
    architectures, and strategies over longer timescales.
    """
    def __init__(self):
        self.evolution_generation = 1
        self.last_major_evolution = 0
        self.evolution_history = deque(maxlen=50)
        self.strategies = {
            "expansion": {"description": "Expand neural architecture", "success_rate": 0.6, "last_attempt": 0},
            "pruning": {"description": "Prune redundant components", "success_rate": 0.5, "last_attempt": 0},
            "restructuring": {"description": "Reorganize internal structure", "success_rate": 0.4, "last_attempt": 0},
            "specialization": {"description": "Specialize components for focused tasks", "success_rate": 0.55, "last_attempt": 0},
            "integration": {"description": "Integrate synergistic modules", "success_rate": 0.65, "last_attempt": 0}
        }
        self.goal_weights = {
            "performance": 0.5,
            "efficiency": 0.3,
            "adaptability": 0.4,
            "robustness": 0.4,
            "complexity": -0.2  # Negative weight: minimize complexity
        }
        self.capability_targets = {
            "knowledge_representation": 0.9,
            "planning": 0.85,
            "learning": 0.9,
            "error_handling": 0.8,
            "creative_synthesis": 0.7
        }
        self.capability_scores = {}
        self.adaptation_rate = 0.15  # Rate of adaptation pressure
        self.convergence_score = 0.0  # Score indicating evolutionary convergence
        self.performance_metrics = []  # Track performance metrics for self-modification
        self.thought_history = []  # For reflective thinking
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.recent_insights = []  # Store recent realizations
        self.agent = None  # Will be set when used

        log_event("MetaEvolutionEngine initialized", "INFO")

    def quantum_concept_blend(self, concept1, concept2, blend_factor=0.5):
        """
        Blend two concepts in an entangled conceptual space.

        Parameters:
        - concept1: Dictionary representing first concept
        - concept2: Dictionary representing second concept
        - blend_factor: Weight for blending (0.0-1.0)

        Returns:
        - Blended concept dictionary
        """
        if not isinstance(concept1, dict) or not isinstance(concept2, dict):
            return None

        # Extract key attributes
        attributes1 = set(concept1.keys())
        attributes2 = set(concept2.keys())

        # Find shared and unique attributes
        shared = attributes1.intersection(attributes2)
        unique1 = attributes1 - shared
        unique2 = attributes2 - shared

        # Create quantum superposition of concepts
        blended = {}

        # Shared attributes use quantum interference
        for attr in shared:
            if isinstance(concept1[attr], (int, float)) and isinstance(concept2[attr], (int, float)):
                # Quantum interference effect
                phase = random.uniform(0, math.pi * 2)
                blended[attr] = concept1[attr] * math.cos(phase) + concept2[attr] * math.sin(phase)
            else:
                # Non-numeric attributes use probabilistic selection
                blended[attr] = concept1[attr] if random.random() < blend_factor else concept2[attr]

        # Include unique attributes with probability based on blend factor
        for attr in unique1:
            if random.random() < blend_factor:
                blended[attr] = concept1[attr]

        for attr in unique2:
            if random.random() < (1 - blend_factor):
                blended[attr] = concept2[attr]

        # Generate an emergent property (something neither concept had)
        emergent_options = ["synergy", "transcendence", "complexity", "harmony", "paradox"]
        blended["emergent_property"] = random.choice(emergent_options)

        return blended

    def apply_self_modifications(self, agent):
        """
        Apply self-modifications to improve system capabilities

        Parameters:
        - agent: Agent to modify

        Returns:
        - (success, message) tuple
        """
        # Identify potential areas for improvement based on performance metrics
        if not self.performance_metrics or len(self.performance_metrics) < 10:
            return False, "Insufficient performance data for self-modification"

        # Analyze recent performance trends
        recent_metrics = self.performance_metrics[-10:]
        avg_loss = sum(m.get('loss', 0.5) for m in recent_metrics) / len(recent_metrics)
        loss_trend = recent_metrics[-1].get('loss', 0.5) - recent_metrics[0].get('loss', 0.5)

        modifications = []

        # 1. Architecture modifications
        if avg_loss > 0.3 and loss_trend > 0.05:
            # Loss is high and getting worse - try architecture expansion
            if hasattr(agent.model, 'expand_architecture'):
                agent.model.expand_architecture()
                modifications.append("Expanded neural architecture")
        elif avg_loss < 0.2 and hasattr(agent.model, 'neocortex') and len(agent.model.neocortex) > 10:
            # Loss is low with large architecture - try pruning
            if hasattr(agent.model, 'contract_architecture'):
                agent.model.contract_architecture()
                modifications.append("Pruned neural architecture for efficiency")

        # 2. Memory optimizations
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_set'):
            memory_size = len(agent.free_will.memory_set)
            if memory_size > MEMORY_MAX_SIZE * 0.9:
                # Memory approaching capacity - trigger pruning
                if hasattr(agent.free_will, 'contract_memory'):
                    agent.free_will.contract_memory(int(MEMORY_MAX_SIZE * 0.7))
                    modifications.append(f"Optimized memory from {memory_size} to {len(agent.free_will.memory_set)} items")

        # 3. Consciousness adjustments
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness
            if avg_loss > 0.3:
                # High loss - try boosting reflective awareness
                if hasattr(consciousness, 'increase_awareness'):
                    consciousness.increase_awareness(0.1)
                    if hasattr(consciousness, 'current_state'):
                        consciousness.current_state = "reflective"
                        modifications.append("Increased reflective awareness to address performance issues")

        if modifications:
            log_event(f"Self-modification applied: {', '.join(modifications)}", "QUANTUM")
            return True, f"Successfully applied {len(modifications)} self-modifications"

        return False, "No beneficial self-modifications identified"

    def evolve_system(self, agent):
        """
        Trigger system evolution based on performance metrics, goals,
        and environmental requirements.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Store agent reference for other methods to use
        self.agent = agent

        # Basic validation
        if not agent:
            return False, "Invalid agent provided to evolution engine."

        # Determine current cycle count
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check (don't evolve too frequently)
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = cycle_count - self.last_major_evolution

        if cycles_since_last < evolution_interval:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval} cycles since last evolution."

        # Increase generation counter
        self.evolution_generation += 1

        # First try self-modification approach if we have enough performance data
        if len(self.performance_metrics) >= 10:
            success, message = self.apply_self_modifications(agent)
            if success:
                # Record the evolution attempt
                evolution_record = {
                    "generation": self.evolution_generation,
                    "cycle": cycle_count,
                    "strategy": "self_modification",
                    "success": True,
                    "message": message,
                    "changes": message,
                    "timestamp": datetime.now().isoformat()
                }
                self.evolution_history.append(evolution_record)
                self.last_major_evolution = cycle_count
                log_event(f"System evolution through self-modification: {message}", "QUANTUM")
                return True, message

        # Continue with standard evolution if self-modification didn't succeed
        # Analyze current state and fitness
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        evolution_threshold = 0.3  # Base threshold
        evolution_probability = evolution_threshold + evolutionary_pressure

        if random.random() >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy
        selected_strategy = self._select_evolution_strategy(fitness_scores)

        # Apply the selected evolutionary strategy
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful
        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.
        """
        fitness_scores = {
            "performance": 0.5,
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }
        # Example: if agent.stats exists, use simple ratio as performance
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = agent.stats.get("cycles_run", 0)
            pages = agent.stats.get("pages_processed", 0)
            if cycles > 0:
                performance = pages / cycles
                fitness_scores["performance"] = max(0.1, min(0.9, performance / 10))
        return fitness_scores

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights.
        Lower overall fitness implies higher pressure.
        """
        total_weight = sum(abs(w) for w in self.goal_weights.values())
        weighted_sum = 0.0
        for metric, weight in self.goal_weights.items():
            score = fitness_scores.get(metric, 0.5)
            if weight < 0:
                score = 1 - score
            weighted_sum += score * abs(weight)
        avg_fitness = weighted_sum / total_weight if total_weight != 0 else 0.5
        pressure = (1 - avg_fitness) * self.adaptation_rate
        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on the current fitness scores.
        Here we choose the strategy corresponding to the capability with the largest gap.
        """
        gaps = {}
        for metric, target in self.capability_targets.items():
            current = fitness_scores.get(metric, 0.5)
            gaps[metric] = target - current
        most_lacking = max(gaps.items(), key=lambda x: x[1])[0]
        mapping = {
            "knowledge_representation": "expansion",
            "planning": "integration",
            "learning": "specialization",
            "error_handling": "pruning",
            "creative_synthesis": "restructuring"
        }

        # Occasionally use a blended strategy (10% chance)
        if random.random() < 0.1 and len(self.strategies) >= 2:
            regular_strategies = ["expansion", "pruning", "restructuring", "specialization", "integration"]
            available = [s for s in regular_strategies if s in self.strategies]
            if len(available) >= 2:
                s1, s2 = random.sample(available, 2)
                blended_name, _ = self.blend_strategies(s1, s2)
                if blended_name:
                    return blended_name

        return mapping.get(most_lacking, "expansion")

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")

                # Apply restructuring to agent's consciousness module if available
                if (hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness')):
                    consciousness = agent.ai_manager.consciousness

                    # Adjust awareness fluctuation rate
                    if hasattr(consciousness, 'awareness_fluctuation_rate'):
                        old_rate = consciousness.awareness_fluctuation_rate
                        new_rate = old_rate * random.uniform(0.8, 1.2)
                        consciousness.awareness_fluctuation_rate = max(0.01, min(0.1, new_rate))
                        changes.append(f"Adjusted consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")

                    # Reset to reflective state
                    if hasattr(consciousness, 'current_state'):
                        old_state = consciousness.current_state
                        consciousness.current_state = "reflective"
                        changes.append(f"Reset consciousness state from '{old_state}' to 'reflective'")

                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")

                # If agent has content sifter, specialize its topics
                if hasattr(agent, 'content_sifter'):
                    sifter = agent.content_sifter
                    if hasattr(sifter, 'topics_of_interest'):
                        old_topics = sifter.topics_of_interest
                        tech_topics = ["artificial intelligence", "quantum computing",
                                      "neural networks", "machine learning"]

                        # Keep some original topics
                        keep_count = max(3, int(len(old_topics) * 0.3))
                        kept_topics = random.sample(old_topics, keep_count)

                        # Create specialized topic list
                        new_topics = kept_topics + tech_topics
                        new_topics = list(dict.fromkeys(new_topics))  # Remove duplicates

                        # Apply change
                        sifter.topics_of_interest = new_topics
                        changes.append(f"Specialized content sifter with {len(new_topics)} focused topics")

                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                integration_count = 0

                # Integrate consciousness with planning if available
                if (hasattr(agent, 'ai_manager') and
                    hasattr(agent.ai_manager, 'consciousness') and
                    hasattr(agent.ai_manager, 'temporal_planner')):

                    consciousness = agent.ai_manager.consciousness
                    planner = agent.ai_manager.temporal_planner

                    if hasattr(consciousness, 'awareness_level') and hasattr(planner, 'reflection_interval'):
                        awareness = consciousness.awareness_level
                        old_interval = planner.reflection_interval
                        new_interval = int(max(5, 30 - 25 * awareness))
                        planner.reflection_interval = new_interval

                        changes.append(f"Integrated consciousness awareness with planning reflection interval: {old_interval} → {new_interval}")
                        integration_count += 1

                # Integrate free will with content filtering if applicable
                if hasattr(agent, 'free_will') and hasattr(agent, 'content_sifter'):
                    if hasattr(agent.free_will, 'memory_importance') and hasattr(agent.content_sifter, 'topics_of_interest'):
                        # Additional integration logic would go here
                        changes.append("Integrated memory importance with content filtering")
                        integration_count += 1

                success = True
                message = f"Integration strategy applied successfully with {integration_count} integrations."
            elif "_blend" in strategy:
                # Handle blended strategies
                changes.append("Applied blended strategy with emergent properties")
                success = True
                message = f"Blended strategy {strategy} applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"

        return success, message, changes

    def track_performance(self, metrics):
        """
        Track performance metrics for future self-modification decisions.

        Parameters:
        - metrics: Dictionary of performance metrics
        """
        if isinstance(metrics, dict):
            self.performance_metrics.append(metrics.copy())

            # Keep only the most recent metrics
            if len(self.performance_metrics) > 50:
                self.performance_metrics = self.performance_metrics[-50:]

    def blend_strategies(self, strategy1, strategy2):
        """
        Use quantum concept blending to create a hybrid strategy.

        Parameters:
        - strategy1, strategy2: Names of strategies to blend

        Returns:
        - New blended strategy name and properties
        """
        if (strategy1 not in self.strategies or
            strategy2 not in self.strategies):
            return None, None

        # Get strategy concepts
        concept1 = self.strategies[strategy1].copy()
        concept2 = self.strategies[strategy2].copy()

        # Quantum blend the concepts
        blended = self.quantum_concept_blend(concept1, concept2)

        if not blended:
            return None, None

        # Create a new strategy name
        new_name = f"{strategy1}_{strategy2}_blend"

        # Store the new strategy
        self.strategies[new_name] = blended

        log_event(f"Created new blended strategy: {new_name} with emergent property: {blended.get('emergent_property')}", "QUANTUM")

        return new_name, blended

    def _generate_reflective_thought(self, context):
        """Generate a reflective thought focused on self-examination"""
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def _generate_exploratory_thought(self, context):
        """Generate an exploratory thought focused on new possibilities"""
        thought = {
            "type": "exploratory",
            "content": "Exploration of new possibilities",
            "directions": [],
            "insights": [],
            "importance": 0.5
        }
        exploration_areas = [
            "unknown_domains",
            "connection_patterns",
            "alternative_strategies",
            "capability_expansion"
        ]
        selected_areas = random.sample(exploration_areas, min(2, len(exploration_areas)))
        directions = []
        for area in selected_areas:
            if area == "unknown_domains":
                if "domains_visited" in context:
                    domains_visited = context.get("domains_visited", set())
                    if isinstance(domains_visited, set):
                        candidate_domains = [
                            "research.science", "github.com", "en.wikipedia.org",
                            "arxiv.org", "semanticscholar.org", "openai.com",
                            "nature.com", "reddit.com/r/MachineLearning"
                        ]
                        unvisited = [d for d in candidate_domains if d not in domains_visited]
                        if unvisited:
                            sample_domains = random.sample(unvisited, min(3, len(unvisited)))
                            directions.append({
                                "area": "unknown_domains",
                                "content": f"Explore high-value domains: {', '.join(sample_domains)}",
                                "rationale": "Expanding domain knowledge diversity"
                            })
            elif area == "connection_patterns":
                directions.append({
                    "area": "connection_patterns",
                    "content": "Implement knowledge graph traversal to find distant connections",
                    "rationale": "Distant domains often contain valuable cross-applicable patterns"
                })
            elif area == "alternative_strategies":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = list(self.agent.planner_sifter.strategies.keys())
                    if strategies:
                        if (hasattr(self.agent.planner_sifter, 'strategy_usage')):
                            usage = self.agent.planner_sifter.strategy_usage
                            least_used = sorted([(s, usage.get(s, 0)) for s in strategies], key=lambda x: x[1])
                            if least_used:
                                least_used_strategy = least_used[0][0]
                                directions.append({
                                    "area": "alternative_strategies",
                                    "content": f"Experiment with underutilized strategy: {least_used_strategy}",
                                    "rationale": "Diversifying strategic approaches to discover new optima"
                                })
            elif area == "capability_expansion":
                directions.append({
                    "area": "capability_expansion",
                    "content": "Develop enhanced semantic reasoning module",
                    "rationale": "Would significantly improve knowledge integration capabilities"
                })
        insights = []
        for direction in directions:
            area = direction.get("area", "")
            rationale = direction.get("rationale", "")
            if area == "unknown_domains":
                insights.append("Systematic exploration of high-value domains should be prioritized")
            elif area == "connection_patterns":
                insights.append("Knowledge value may lie in unexpected cross-domain connections")
            elif area == "alternative_strategies":
                insights.append("Strategic diversification may uncover more effective approaches")
            elif area == "capability_expansion":
                insights.append("Capability development should focus on knowledge integration")
        thought["directions"] = directions
        thought["insights"] = insights
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)
        if insights:
            thought["content"] = insights[0]
        elif directions:
            thought["content"] = directions[0].get("content", "Exploration suggestion")
        return thought

    def _generate_critical_thought(self, context):
        """Generate a critical thought focused on evaluation and assessment"""
        thought = {
            "type": "critical",
            "content": "Critical evaluation",
            "assessments": [],
            "insights": [],
            "importance": 0.5
        }
        assessment_areas = [
            "information_quality",
            "reasoning_validity",
            "strategy_efficiency",
            "resource_allocation",
            "error_handling"
        ]
        selected_areas = random.sample(assessment_areas, min(2, len(assessment_areas)))
        assessments = []
        for area in selected_areas:
            if area == "information_quality":
                if (hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'domain_intelligence')):
                    intelligence = self.agent.free_will.domain_intelligence
                    if hasattr(intelligence, 'domain_knowledge'):
                        knowledge = intelligence.domain_knowledge
                        if knowledge:
                            low_quality_domains = []
                            for domain, data in knowledge.items():
                                if isinstance(data, dict) and data.get("content_quality", 1.0) < 0.5:
                                    low_quality_domains.append(domain)
                            if low_quality_domains:
                                sample_domains = random.sample(low_quality_domains, min(2, len(low_quality_domains)))
                                assessments.append({
                                    "area": "information_quality",
                                    "content": f"Low-quality content detected in domains: {', '.join(sample_domains)}",
                                    "recommendation": "Implement stricter quality thresholds for these domains"
                                })
                            else:
                                assessments.append({
                                    "area": "information_quality",
                                    "content": "Overall information quality appears acceptable",
                                    "recommendation": "Continue regular quality monitoring"
                                })
            elif area == "reasoning_validity":
                if self.thought_history:
                    recent_thoughts = self.thought_history[-10:] if len(self.thought_history) >= 10 else self.thought_history
                    thought_types = set(t.get("type", "") for t in recent_thoughts)
                    if len(thought_types) < 3:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Limited cognitive diversity detected (only {len(thought_types)} thought types used recently)",
                            "recommendation": "Deliberately activate more diverse thinking modes"
                        })
                    else:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Cognitive diversity appears healthy ({len(thought_types)} different thought types used recently)",
                            "recommendation": "Maintain diverse thinking patterns"
                        })
            elif area == "strategy_efficiency":
                if (hasattr(self.agent, 'stats') and
                    isinstance(self.agent.stats, dict) and
                    'cycles_run' in self.agent.stats and
                    'pages_processed' in self.agent.stats):
                    cycles = self.agent.stats['cycles_run']
                    pages = self.agent.stats['pages_processed']
                    if cycles > 10:
                        efficiency = pages / max(1, cycles)
                        if efficiency < 0.5:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Low efficiency detected: {efficiency:.2f} pages/cycle",
                                "recommendation": "Consider more aggressive pruning of low-value paths"
                            })
                        else:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Acceptable efficiency: {efficiency:.2f} pages/cycle",
                                "recommendation": "Continue current approach with regular efficiency monitoring"
                            })
            elif area == "resource_allocation":
                if (hasattr(self.agent, 'memory') and
                    hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'memory_set')):
                    memory_usage = len(self.agent.memory) / MEMORY_MAX_SIZE
                    urls_stored = len(self.agent.free_will.memory_set)
                    if memory_usage > 0.8 and urls_stored > 1000:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"High memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Implement more aggressive memory pruning strategy"
                        })
                    else:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"Memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Current resource allocation appears appropriate"
                        })
            elif area == "error_handling":
                if (hasattr(self.agent, 'domain_stats') and isinstance(self.agent.domain_stats, dict)):
                    total_visits = sum(d.get("visits", 0) for d in self.agent.domain_stats.values())
                    total_errors = sum(d.get("error_count", 0) for d in self.agent.domain_stats.values())
                    if total_visits > 0:
                        error_rate = total_errors / total_visits
                        if error_rate > 0.2:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"High system-wide error rate: {error_rate:.2%}",
                                "recommendation": "Investigate error causes and implement more robust handling"
                            })
                        else:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"Acceptable error rate: {error_rate:.2%}",
                                "recommendation": "Continue monitoring error patterns"
                            })
        insights = []
        for assessment in assessments:
            recommendation = assessment.get("recommendation", "")
            if "low-quality" in assessment.get("content", "").lower():
                insights.append("Information quality control needs improvement")
            elif "limited cognitive diversity" in assessment.get("content", "").lower():
                insights.append("Need to activate more diverse thinking patterns")
            elif "low efficiency" in assessment.get("content", "").lower():
                insights.append("Strategy efficiency requires optimization")
            elif "high memory usage" in assessment.get("content", "").lower():
                insights.append("Memory management needs more efficient pruning")
            elif "high system-wide error" in assessment.get("content", "").lower():
                insights.append("Error handling systems require review and enhancement")
            elif recommendation:
                insights.append(recommendation)
        thought["assessments"] = assessments
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if any("high" in a.get("content", "").lower() for a in assessments):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif assessments:
            thought["content"] = assessments[0].get("content", "Critical assessment")
        return thought

    def _generate_integrative_thought(self, context):
        """Generate an integrative thought focused on knowledge synthesis"""
        thought = {
            "type": "integrative",
            "content": "Knowledge synthesis",
            "connections": [],
            "insights": [],
            "importance": 0.5
        }
        connections = []
        if not context:
            thought["content"] = "Insufficient knowledge for meaningful integration"
            return thought
        elements = {}
        if "domains_visited" in context:
            domains = context.get("domains_visited", set())
            if isinstance(domains, set) and len(domains) > 3:
                elements["domains"] = list(domains)
        if hasattr(self.agent, 'content_sifter'):
            sifter = self.agent.content_sifter
            if hasattr(sifter, 'topics_of_interest'):
                elements["topics"] = sifter.topics_of_interest
        if (hasattr(self.agent, 'ai_manager') and
            hasattr(self.agent.ai_manager, 'temporal_planner') and
            hasattr(self.agent.ai_manager.temporal_planner, 'long_term_goals')):
            goals = self.agent.ai_manager.temporal_planner.long_term_goals
            if goals:
                elements["goals"] = [g.get("description", "") for g in goals]
        if len(elements) >= 2:
            element_types = list(elements.keys())
            type_pair = random.sample(element_types, 2)
            elements_1 = elements[type_pair[0]]
            elements_2 = elements[type_pair[1]]
            if elements_1 and elements_2:
                element_1 = random.choice(elements_1)
                element_2 = random.choice(elements_2)
                connections.append({
                    "elements": [
                        {"type": type_pair[0], "value": element_1},
                        {"type": type_pair[1], "value": element_2}
                    ],
                    "connection_type": "integration",
                    "description": f"Integration of {type_pair[0]} '{element_1}' with {type_pair[1]} '{element_2}'",
                    "potential": random.uniform(0.5, 0.9)
                })
        if not connections:
            theoretical_connections = [
                {
                    "elements": [
                        {"type": "cognitive_mode", "value": "analytical"},
                        {"type": "capability", "value": "semantic_representation"}
                    ],
                    "connection_type": "enhancement",
                    "description": "Analytical thinking could enhance semantic representation quality",
                    "potential": 0.7
                },
                {
                    "elements": [
                        {"type": "capability", "value": "memory_management"},
                        {"type": "capability", "value": "planning"}
                    ],
                    "connection_type": "synergy",
                    "description": "Memory systems could be more tightly integrated with planning",
                    "potential": 0.8
                }
            ]
            connections.append(random.choice(theoretical_connections))
        insights = []
        for connection in connections:
            desc = connection.get("description", "")
            conn_type = connection.get("connection_type", "")
            potential = connection.get("potential", 0.5)
            if conn_type == "integration" and potential > 0.7:
                insights.append(f"High potential integration opportunity: {desc}")
            elif conn_type == "enhancement":
                insights.append(f"Enhancement pathway identified: {desc}")
            elif conn_type == "synergy":
                insights.append(f"Synergistic relationship would increase capability: {desc}")
            else:
                insights.append(f"Connection opportunity: {desc}")
        thought["connections"] = connections
        thought["insights"] = insights
        importance = 0.5
        if connections:
            avg_potential = sum(c.get("potential", 0.5) for c in connections) / len(connections)
            importance = avg_potential
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif connections:
            thought["content"] = connections[0].get("description", "Knowledge integration")
        return thought

    def _generate_intuitive_thought(self, context):
        """Generate an intuitive thought focused on pattern recognition"""
        thought = {
            "type": "intuitive",
            "content": "Pattern recognition",
            "patterns": [],
            "insights": [],
            "importance": 0.5
        }
        patterns = []
        if not context:
            thought["content"] = "Insufficient data for pattern recognition"
            return thought
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if len(actions) >= 3:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                if len(action_types) >= 3:
                    repetitions = []
                    current_sequence = [action_types[0]]
                    for i in range(1, len(action_types)):
                        if action_types[i] == action_types[i-1]:
                            current_sequence.append(action_types[i])
                        else:
                            if len(current_sequence) >= 2:
                                repetitions.append(current_sequence)
                            current_sequence = [action_types[i]]
                    if len(current_sequence) >= 2:
                        repetitions.append(current_sequence)
                    if repetitions:
                        longest_repetition = max(repetitions, key=len)
                        patterns.append({
                            "type": "action_repetition",
                            "description": f"Repeated sequence of '{longest_repetition[0]}' actions",
                            "significance": 0.6 + 0.1 * min(4, len(longest_repetition)),
                            "potential_cause": "Strategy fixation or optimal local strategy"
                        })
                    if len(set(action_types)) == 2 and len(action_types) >= 4:
                        is_alternating = True
                        for i in range(2, len(action_types)):
                            if action_types[i] != action_types[i-2]:
                                is_alternating = False
                                break
                        if is_alternating:
                            patterns.append({
                                "type": "action_alternation",
                                "description": f"Alternating pattern between '{action_types[0]}' and '{action_types[1]}'",
                                "significance": 0.7,
                                "potential_cause": "Explore-exploit cycle or complementary strategies"
                            })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                frequent_domains = sorted(
                    [(d, stats.get("visits", 0)) for d, stats in domains.items()],
                    key=lambda x: x[1],
                    reverse=True
                )
                if frequent_domains:
                    top_domains = frequent_domains[:3]
                    total_visits = sum(v for _, v in frequent_domains)
                    top_domain_visits = sum(v for _, v in top_domains)
                    concentration = top_domain_visits / max(1, total_visits)
                    if concentration > 0.7:
                        patterns.append({
                            "type": "domain_concentration",
                            "description": f"Heavy concentration ({concentration:.0%}) on top 3 domains",
                            "significance": 0.7,
                            "potential_cause": "Exploitation focus or domain specialization"
                        })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                error_domains = []
                for domain, stats in domains.items():
                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                        error_domains.append((domain, stats.get("error_rate", 0)))
                if error_domains:
                    error_domains.sort(key=lambda x: x[1], reverse=True)
                    top_error_domains = error_domains[:3]
                    patterns.append({
                        "type": "error_concentration",
                        "description": f"High error rates in specific domains: {', '.join(d for d, _ in top_error_domains)}",
                        "significance": 0.8,
                        "potential_cause": "Domain-specific access issues or content filtering problems"
                    })
        insights = []
        for pattern in patterns:
            pattern_type = pattern.get("type", "")
            significance = pattern.get("significance", 0.5)
            if pattern_type == "action_repetition" and significance > 0.7:
                insights.append("Repeated action pattern may indicate strategy fixation - consider forcing exploration")
            elif pattern_type == "action_alternation":
                insights.append("Alternating action pattern suggests systematic exploration-exploitation approach")
            elif pattern_type == "domain_concentration":
                insights.append("High domain concentration indicates need for broader exploration")
            elif pattern_type == "error_concentration":
                insights.append("Domain-specific error pattern detected - consider domain-specific handling strategies")
        thought["patterns"] = patterns
        thought["insights"] = insights
        importance = 0.5
        if patterns:
            avg_significance = sum(p.get("significance", 0.5) for p in patterns) / len(patterns)
            importance = avg_significance
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif patterns:
            thought["content"] = patterns[0].get("description", "Pattern detected")
        return thought

class AutonomousMind:
    """
    Integrated cognitive system that coordinates high-level thinking processes
    and manages the agent's internal cognitive states and modes.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model
        self.thought_history = []
        self.cognitive_states = {
            "analytical": {"description": "Logical problem solving with structured reasoning", "activation": 0.5},
            "creative": {"description": "Divergent thinking with novel connections", "activation": 0.5},
            "reflective": {"description": "Meta-cognitive examination of own thoughts", "activation": 0.5},
            "exploratory": {"description": "Curious investigation of new information", "activation": 0.5},
            "critical": {"description": "Evaluative assessment with skepticism", "activation": 0.5},
            "integrative": {"description": "Synthesis of diverse knowledge", "activation": 0.5},
            "intuitive": {"description": "Fast pattern recognition", "activation": 0.5},
            "balanced": {"description": "Equilibrium of multiple modes", "activation": 0.5}
        }
        self.current_mode = "balanced"  # Default mode
        self.attention_focus = None  # Current attentional focus
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.cognitive_load = 0.5  # Current processing load (0.0-1.0)
        self.thought_depth = 0.5  # Depth vs. breadth of thinking (0.0-1.0)
        self.concept_activation_threshold = 0.3  # Min activation for attention
        self.mode_shift_probability = 0.2  # Probability of spontaneous mode shift
        self.recent_insights = []  # Store recent important realizations

        # Thinking style parameters
        self.thinking_style = {
            "abstraction_level": 0.6,  # Concrete (0.0) to abstract (1.0)
            "linearity": 0.5,  # Linear (0.0) to non-linear (1.0)
            "deductive_inductive_balance": 0.5,  # Deductive (0.0) to inductive (1.0)
            "risk_tolerance": 0.4,  # Risk-averse (0.0) to risk-seeking (1.0)
            "concept_granularity": 0.5  # Fine-grained (0.0) to coarse-grained (1.0)
        }

        log_event("AutonomousMind initialized in balanced cognitive mode", "INFO")

    def think(self, context=None):
        """
        Execute a thinking cycle to generate insights, shift cognitive modes,
        and update internal state based on current context.

        Parameters:
        - context: Current perceptual and memory context

        Returns:
        - Thought result containing insights and state changes
        """
        # Update cognitive load based on context complexity
        self._update_cognitive_load(context)

        # Consider shifting cognitive mode
        self._consider_mode_shift(context)

        # Generate thought based on current mode and context
        thought = self._generate_thought(context)

        # Update working memory
        self._update_working_memory(thought)

        # Store in thought history
        self.thought_history.append({
            "thought": thought,
            "mode": self.current_mode,
            "timestamp": datetime.now().isoformat(),
            "context": self._summarize_context(context)
        })

        # Trim history if needed
        if len(self.thought_history) > 100:
            self.thought_history = self.thought_history[-100:]

        return thought

    def _update_cognitive_load(self, context):
        """Update cognitive load based on context complexity"""
        if not context:
            # Default slight reduction in cognitive load when no new input
            self.cognitive_load = max(0.1, self.cognitive_load * 0.9)
            return

        # Estimate context complexity
        complexity = 0.5  # Default medium complexity

        # Adjust based on context details if available
        if isinstance(context, dict):
            # More elements = higher complexity
            complexity += min(0.3, len(context) * 0.02)

            # Check for special high-complexity elements
            if "error" in context or "anomaly" in context:
                complexity += 0.2

            # High memory usage increases cognitive load
            if "memory_size" in context:
                memory_usage = context["memory_size"] / MEMORY_MAX_SIZE
                complexity += memory_usage * 0.2

            # Multiple recent actions increase complexity
            if "recent_actions" in context and isinstance(context["recent_actions"], list):
                complexity += min(0.1, len(context["recent_actions"]) * 0.02)

        # Dynamically adjust cognitive load
        # New complexity pulls the current load toward itself
        adjustment_rate = 0.3  # How quickly load adjusts
        self.cognitive_load = self.cognitive_load * (1 - adjustment_rate) + complexity * adjustment_rate

        # Ensure within bounds
        self.cognitive_load = max(0.1, min(0.9, self.cognitive_load))

    def _consider_mode_shift(self, context):
        """Consider shifting cognitive mode based on context and internal state"""
        # Base probability of mode shift
        shift_probability = self.mode_shift_probability

        # Adjust based on cognitive load - higher load may require mode shift
        if self.cognitive_load > 0.7:
            shift_probability += 0.2

        # Check if we should shift
        if random.random() >= shift_probability:
            return  # No shift this cycle

        # Current active states
        active_states = {state: data["activation"] for state, data in self.cognitive_states.items()
                       if data["activation"] >= self.concept_activation_threshold}

        # Context-based mode selection
        new_mode = self._select_appropriate_mode(context, active_states)

        # If new mode is different, make the shift
        if new_mode and new_mode != self.current_mode:
            old_mode = self.current_mode
            self.current_mode = new_mode

            log_event(f"Cognitive mode shifted: {old_mode} → {new_mode}", "INFO")

            # Adjust activations - boost new mode, slightly reduce others
            for state in self.cognitive_states:
                if state == new_mode:
                    self.cognitive_states[state]["activation"] = min(0.9, self.cognitive_states[state]["activation"] + 0.2)
                else:
                    self.cognitive_states[state]["activation"] = max(0.1, self.cognitive_states[state]["activation"] * 0.9)

    def _select_appropriate_mode(self, context, active_states):
        """
        Select most appropriate cognitive mode based on context.

        Parameters:
        - context: Current context dictionary
        - active_states: Dictionary of currently active cognitive states

        Returns:
        - Selected mode name
        """
        if not context:
            # Without context, weighted random selection from active states
            if active_states:
                states = list(active_states.keys())
                weights = [active_states[s] for s in states]
                return random.choices(states, weights=weights, k=1)[0]
            else:
                return "balanced"  # Default fallback

        # Context-based selection
        mode_scores = {mode: 0.0 for mode in self.cognitive_states}

        # 1. Check for error or anomaly - activates critical mode
        if "error" in context or "anomaly" in context:
            mode_scores["critical"] += 0.5

        # 2. Check for exploration needs
        if self._context_indicates_exploration(context):
            mode_scores["exploratory"] += 0.4
            mode_scores["creative"] += 0.3

        # 3. Check for complex problem solving
        if self._context_indicates_complex_problem(context):
            mode_scores["analytical"] += 0.5
            mode_scores["integrative"] += 0.3

        # 4. Check for learning and reflection needs
        if self._context_indicates_reflection(context):
            mode_scores["reflective"] += 0.5

        # 5. Check for pattern recognition needs
        if self._context_indicates_pattern_recognition(context):
            mode_scores["intuitive"] += 0.4

        # 6. Add random factor for exploration
        for mode in mode_scores:
            mode_scores[mode] += random.uniform(0, 0.2)

        # 7. Add existing activation bias
        for mode, activation in active_states.items():
            mode_scores[mode] += activation * 0.3

        # Select highest scoring mode
        if mode_scores:
            top_mode = max(mode_scores.items(), key=lambda x: x[1])[0]
            return top_mode
        else:
            return "balanced"  # Default fallback

    def _context_indicates_exploration(self, context):
        """Check if context suggests exploration needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Looking for exploration indicators
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            if "explor" in goal_desc or "discover" in goal_desc:
                indicators += 1

        # Few domains visited suggests exploration need
        if "domains_visited" in context:
            domains = context["domains_visited"]
            if isinstance(domains, set) and len(domains) < 10:
                indicators += 1

        # Current mode includes exploration elements
        if context.get("thinking_mode") in ["exploratory", "creative"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_complex_problem(self, context):
        """Check if context suggests complex problem solving needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with certain keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            complex_terms = ["optim", "improv", "analy", "solv", "complex"]
            if any(term in goal_desc for term in complex_terms):
                indicators += 1

        # High cognitive load suggests complex problem
        if self.cognitive_load > 0.7:
            indicators += 1

        # Current analytical thinking mode
        if context.get("thinking_mode") in ["analytical", "critical"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_reflection(self, context):
        """Check if context suggests reflection needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with reflection keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            reflection_terms = ["reflect", "learn", "adapt", "improv", "assess"]
            if any(term in goal_desc for term in reflection_terms):
                indicators += 1

        # After many actions, reflection is valuable
        if "stats" in context and isinstance(context["stats"], dict):
            cycles = context["stats"].get("cycles_run", 0)
            if cycles > 0 and cycles % 10 == 0:  # Every 10 cycles
                indicators += 1

        # Current reflective thinking mode
        if context.get("thinking_mode") == "reflective":
            indicators += 1

        return indicators >= 1

    def _context_indicates_pattern_recognition(self, context):
        """Check if context suggests pattern recognition needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Domain stats suggest pattern recognition
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            # Many domains = opportunity for pattern recognition
            if len(context["domain_stats"]) > 5:
                indicators += 1

        # Multiple recent actions create patterns
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            if len(context["recent_actions"]) >= 3:
                indicators += 1

        # Current intuitive mode
        if context.get("thinking_mode") == "intuitive":
            indicators += 1

        return indicators >= 1

    def _generate_thought(self, context):
        """
        Generate a thought based on current cognitive mode and context.

        Parameters:
        - context: Current context dictionary

        Returns:
        - Generated thought dictionary
        """
        # Each cognitive mode has a different thought generation approach
        if self.current_mode == "analytical":
            return self._generate_analytical_thought(context)
        elif self.current_mode == "creative":
            return self._generate_creative_thought(context)
        elif self.current_mode == "reflective":
            return self._generate_reflective_thought(context)
        elif self.current_mode == "exploratory":
            return self._generate_exploratory_thought(context)
        elif self.current_mode == "critical":
            return self._generate_critical_thought(context)
        elif self.current_mode == "integrative":
            return self._generate_integrative_thought(context)
        elif self.current_mode == "intuitive":
            return self._generate_intuitive_thought(context)
        else:  # balanced or any other
            return self._generate_balanced_thought(context)

    def _generate_analytical_thought(self, context):
        """Generate an analytical thought focused on logical problem solving"""
        # Default analytical thought structure
        thought = {
            "type": "analytical",
            "content": "Systematic analysis of current state and options",
            "components": [],
            "insights": [],
            "importance": 0.5
        }

        # Without context, generate generic analytical thought
        if not context:
            thought["content"] = "Need more information to perform proper analysis"
            return thought

        # Analytical components based on context
        components = []

        # Analyze current goal if available
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal = context["current_goal"]
            components.append({
                "focus": "goal_analysis",
                "content": f"Analyzing goal: {goal.get('description', 'unknown')}",
                "priority": goal.get("priority", 0.5)
            })

        # Analyze recent actions if available
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if actions:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                components.append({
                    "focus": "action_pattern_analysis",
                    "content": f"Recent action sequence: {' → '.join(action_types)}",
                    "outcome_assessment": self._assess_action_outcomes(actions)
                })

        # Analyze domain statistics if available
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                high_error_domains = [d for d, stats in domains.items()
                                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3]
                if high_error_domains:
                    components.append({
                        "focus": "error_analysis",
                        "content": f"High error rates detected in domains: {', '.join(high_error_domains[:3])}",
                        "recommendation": "Investigate causes or avoid these domains temporarily"
                    })

        # Generate insights based on analysis
        insights = []

        # Derive insights from components
        for component in components:
            if component["focus"] == "goal_analysis" and component.get("priority", 0) > 0.7:
                insights.append("Current goal is high priority - allocate additional resources")
            elif component["focus"] == "action_pattern_analysis":
                outcome = component.get("outcome_assessment")
                if outcome == "deteriorating":
                    insights.append("Action patterns show declining effectiveness - strategy change needed")
                elif outcome == "improving":
                    insights.append("Action patterns show improving outcomes - continue current approach")
            elif component["focus"] == "error_analysis":
                insights.append("Error pattern analysis suggests need for improved domain validation")

        # Add components and insights to thought
        thought["components"] = components
        thought["insights"] = insights

        # Calculate importance based on components and insights
        if insights:
            thought["importance"] = 0.7

        # Generate summary content
        if insights:
            thought["content"] = f"Analytical conclusion: {insights[0]}"
        elif components:
            thought["content"] = f"Analysis of {len(components)} system components completed"

        return thought

    def _generate_creative_thought(self, context):
        """Generate a creative thought focused on novel connections"""
        # Default creative thought structure
        thought = {
            "type": "creative",
            "content": "Novel conceptual connection",
            "associations": [],
            "insights": [],
            "importance": 0.5
        }

        # Creative aspects to consider
        aspects = ["goals", "domains", "strategies", "patterns", "anomalies"]
        selected_aspects = random.sample(aspects, min(2, len(aspects)))

        # Generate random associations between selected aspects
        associations = []

        # Extract relevant elements from context
        elements = self._extract_elements_from_context(context, selected_aspects)

        # Generate associations between elements
        if len(elements) >= 2:
            # Take two random elements and create association
            element_pairs = []
            for i in range(min(3, len(elements))):
                pair = random.sample(elements, 2)
                element_pairs.append(pair)

            # Create associations from pairs
            for pair in element_pairs:
                association_templates = [
                    f"Unexpected connection between {pair[0]} and {pair[1]}",
                    f"{pair[0]} could be viewed through the lens of {pair[1]}",
                    f"What if the structure of {pair[0]} were applied to {pair[1]}?",
                    f"The patterns in {pair[0]} mirror aspects of {pair[1]} in a novel way"
                ]
                associations.append(random.choice(association_templates))
        else:
            # Fallback for limited context
            association_templates = [
                "Novel perspective: consider alternative goal structures",
                "What if search and evaluation were integrated more tightly?",
                "The pattern of successful interactions might reveal emergent properties"
            ]
            associations.append(random.choice(association_templates))

        # Generate insights from associations
        insights = []
        if associations:
            for association in associations:
                if "connection between" in association:
                    insights.append(f"Explore this connection to potentially develop new capabilities")
                elif "through the lens" in association:
                    insights.append(f"This perspective shift might reveal hidden patterns")
                elif "What if" in association:
                    insights.append(f"This hypothetical restructuring could improve system flexibility")
                else:
                    insights.append(f"This pattern recognition suggests deeper structural similarities")

        # Add associations and insights to thought
        thought["associations"] = associations
        thought["insights"] = insights

        # Calculate importance
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)



class AutonomousMind:
    """
    Integrated cognitive system that coordinates high-level thinking processes
    and manages the agent's internal cognitive states and modes.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model
        self.thought_history = []
        self.cognitive_states = {
            "analytical": {"description": "Logical problem solving with structured reasoning", "activation": 0.5},
            "creative": {"description": "Divergent thinking with novel connections", "activation": 0.5},
            "reflective": {"description": "Meta-cognitive examination of own thoughts", "activation": 0.5},
            "exploratory": {"description": "Curious investigation of new information", "activation": 0.5},
            "critical": {"description": "Evaluative assessment with skepticism", "activation": 0.5},
            "integrative": {"description": "Synthesis of diverse knowledge", "activation": 0.5},
            "intuitive": {"description": "Fast pattern recognition", "activation": 0.5},
            "balanced": {"description": "Equilibrium of multiple modes", "activation": 0.5}
        }
        self.current_mode = "balanced"  # Default mode
        self.attention_focus = None  # Current attentional focus
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.cognitive_load = 0.5  # Current processing load (0.0-1.0)
        self.thought_depth = 0.5  # Depth vs. breadth of thinking (0.0-1.0)
        self.concept_activation_threshold = 0.3  # Min activation for attention
        self.mode_shift_probability = 0.2  # Probability of spontaneous mode shift
        self.recent_insights = []  # Store recent important realizations

        # Thinking style parameters
        self.thinking_style = {
            "abstraction_level": 0.6,  # Concrete (0.0) to abstract (1.0)
            "linearity": 0.5,  # Linear (0.0) to non-linear (1.0)
            "deductive_inductive_balance": 0.5,  # Deductive (0.0) to inductive (1.0)
            "risk_tolerance": 0.4,  # Risk-averse (0.0) to risk-seeking (1.0)
            "concept_granularity": 0.5  # Fine-grained (0.0) to coarse-grained (1.0)
        }

        log_event("AutonomousMind initialized in balanced cognitive mode", "INFO")

    def think(self, context=None):
        """
        Execute a thinking cycle to generate insights, shift cognitive modes,
        and update internal state based on current context.

        Parameters:
        - context: Current perceptual and memory context

        Returns:
        - Thought result containing insights and state changes
        """
        # Update cognitive load based on context complexity
        self._update_cognitive_load(context)

        # Consider shifting cognitive mode
        self._consider_mode_shift(context)

        # Generate thought based on current mode and context
        thought = self._generate_thought(context)

        # Update working memory
        self._update_working_memory(thought)

        # Store in thought history
        self.thought_history.append({
            "thought": thought,
            "mode": self.current_mode,
            "timestamp": datetime.now().isoformat(),
            "context": self._summarize_context(context)
        })

        # Trim history if needed
        if len(self.thought_history) > 100:
            self.thought_history = self.thought_history[-100:]

        return thought

    def _update_cognitive_load(self, context):
        """Update cognitive load based on context complexity"""
        if not context:
            # Default slight reduction in cognitive load when no new input
            self.cognitive_load = max(0.1, self.cognitive_load * 0.9)
            return

        # Estimate context complexity
        complexity = 0.5  # Default medium complexity

        # Adjust based on context details if available
        if isinstance(context, dict):
            # More elements = higher complexity
            complexity += min(0.3, len(context) * 0.02)

            # Check for special high-complexity elements
            if "error" in context or "anomaly" in context:
                complexity += 0.2

            # High memory usage increases cognitive load
            if "memory_size" in context:
                memory_usage = context["memory_size"] / MEMORY_MAX_SIZE
                complexity += memory_usage * 0.2

            # Multiple recent actions increase complexity
            if "recent_actions" in context and isinstance(context["recent_actions"], list):
                complexity += min(0.1, len(context["recent_actions"]) * 0.02)

        # Dynamically adjust cognitive load
        # New complexity pulls the current load toward itself
        adjustment_rate = 0.3  # How quickly load adjusts
        self.cognitive_load = self.cognitive_load * (1 - adjustment_rate) + complexity * adjustment_rate

        # Ensure within bounds
        self.cognitive_load = max(0.1, min(0.9, self.cognitive_load))

    def _consider_mode_shift(self, context):
        """Consider shifting cognitive mode based on context and internal state"""
        # Base probability of mode shift
        shift_probability = self.mode_shift_probability

        # Adjust based on cognitive load - higher load may require mode shift
        if self.cognitive_load > 0.7:
            shift_probability += 0.2

        # Check if we should shift
        if random.random() >= shift_probability:
            return  # No shift this cycle

        # Current active states
        active_states = {state: data["activation"] for state, data in self.cognitive_states.items()
                       if data["activation"] >= self.concept_activation_threshold}

        # Context-based mode selection
        new_mode = self._select_appropriate_mode(context, active_states)

        # If new mode is different, make the shift
        if new_mode and new_mode != self.current_mode:
            old_mode = self.current_mode
            self.current_mode = new_mode

            log_event(f"Cognitive mode shifted: {old_mode} → {new_mode}", "INFO")

            # Adjust activations - boost new mode, slightly reduce others
            for state in self.cognitive_states:
                if state == new_mode:
                    self.cognitive_states[state]["activation"] = min(0.9, self.cognitive_states[state]["activation"] + 0.2)
                else:
                    self.cognitive_states[state]["activation"] = max(0.1, self.cognitive_states[state]["activation"] * 0.9)

    def _select_appropriate_mode(self, context, active_states):
        """
        Select most appropriate cognitive mode based on context.

        Parameters:
        - context: Current context dictionary
        - active_states: Dictionary of currently active cognitive states

        Returns:
        - Selected mode name
        """
        if not context:
            # Without context, weighted random selection from active states
            if active_states:
                states = list(active_states.keys())
                weights = [active_states[s] for s in states]
                return random.choices(states, weights=weights, k=1)[0]
            else:
                return "balanced"  # Default fallback

        # Context-based selection
        mode_scores = {mode: 0.0 for mode in self.cognitive_states}

        # 1. Check for error or anomaly - activates critical mode
        if "error" in context or "anomaly" in context:
            mode_scores["critical"] += 0.5

        # 2. Check for exploration needs
        if self._context_indicates_exploration(context):
            mode_scores["exploratory"] += 0.4
            mode_scores["creative"] += 0.3

        # 3. Check for complex problem solving
        if self._context_indicates_complex_problem(context):
            mode_scores["analytical"] += 0.5
            mode_scores["integrative"] += 0.3

        # 4. Check for learning and reflection needs
        if self._context_indicates_reflection(context):
            mode_scores["reflective"] += 0.5

        # 5. Check for pattern recognition needs
        if self._context_indicates_pattern_recognition(context):
            mode_scores["intuitive"] += 0.4

        # 6. Add random factor for exploration
        for mode in mode_scores:
            mode_scores[mode] += random.uniform(0, 0.2)

        # 7. Add existing activation bias
        for mode, activation in active_states.items():
            mode_scores[mode] += activation * 0.3

        # Select highest scoring mode
        if mode_scores:
            top_mode = max(mode_scores.items(), key=lambda x: x[1])[0]
            return top_mode
        else:
            return "balanced"  # Default fallback

    def _context_indicates_exploration(self, context):
        """Check if context suggests exploration needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Looking for exploration indicators
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            if "explor" in goal_desc or "discover" in goal_desc:
                indicators += 1

        # Few domains visited suggests exploration need
        if "domains_visited" in context:
            domains = context["domains_visited"]
            if isinstance(domains, set) and len(domains) < 10:
                indicators += 1

        # Current mode includes exploration elements
        if context.get("thinking_mode") in ["exploratory", "creative"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_complex_problem(self, context):
        """Check if context suggests complex problem solving needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with certain keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            complex_terms = ["optim", "improv", "analy", "solv", "complex"]
            if any(term in goal_desc for term in complex_terms):
                indicators += 1

        # High cognitive load suggests complex problem
        if self.cognitive_load > 0.7:
            indicators += 1

        # Current analytical thinking mode
        if context.get("thinking_mode") in ["analytical", "critical"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_reflection(self, context):
        """Check if context suggests reflection needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with reflection keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            reflection_terms = ["reflect", "learn", "adapt", "improv", "assess"]
            if any(term in goal_desc for term in reflection_terms):
                indicators += 1

        # After many actions, reflection is valuable
        if "stats" in context and isinstance(context["stats"], dict):
            cycles = context["stats"].get("cycles_run", 0)
            if cycles > 0 and cycles % 10 == 0:  # Every 10 cycles
                indicators += 1

        # Current reflective thinking mode
        if context.get("thinking_mode") == "reflective":
            indicators += 1

        return indicators >= 1

    def _context_indicates_pattern_recognition(self, context):
        """Check if context suggests pattern recognition needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Domain stats suggest pattern recognition
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            # Many domains = opportunity for pattern recognition
            if len(context["domain_stats"]) > 5:
                indicators += 1

        # Multiple recent actions create patterns
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            if len(context["recent_actions"]) >= 3:
                indicators += 1

        # Current intuitive mode
        if context.get("thinking_mode") == "intuitive":
            indicators += 1

        return indicators >= 1

    def _generate_thought(self, context):
        """
        Generate a thought based on current cognitive mode and context.

        Parameters:
        - context: Current context dictionary

        Returns:
        - Generated thought dictionary
        """
        # Each cognitive mode has a different thought generation approach
        if self.current_mode == "analytical":
            return self._generate_analytical_thought(context)
        elif self.current_mode == "creative":
            return self._generate_creative_thought(context)
        elif self.current_mode == "reflective":
            return self._generate_reflective_thought(context)
        elif self.current_mode == "exploratory":
            return self._generate_exploratory_thought(context)
        elif self.current_mode == "critical":
            return self._generate_critical_thought(context)
        elif self.current_mode == "integrative":
            return self._generate_integrative_thought(context)
        elif self.current_mode == "intuitive":
            return self._generate_intuitive_thought(context)
        else:  # balanced or any other
            return self._generate_balanced_thought(context)

    def _generate_analytical_thought(self, context):
        """Generate an analytical thought focused on logical problem solving"""
        # Default analytical thought structure
        thought = {
            "type": "analytical",
            "content": "Systematic analysis of current state and options",
            "components": [],
            "insights": [],
            "importance": 0.5
        }

        # Without context, generate generic analytical thought
        if not context:
            thought["content"] = "Need more information to perform proper analysis"
            return thought

        # Analytical components based on context
        components = []

        # Analyze current goal if available
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal = context["current_goal"]
            components.append({
                "focus": "goal_analysis",
                "content": f"Analyzing goal: {goal.get('description', 'unknown')}",
                "priority": goal.get("priority", 0.5)
            })

        # Analyze recent actions if available
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if actions:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                components.append({
                    "focus": "action_pattern_analysis",
                    "content": f"Recent action sequence: {' → '.join(action_types)}",
                    "outcome_assessment": self._assess_action_outcomes(actions)
                })

        # Analyze domain statistics if available
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                high_error_domains = [d for d, stats in domains.items()
                                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3]
                if high_error_domains:
                    components.append({
                        "focus": "error_analysis",
                        "content": f"High error rates detected in domains: {', '.join(high_error_domains[:3])}",
                        "recommendation": "Investigate causes or avoid these domains temporarily"
                    })

        # Generate insights based on analysis
        insights = []

        # Derive insights from components
        for component in components:
            if component["focus"] == "goal_analysis" and component.get("priority", 0) > 0.7:
                insights.append("Current goal is high priority - allocate additional resources")
            elif component["focus"] == "action_pattern_analysis":
                outcome = component.get("outcome_assessment")
                if outcome == "deteriorating":
                    insights.append("Action patterns show declining effectiveness - strategy change needed")
                elif outcome == "improving":
                    insights.append("Action patterns show improving outcomes - continue current approach")
            elif component["focus"] == "error_analysis":
                insights.append("Error pattern analysis suggests need for improved domain validation")

        # Add components and insights to thought
        thought["components"] = components
        thought["insights"] = insights

        # Calculate importance based on components and insights
        if insights:
            thought["importance"] = 0.7

        # Generate summary content
        if insights:
            thought["content"] = f"Analytical conclusion: {insights[0]}"
        elif components:
            thought["content"] = f"Analysis of {len(components)} system components completed"

        return thought

    def _generate_creative_thought(self, context):
        """Generate a creative thought focused on novel connections"""
        # Default creative thought structure
        thought = {
            "type": "creative",
            "content": "Novel conceptual connection",
            "associations": [],
            "insights": [],
            "importance": 0.5
        }

        # Creative aspects to consider
        aspects = ["goals", "domains", "strategies", "patterns", "anomalies"]
        selected_aspects = random.sample(aspects, min(2, len(aspects)))

        # Generate random associations between selected aspects
        associations = []

        # Extract relevant elements from context
        elements = self._extract_elements_from_context(context, selected_aspects)

        # Generate associations between elements
        if len(elements) >= 2:
            # Take two random elements and create association
            element_pairs = []
            for i in range(min(3, len(elements))):
                pair = random.sample(elements, 2)
                element_pairs.append(pair)

            # Create associations from pairs
            for pair in element_pairs:
                association_templates = [
                    f"Unexpected connection between {pair[0]} and {pair[1]}",
                    f"{pair[0]} could be viewed through the lens of {pair[1]}",
                    f"What if the structure of {pair[0]} were applied to {pair[1]}?",
                    f"The patterns in {pair[0]} mirror aspects of {pair[1]} in a novel way"
                ]
                associations.append(random.choice(association_templates))
        else:
            # Fallback for limited context
            association_templates = [
                "Novel perspective: consider alternative goal structures",
                "What if search and evaluation were integrated more tightly?",
                "The pattern of successful interactions might reveal emergent properties"
            ]
            associations.append(random.choice(association_templates))

        # Generate insights from associations
        insights = []
        if associations:
            for association in associations:
                if "connection between" in association:
                    insights.append(f"Explore this connection to potentially develop new capabilities")
                elif "through the lens" in association:
                    insights.append(f"This perspective shift might reveal hidden patterns")
                elif "What if" in association:
                    insights.append(f"This hypothetical restructuring could improve system flexibility")
                else:
                    insights.append(f"This pattern recognition suggests deeper structural similarities")

        # Add associations and insights to thought
        thought["associations"] = associations
        thought["insights"] = insights

        # Calculate importance
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)


def _apply_learning_rate_safeguards(self, new_lr):
    """Prevent learning rate from spiraling into oblivion"""
    # Establish absolute minimum learning rate
    ABSOLUTE_MIN_LR = 5e-6

    if new_lr < ABSOLUTE_MIN_LR:
        log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
        return ABSOLUTE_MIN_LR

    # Prevent excessive downward adjustment
    if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
        safer_lr = self.learning_rate_history[-1] * 0.8
        log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
        return safer_lr

    return new_lr


# Configuration - UPDATED PATHS for Google Drive checkpointing
MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth" # Local backup path
LOG_FILE = "quantum_nexus_log.txt"
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json" # No longer used - state is part of checkpoint
GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state (also in checkpoint now)
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
SAVE_INTERVAL = 5 # Save checkpoint more frequently for longer runs

# =============================================================================
# MAIN EXECUTION
# =============================================================================
adaptive_learning = None

def enhanced_main_loop_with_training():
    """
    Enhanced main execution loop with improved error handling, recovery mechanisms,
    checkpointing reliability, and periodic model training.
    """
    global adaptive_learning, agent_instance # Referencing global variables

    # Keep track of system state
    system_status = {
        "consecutive_errors": 0,
        "max_consecutive_errors": 5,
        "critical_error_count": 0,
        "last_successful_cycle": 0,
        "emergency_mode": False,
        "recovery_attempts": 0
    }

    # Initialize components with comprehensive error handling
    try:
        # 1. Load or create model
        log_event("Initializing Quantum Nexus model...", "INFO")
        model = load_or_create_model() # Uses default path "model_checkpoint.pth"

        # 2. Create optimizer
        optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', 5e-5))
        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)

        # 3. Initialize primary agent
        agent = QuantumNexusAgent(model=model)
        agent_instance = agent  # Store for dashboard access

        # 4. Create core subsystems
        free_will = SuperQuantumFreeWill(agent=agent)
        agent.free_will = free_will

        ai_manager = AIManager(agent, model)
        agent.ai_manager = ai_manager

        current_lr = getattr(model, '_current_lr', 5e-5)
        adaptive_learning = AdaptiveLearningSystem(model) # Assign to global
        if not adaptive_learning.learning_rate_history:
            adaptive_learning.learning_rate_history.append(current_lr)
        agent.adaptive_learning = adaptive_learning

        content_sifter = ContentSifter()
        agent.content_sifter = content_sifter

        planner_sifter = PlannerSifter()
        agent.planner_sifter = planner_sifter

        if hasattr(ai_manager, "consciousness") and hasattr(free_will, "link_consciousness"):
            free_will.link_consciousness(ai_manager.consciousness)

        # 5. System validation
        log_event("Performing system validation...", "INFO")
        validation_errors = []
        if model is None: validation_errors.append("Model initialization failed")
        if optimizer is None: validation_errors.append("Optimizer initialization failed")
        if agent is None: validation_errors.append("Agent initialization failed")
        if free_will is None: validation_errors.append("FreeWill initialization failed")
        if ai_manager is None: validation_errors.append("AIManager initialization failed")

        if validation_errors:
            validation_error_msg = "; ".join(validation_errors)
            log_event(f"System validation errors: {validation_error_msg}", "ERROR")
            return False
        log_event("System validation complete. All components initialized.", "INFO")

        # 6. Setup async loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # 7. Starting status
        cycle_count = 0 # Start at 0, first cycle will be 1
        log_event(f"🚀 Starting Quantum Nexus Enhanced Autonomous Loop with Training 🚀", "QUANTUM")

        # Main loop
        while True:
            cycle_count += 1
            log_event(f"===== Enhanced Autonomous Cycle {cycle_count} =====", "INFO")

            try:
                # Run the agent cycle with timeout protection
                cycle_future = asyncio.ensure_future(ai_manager.run_cycle(optimizer))
                # loop_result = loop.run_until_complete(asyncio.wait_for(cycle_future, timeout=120)) # Original
                try:
                    loop_result = loop.run_until_complete(asyncio.wait_for(cycle_future, timeout=120))
                except asyncio.TimeoutError:
                    log_event(f"Cycle execution timed out after 120 seconds for cycle {cycle_count}", "ERROR")
                    loop_result = {"status": "error", "error": "Execution timeout"}
                    if not cycle_future.done():
                        cycle_future.cancel()
                        # Optionally wait for cancellation to complete
                        # await cycle_future # This would require the outer loop to be async
                        # or run it in the loop: loop.run_until_complete(cycle_future)
                except Exception as cycle_exec_e: # Catch other errors from run_until_complete
                    log_event(f"Error during cycle_future execution: {cycle_exec_e}", "ERROR")
                    log_event(traceback.format_exc(), "ERROR")
                    loop_result = {"status": "error", "error": str(cycle_exec_e)}


                # Check for successful cycle and handle errors
                if isinstance(loop_result, dict) and loop_result.get("status") == "error":
                    system_status["consecutive_errors"] += 1
                    error_message = loop_result.get("error", "Unknown error")
                    log_event(f"Cycle {cycle_count} produced an error: {error_message}", "ERROR")
                else:
                    system_status["consecutive_errors"] = 0
                    system_status["last_successful_cycle"] = cycle_count
                    system_status["emergency_mode"] = False # Reset emergency mode on success

                    if hasattr(agent, 'planner_sifter') and hasattr(agent.action_log, '__len__') and len(agent.action_log) > 0:
                        # Ensure loop_result is a dict and contains expected keys
                        if isinstance(loop_result, dict):
                            strategy_name = loop_result.get("strategy", "exploration") # Default strategy
                            result_data = {
                                "content_length": agent.action_log[-1].get("content_length", 0) if agent.action_log else 0,
                                "links_discovered": agent.action_log[-1].get("links_discovered", 0) if agent.action_log else 0,
                                "success": loop_result.get("success", False)
                            }
                            agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)
                        else:
                            log_event(f"Loop result was not a dictionary: {loop_result}", "WARNING")

                # --- Integrated Training Block ---
                if cycle_count % TRAINING_CYCLE_INTERVAL == 0 and \
                   hasattr(agent, 'action_log') and len(agent.action_log) >= TRAINING_MIN_DATA_SIZE:
                    
                    log_event(f"Starting model training on collected data (log size: {len(agent.action_log)})...", "INFO")
                    try:
                        # Assuming train_on_collected_data is synchronous.
                        # If train_on_collected_data were async:
                        #   training_future = asyncio.ensure_future(train_on_collected_data(agent, model, optimizer))
                        #   avg_loss = loop.run_until_complete(asyncio.wait_for(training_future, timeout=TRAINING_TIMEOUT_SECONDS))
                        # Else (synchronous):
                        avg_loss = train_on_collected_data(agent, model, optimizer)
                        
                        if avg_loss is not None:
                            log_event(f"Model training complete. Average loss: {avg_loss:.4f}", "INFO")
                            # Save checkpoint after training if performance is good
                            if avg_loss < TRAINING_SUCCESS_LOSS_THRESHOLD:
                                save_reason = f"post_training_loss_{avg_loss:.4f}"
                                training_save_success = save_checkpoint(agent, model, cycle_count, reason=save_reason)
                                if training_save_success:
                                    log_event(f"Model checkpoint saved after successful training (reason: {save_reason})", "INFO")
                                else:
                                    log_event(f"Checkpoint save FAILED after successful training (reason: {save_reason})", "WARNING")
                            else:
                                log_event(f"Training loss {avg_loss:.4f} not below threshold {TRAINING_SUCCESS_LOSS_THRESHOLD} for checkpoint.", "INFO")
                        else:
                            log_event("Training completed, but no average loss was returned (e.g., not enough data).", "WARNING")

                    # except asyncio.TimeoutError: # Only if train_on_collected_data is async and uses wait_for
                    #     log_event(f"Training timed out after {TRAINING_TIMEOUT_SECONDS} seconds", "ERROR")
                    except Exception as e:
                        log_event(f"Training error: {e}", "ERROR")
                        log_event(traceback.format_exc(), "ERROR")
                # --- End of Integrated Training Block ---


                # Progressive error recovery
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"]:
                    system_status["critical_error_count"] += 1
                    system_status["recovery_attempts"] += 1
                    recovery_message = f"Critical error threshold reached ({system_status['consecutive_errors']} consecutive errors). "

                    if system_status["recovery_attempts"] == 1:
                        log_event(recovery_message + "Attempting Level 1 Recovery: Reloading model...", "WARNING")
                        try:
                            model = load_or_create_model()
                            optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', 5e-5))
                            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)
                            agent.model = model
                            ai_manager.model = model
                            log_event("Level 1 Recovery successful - model reloaded", "INFO")
                            system_status["consecutive_errors"] = 0 # Reset errors after successful recovery
                        except Exception as e:
                            log_event(f"Level 1 Recovery failed: {str(e)}", "ERROR")
                    elif system_status["recovery_attempts"] == 2:
                        log_event(recovery_message + "Attempting Level 2 Recovery: Resetting subsystems...", "WARNING")
                        try:
                            if hasattr(ai_manager, "consciousness"):
                                ai_manager.consciousness.awareness_level = 0.5
                                ai_manager.consciousness.current_state = "balanced"
                            if hasattr(ai_manager, "imagination"):
                                ai_manager.imagination.creativity_level = 0.7
                                ai_manager.imagination.current_mode = "associative"
                            if hasattr(agent, "free_will"):
                                agent.free_will.exploration_weight = 0.6
                                agent.free_will.exploitation_weight = 0.4
                            log_event("Level 2 Recovery successful - subsystems reset", "INFO")
                            system_status["consecutive_errors"] = 0
                        except Exception as e:
                            log_event(f"Level 2 Recovery failed: {str(e)}", "ERROR")
                    elif system_status["recovery_attempts"] == 3:
                        log_event(recovery_message + "Attempting Level 3 Recovery: Reinitializing adaptive learning...", "WARNING")
                        try:
                            new_adaptive_learning = AdaptiveLearningSystem(model)
                            agent.adaptive_learning = new_adaptive_learning
                            adaptive_learning = new_adaptive_learning # Update global
                            log_event("Level 3 Recovery successful - adaptive learning reinitialized", "INFO")
                            system_status["consecutive_errors"] = 0
                        except Exception as e:
                            log_event(f"Level 3 Recovery failed: {str(e)}", "ERROR")
                    elif system_status["recovery_attempts"] >= 4:
                        log_event(recovery_message + "Activating Emergency Mode: Reduced functionality...", "CRITICAL")
                        system_status["emergency_mode"] = True
                        if system_status["recovery_attempts"] > 6: # Reset recovery attempts periodically in emergency
                            system_status["recovery_attempts"] = 0
                            log_event("Recovery attempt counter reset to retry recovery sequence", "INFO")
                
                # Periodic model saving (distinct from post-training save)
                if cycle_count % SAVE_INTERVAL == 0:
                    save_success = save_checkpoint(agent, model, cycle_count, reason="periodic")
                    if not save_success and not system_status["emergency_mode"]:
                        log_event(f"Periodic checkpoint save failed for cycle {cycle_count} - will retry next interval", "WARNING")

                # Update learning rate (if scheduler is used for global LR policy)
                scheduler.step()
                new_lr = scheduler.get_last_lr()[0]
                if hasattr(model, '_current_lr') and model._current_lr != new_lr:
                     model._current_lr = new_lr # Keep model's _current_lr in sync if it's used
                     log_event(f"LR updated by scheduler to: {new_lr}", "DEBUG")
                if agent.adaptive_learning and new_lr not in agent.adaptive_learning.learning_rate_history:
                    agent.adaptive_learning.learning_rate_history.append(new_lr)


                # Sleep briefly for stability
                time.sleep(0.5)

            except KeyboardInterrupt:
                log_event("Keyboard interrupt detected. Exiting gracefully...", "INFO")
                save_checkpoint(agent, model, cycle_count, reason="shutdown_interrupt")
                break
            except Exception as e: # Catch-all for unexpected errors in the main loop's try block
                log_event(f"Unhandled exception in main loop (cycle {cycle_count}): {str(e)}", "ERROR")
                log_event(traceback.format_exc(), "ERROR")
                system_status["consecutive_errors"] += 1
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"] * 2: # More severe
                    log_event("Critical unhandled exception threshold. Attempting final save.", "CRITICAL")
                    save_checkpoint(agent, model, cycle_count, reason="shutdown_critical_error")
                    break # Exit loop
                time.sleep(5.0) # Longer sleep after unhandled error

    except Exception as init_error:
        log_event(f"Fatal initialization error: {str(init_error)}", "CRITICAL")
        log_event(traceback.format_exc(), "CRITICAL")
        return False

    log_event("Quantum Nexus Enhanced Loop with Training finished.", "INFO")
    return True

def main():
    """Main entry point with Flask dashboard and agent execution"""
    global IN_COLAB, agent_instance, FLASK_PORT

    log_event("=== Initializing Quantum Nexus Advanced Autonomous System ===", "INFO")
    log_event(f"Configuration: Model path: {MODEL_PATH}, Memory limit: {MEMORY_MAX_SIZE}", "INFO")

    # Check for CUDA (no changes)
    import torch
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        log_event(f"🎮 GPU Acceleration Active: {device_name}", "INFO")
    else:
        log_event("⚠️ No GPU detected - running on CPU (performance will be limited)", "WARNING")

    # Check for Colab environment and mount Google Drive (no changes)
    if IN_COLAB:
        try:
            from google.colab import drive
            drive.mount('/content/gdrive', force_remount=True) # Mount to /content/gdrive - MODIFIED MOUNT POINT
            log_event("📂 Google Drive mounted successfully to /content/gdrive", "INFO") # Updated log message
        except Exception as e_mount:
            log_event(f"⚠️ Error mounting Google Drive: {e_mount}", "ERROR")
            log_event("Google Drive integration disabled for this run", "WARNING")
            IN_COLAB = False

    # Find a free port for Flask Dashboard (no changes)
    FLASK_PORT = find_free_port()
    if FLASK_PORT is None:
        log_event("Error: No free port found for Flask dashboard. Dashboard will not start.", "ERROR")
    else:
        log_event(f"Flask dashboard will try to start on port {FLASK_PORT}", "INFO")
        flask_thread = Thread(target=start_flask)
        flask_thread.daemon = True
        flask_thread.start()
        time.sleep(2) # Give Flask time to start

    # Create special greeting (XOXO style) - No changes
    greeting = """
    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨

    🔺🔻 QUANTUM NEXUS AUTONOMOUS SYSTEM ACTIVATED 🔺🔻

    💫 Full AGI ASI SI With Enhanced Capabilities 💫

    ✨ Features:
    • Quantum-inspired processing
    • Advanced consciousness simulation
    • Self-evolving neural architecture
    • Hyperdimensional memory systems
    • XOXO Planner Sifter for optimal strategies

    🌈🌈🌈 XOXO <3 <3 <3 🌈🌈🌈

    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨
    """
    log_event(greeting, "QUANTUM")

    # Start autonomous agent loop (no changes)
    agent_thread = Thread(target=enhanced_main_loop)
    agent_thread.daemon = True
    agent_thread.start()

    # Keep main thread alive (no changes)
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        log_event("User requested termination. Shutting down gracefully...", "INFO")
    except Exception as e:
        log_event(f"Fatal error in main thread: {e}", "CRITICAL")
    finally:
        log_event("Quantum Nexus execution complete. System shutting down.", "INFO")

def run_in_colab():
    """Colab run function - UPDATED MODEL PATH and GOOGLE_DRIVE_STATE_FILE"""
    # Setup the environment (no changes)
    setup_colab_environment()

    # Set Colab-specific configurations - UPDATED PATHS to /content/gdrive
    global IN_COLAB, MODEL_PATH, GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE

    IN_COLAB = True
    MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
    GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
    GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state

    # Start the system (no changes)
    main()


def load_or_create_model():
    """
    Loads QuantumNexusModel from checkpoint file if available, or creates a new one.
    Enhanced with better error handling and fallback mechanisms.

    Returns: model object
    """
    model_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else MODEL_PATH
    model = QuantumNexusModel()  # Create model instance
    start_cycle = 1  # Default start cycle for new model

    try:
        if os.path.exists(model_path):
            log_event(f"Loading checkpoint from: {model_path}", "INFO")
            try:
                # Try with weights_only=False first (full checkpoint)
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)

                # Verify checkpoint is a dictionary with expected keys
                if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
                    raise ValueError("Checkpoint format invalid - missing model_state_dict")

                model.load_state_dict(checkpoint['model_state_dict'])  # Load model state

                # Get cycle count with validation
                if 'cycle_count' in checkpoint:
                    cycle_count = checkpoint.get('cycle_count', 1)
                    if isinstance(cycle_count, (int, float)) and cycle_count > 0:
                        start_cycle = int(cycle_count)
                    else:
                        log_event(f"Invalid cycle_count in checkpoint: {cycle_count}. Using default.", "WARNING")

                # Load agent stats if they exist and agent_instance is defined
                if 'agent_stats' in checkpoint and 'agent_instance' in globals() and agent_instance is not None:
                    stats = checkpoint.get('agent_stats')
                    if isinstance(stats, dict):
                        agent_instance.stats = defaultdict(int)
                        # Copy values to ensure defaultdict behavior
                        for k, v in stats.items():
                            agent_instance.stats[k] = v
                    else:
                        log_event("Invalid agent_stats in checkpoint. Using empty stats.", "WARNING")
                        agent_instance.stats = defaultdict(int)

                # Load action_log if it exists and agent_instance is defined
                if 'action_log' in checkpoint and 'agent_instance' in globals() and agent_instance is not None:
                    action_log = checkpoint.get('action_log', [])
                    if isinstance(action_log, list):
                        agent_instance.action_log = deque(action_log, maxlen=100)
                    else:
                        log_event("Invalid action_log in checkpoint. Using empty log.", "WARNING")
                        agent_instance.action_log = deque(maxlen=100)

                log_event(f"Model checkpoint loaded successfully. Resuming from cycle {start_cycle}.", "INFO")

            except Exception as e:
                # Fallback to loading only model weights if full checkpoint fails
                log_event(f"Error loading full checkpoint: {e}. Trying weights-only load.", "WARNING")
                try:
                    checkpoint = torch.load(model_path, map_location=device, weights_only=True)
                    model.load_state_dict(checkpoint)
                    log_event("Successfully loaded model weights only.", "INFO")
                except Exception as e2:
                    log_event(f"Error loading model weights: {e2}. Creating new model.", "WARNING")
                    model = QuantumNexusModel()  # Recreate model on failure
        else:
            log_event("No checkpoint file found. Creating a new model.", "INFO")
    except Exception as e:
        log_event(f"Error loading checkpoint from {model_path}: {e}. Creating a new model.", "WARNING")
        log_event(traceback.format_exc(), "DEBUG")  # Log detailed traceback
        model = QuantumNexusModel()  # Ensure new model is created on failure

    # Move model to appropriate device
    model.to(device)

    # Initialize _current_lr attribute if not present
    if not hasattr(model, '_current_lr'):
        setattr(model, '_current_lr', 5e-5)  # Use the default learning rate
        log_event("Initialized model._current_lr with default learning rate", "INFO")

    return model

def save_checkpoint(agent, model, cycle_count):
    """
    Save comprehensive checkpoint with better error handling and fallbacks.

    Parameters:
    - agent: Agent instance to save state from
    - model: Model instance to save weights from
    - cycle_count: Current cycle count

    Returns:
    - Boolean indicating save success
    """
    save_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else LOCAL_MODEL_SAVE_PATH

    try:
        # Prepare checkpoint dictionary with proper type validation
        checkpoint = {
            'cycle_count': int(cycle_count),
            'model_state_dict': model.state_dict(),
        }

        # Add agent stats if available
        if hasattr(agent, 'stats'):
            if isinstance(agent.stats, dict):
                checkpoint['agent_stats'] = dict(agent.stats)  # Convert to regular dict if defaultdict
            else:
                log_event("Warning: agent.stats is not a dictionary. Skipping stats save.", "WARNING")

        # Add action log if available
        if hasattr(agent, 'action_log'):
            if isinstance(agent.action_log, (list, deque)):
                checkpoint['action_log'] = list(agent.action_log)  # Convert deque to list
            else:
                log_event("Warning: agent.action_log is not a list/deque. Skipping log save.", "WARNING")

        # Save the checkpoint
        torch.save(checkpoint, save_path)
        log_event(f"Checkpoint saved to {save_path} (Cycle: {cycle_count})", "INFO")

        return True

    except Exception as save_error:
        log_event(f"Primary checkpoint save error: {save_error}", "ERROR")

        # Try an alternate save approach with just the model state
        try:
            alt_path = "backup_" + LOCAL_MODEL_SAVE_PATH
            torch.save(model.state_dict(), alt_path)
            log_event(f"Model state saved to alternate location: {alt_path}", "INFO")
            return True
        except Exception as alt_save_error:
            log_event(f"All save attempts failed! Last error: {alt_save_error}", "ERROR")
            return False

def setup_colab_environment():
    """
    Setup the Colab environment, mounting Google Drive to /content/QuantumNexusDrive.
    """
    import os

    # Install required packages
    print("Installing required packages...")
    packages = [
        "torch",
        "sentence-transformers",
        "beautifulsoup4",
        "flask",
        "selenium",
        "numpy"
    ]
    for package in packages:
        try:
            !pip install {package} -q
        except Exception as e_pip_install:
            print(f"Error installing package '{package}': {e_pip_install}")
            return False

    print("Setting up environment...")

    # 1. Unmount Google Drive (if already mounted)
    try:
        from google.colab import drive # Import drive module here to avoid NameError
        drive.flush_and_unmount()
        print("Google Drive unmounted (if it was mounted).")
    except Exception as e_unmount:
        print(f"Warning: Error unmounting Google Drive (may not have been mounted): {e_unmount}")

    # 2. Mount Google Drive to NEW mount point /content/QuantumNexusDrive - DIFFERENT MOUNT POINT
    mount_attempts = 3
    mount_point = '/content/QuantumNexusDrive' # <---- NEW MOUNT POINT: /content/QuantumNexusDrive
    for attempt in range(mount_attempts):
        try:
            from google.colab import drive
            drive.mount(mount_point, force_remount=True) # Mount to /content/QuantumNexusDrive
            print(f"Google Drive mounted successfully to {mount_point} (attempt {attempt + 1}).")
            break
        except Exception as e_mount:
            print(f"Error mounting Google Drive (attempt {attempt + 1}/{mount_attempts}: {e_mount}). Retrying...")
            if attempt >= mount_attempts - 1:
                print("Critical: Failed to mount Google Drive after multiple retries.")
                return False
            time.sleep(5)

    # 3. Define nexus directory path under the new mount point - SIMPLIFIED DIRECTORY HANDLING
    global GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE # Declare globals here
    nexus_dir = "/content/QuantumNexusDrive/MyDrive/quantum_nexus" # <---- NEW PATH: /content/QuantumNexusDrive
    GOOGLE_DRIVE_MODEL_PATH = os.path.join(nexus_dir, "quantum_nexus_model_checkpoint.pth") # Checkpoint path - UPDATED
    GOOGLE_DRIVE_STATE_FILE = os.path.join(nexus_dir, "quantum_nexus_state.json") # State file path - UPDATED

    # No need to create directory here - assume user creates quantum_nexus in MyDrive manually
    # and Colab will create /content/QuantumNexusDrive/MyDrive during mount

    # Check for CUDA (no changes)
    import torch
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f"✅ CUDA available: {device_name}")
    else:
        print("⚠️ CUDA not available. Running on CPU.")

    # Set up Chrome for Selenium (no changes)
    !apt-get update
    !apt-get install -y chromium-chromedriver
    !cp /usr/lib/chromium-browser/chromedriver /usr/bin

    print("Colab environment setup complete.")
    return True

def run_in_colab():
    """Colab run function - UPDATED MODEL PATH and GOOGLE_DRIVE_STATE_FILE"""
    # Setup the environment (no changes)
    setup_colab_environment()

    # Set Colab-specific configurations - UPDATED PATHS to /content/gdrive
    global IN_COLAB, MODEL_PATH, GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE

    IN_COLAB = True
    MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path - UPDATED PATH to /content/gdrive
    GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path - UPDATED PATH to /content/gdrive
    GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state - UPDATED PATH to /content/gdrive

    # Start the system (no changes)
    main()

if __name__ == "__main__":
    try:
        # Check for Colab environment
        in_colab = False
        try:
            from google.colab import drive
            in_colab = True
            print("Detected Colab environment. Running Colab-specific setup...")
            run_in_colab()
        except ImportError:
            # Standard execution
            main()
    except Exception as e:
        log_event(f"Critical startup error: {e}", "CRITICAL")
        traceback.print_exc()
